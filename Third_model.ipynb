{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 - Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.genmod.families import Poisson\n",
    "from statsmodels.genmod.families.family import Tweedie\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chargement des données ===\n",
      "\n",
      "Chargement des données principales:\n",
      "\n",
      "Chargement du format de soumission:\n",
      "\n",
      "=== Dimensions des données ===\n",
      "Train features initiales: (383610, 373)\n",
      "Train target: (383610, 5)\n",
      "Train après fusion: (383610, 377)\n",
      "Test: (95852, 374)\n",
      "Format soumission: (95852, 5)\n",
      "\n",
      "Types de données dans train:\n",
      "object     280\n",
      "int64       58\n",
      "float64     39\n",
      "dtype: int64\n",
      "\n",
      "=== Résumé du chargement ===\n",
      "Nombre d'observations train: 383610\n",
      "Nombre d'observations test: 95852\n",
      "Nombre de features: 373\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    print(\"=== Chargement des données ===\")\n",
    "    \n",
    "    # 1. Chargement des données d'entraînement et de test\n",
    "    print(\"\\nChargement des données principales:\")\n",
    "    train_features = pd.read_csv('data/train_input_Z61KlZo.csv', low_memory=False)\n",
    "    train_target = pd.read_csv('data/train_output_DzPxaPY.csv')\n",
    "    test_df = pd.read_csv('data/test_input_5qJzHrr.csv', low_memory=False)\n",
    "    \n",
    "    # 2. Chargement du format de soumission\n",
    "    print(\"\\nChargement du format de soumission:\")\n",
    "    submission_format = pd.read_csv('data/submission_csv_file_random_example_3fbDtrr (1).csv')\n",
    "    \n",
    "    # Vérification des colonnes du format de soumission\n",
    "    expected_cols = ['ID', 'FREQ', 'CM', 'ANNEE_ASSURANCE', 'CHARGE']\n",
    "    if not all(col in submission_format.columns for col in expected_cols):\n",
    "        raise ValueError(f\"Format de soumission incorrect. Colonnes attendues: {expected_cols}\")\n",
    "    \n",
    "    # 3. Gestion de la colonne ANNEE_ASSURANCE\n",
    "    if 'ANNEE_ASSURANCE' in train_features.columns:\n",
    "        train_features = train_features.drop('ANNEE_ASSURANCE', axis=1)\n",
    "    \n",
    "    # 4. Fusion des données train\n",
    "    train_df = pd.merge(train_features, train_target, on='ID', how='inner')\n",
    "    \n",
    "    # 5. Vérifications et statistiques\n",
    "    print(\"\\n=== Dimensions des données ===\")\n",
    "    print(f\"Train features initiales: {train_features.shape}\")\n",
    "    print(f\"Train target: {train_target.shape}\")\n",
    "    print(f\"Train après fusion: {train_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "    print(f\"Format soumission: {submission_format.shape}\")\n",
    "    \n",
    "    # 6. Vérification de la cohérence des IDs\n",
    "    test_ids = set(test_df['ID'])\n",
    "    submission_ids = set(submission_format['ID'])\n",
    "    \n",
    "    if test_ids != submission_ids:\n",
    "        print(\"\\nATTENTION: Différence entre les IDs de test et de soumission!\")\n",
    "        print(f\"IDs uniquement dans test: {len(test_ids - submission_ids)}\")\n",
    "        print(f\"IDs uniquement dans soumission: {len(submission_ids - test_ids)}\")\n",
    "    \n",
    "    # 7. Vérification des colonnes\n",
    "    train_cols = set(train_df.columns) - {'FREQ', 'CM', 'CHARGE', 'ID'}\n",
    "    test_cols = set(test_df.columns) - {'ID'}\n",
    "    \n",
    "    missing_cols = test_cols - train_cols\n",
    "    extra_cols = train_cols - test_cols\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\nColonnes manquantes dans train: {missing_cols}\")\n",
    "    if extra_cols:\n",
    "        print(f\"\\nColonnes supplémentaires dans train: {extra_cols}\")\n",
    "    \n",
    "    # 8. Vérification des types de données\n",
    "    print(\"\\nTypes de données dans train:\")\n",
    "    print(train_df.dtypes.value_counts())\n",
    "    \n",
    "    # 9. Création d'un template de soumission vide\n",
    "    submission_template = submission_format.copy()\n",
    "    submission_template[['FREQ', 'CM', 'CHARGE']] = 0\n",
    "    \n",
    "    return {\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df,\n",
    "        'submission_format': submission_format,\n",
    "        'submission_template': submission_template,\n",
    "        'stats': {\n",
    "            'train_shape': train_df.shape,\n",
    "            'test_shape': test_df.shape,\n",
    "            'submission_shape': submission_format.shape,\n",
    "            'n_features': len(train_cols),\n",
    "            'missing_cols': list(missing_cols),\n",
    "            'extra_cols': list(extra_cols)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécution du chargement\n",
    "data_results = load_data()\n",
    "\n",
    "# Récupération des DataFrames\n",
    "train_df = data_results['train_df']\n",
    "test_df = data_results['test_df']\n",
    "submission_template = data_results['submission_template']\n",
    "\n",
    "# Affichage des statistiques détaillées\n",
    "print(\"\\n=== Résumé du chargement ===\")\n",
    "stats = data_results['stats']\n",
    "print(f\"Nombre d'observations train: {stats['train_shape'][0]}\")\n",
    "print(f\"Nombre d'observations test: {stats['test_shape'][0]}\")\n",
    "print(f\"Nombre de features: {stats['n_features']}\")\n",
    "\n",
    "if stats['missing_cols']:\n",
    "    print(\"\\nAttention: colonnes manquantes dans train!\")\n",
    "    print(stats['missing_cols'])\n",
    "\n",
    "if stats['extra_cols']:\n",
    "    print(\"\\nColonnes supplémentaires dans train:\")\n",
    "    print(stats['extra_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_structure():\n",
    "    print(\"=== Analyse de la structure des données ===\")\n",
    "    \n",
    "    # Afficher les colonnes de chaque DataFrame\n",
    "    print(\"\\nColonnes du format de soumission:\")\n",
    "    print(data_results['submission_format'].columns.tolist())\n",
    "    \n",
    "    # Trouver les colonnes différentes\n",
    "    train_cols = set(train_df.columns)\n",
    "    test_cols = set(test_df.columns)\n",
    "    \n",
    "    print(\"\\nColonnes présentes dans train mais pas dans test:\")\n",
    "    diff_cols = train_cols - test_cols\n",
    "    print(sorted(diff_cols))\n",
    "    \n",
    "    # Vérifier si ANNEE_ASSURANCE est bien géré\n",
    "    print(\"\\nPrésence de ANNEE_ASSURANCE:\")\n",
    "    print(f\"Dans train: {'ANNEE_ASSURANCE' in train_df.columns}\")\n",
    "    print(f\"Dans test: {'ANNEE_ASSURANCE' in test_df.columns}\")\n",
    "    print(f\"Dans soumission: {'ANNEE_ASSURANCE' in data_results['submission_format'].columns}\")\n",
    "\n",
    "# Exécuter l'analyse\n",
    "analyze_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_charge_calculation(train_df, submission_format):\n",
    "    print(\"=== Vérification du calcul de CHARGE ===\")\n",
    "    \n",
    "    # 1. Vérification dans le train\n",
    "    print(\"\\nVérification dans le train:\")\n",
    "    train_df['CHARGE_calc'] = train_df['FREQ'] * train_df['CM'] * train_df['ANNEE_ASSURANCE']\n",
    "    train_df['diff_pct'] = abs(train_df['CHARGE_calc'] - train_df['CHARGE']) / train_df['CHARGE'] * 100\n",
    "    \n",
    "    print(\"\\nStatistiques des différences dans train (en %):\")\n",
    "    print(train_df['diff_pct'].describe())\n",
    "    \n",
    "    train_problematic = train_df[train_df['diff_pct'] > 1]\n",
    "    if len(train_problematic) > 0:\n",
    "        print(f\"\\nNombre de cas problématiques dans train: {len(train_problematic)}\")\n",
    "        print(\"\\nExemples de cas problématiques dans train:\")\n",
    "        print(train_problematic[['FREQ', 'CM', 'ANNEE_ASSURANCE', 'CHARGE', 'CHARGE_calc', 'diff_pct']].head())\n",
    "    \n",
    "    # 2. Vérification dans le format de soumission\n",
    "    print(\"\\nVérification dans le format de soumission:\")\n",
    "    submission_format['CHARGE_calc'] = (submission_format['FREQ'] * \n",
    "                                      submission_format['CM'] * \n",
    "                                      submission_format['ANNEE_ASSURANCE'])\n",
    "    submission_format['diff_pct'] = (abs(submission_format['CHARGE_calc'] - \n",
    "                                       submission_format['CHARGE']) / \n",
    "                                    submission_format['CHARGE'] * 100)\n",
    "    \n",
    "    print(\"\\nStatistiques des différences dans soumission (en %):\")\n",
    "    print(submission_format['diff_pct'].describe())\n",
    "    \n",
    "    sub_problematic = submission_format[submission_format['diff_pct'] > 1]\n",
    "    if len(sub_problematic) > 0:\n",
    "        print(f\"\\nNombre de cas problématiques dans soumission: {len(sub_problematic)}\")\n",
    "        print(\"\\nExemples de cas problématiques dans soumission:\")\n",
    "        print(sub_problematic[['FREQ', 'CM', 'ANNEE_ASSURANCE', 'CHARGE', 'CHARGE_calc', 'diff_pct']].head())\n",
    "    \n",
    "    # 3. Statistiques globales\n",
    "    print(\"\\n=== Résumé des vérifications ===\")\n",
    "    stats = {\n",
    "        'train': {\n",
    "            'n_total': len(train_df),\n",
    "            'n_problematic': len(train_problematic),\n",
    "            'pct_problematic': len(train_problematic) / len(train_df) * 100,\n",
    "            'max_diff': train_df['diff_pct'].max()\n",
    "        },\n",
    "        'submission': {\n",
    "            'n_total': len(submission_format),\n",
    "            'n_problematic': len(sub_problematic),\n",
    "            'pct_problematic': len(sub_problematic) / len(submission_format) * 100,\n",
    "            'max_diff': submission_format['diff_pct'].max()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTrain:\")\n",
    "    print(f\"Total: {stats['train']['n_total']}\")\n",
    "    print(f\"Problématiques: {stats['train']['n_problematic']} ({stats['train']['pct_problematic']:.2f}%)\")\n",
    "    print(f\"Différence max: {stats['train']['max_diff']:.2f}%\")\n",
    "    \n",
    "    print(\"\\nSoumission:\")\n",
    "    print(f\"Total: {stats['submission']['n_total']}\")\n",
    "    print(f\"Problématiques: {stats['submission']['n_problematic']} ({stats['submission']['pct_problematic']:.2f}%)\")\n",
    "    print(f\"Différence max: {stats['submission']['max_diff']:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_df': train_df,\n",
    "        'submission_format': submission_format,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "verification_results = verify_charge_calculation(train_df, data_results['submission_format'])\n",
    "\n",
    "# Nettoyer les colonnes temporaires si nécessaire\n",
    "train_df = train_df.drop(['CHARGE_calc', 'diff_pct'], axis=1, errors='ignore')\n",
    "data_results['submission_format'] = data_results['submission_format'].drop(['CHARGE_calc', 'diff_pct'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_quality():\n",
    "    print(\"=== Analyse approfondie de la qualité des données ===\\n\")\n",
    "    \n",
    "    # 1. Analyse des valeurs manquantes\n",
    "    missing_pct = (train_df.isnull().sum() / len(train_df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': train_df.isnull().sum(),\n",
    "        'Missing Percentage': missing_pct\n",
    "    }).sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    print(\"Colonnes avec valeurs manquantes (>50%):\")\n",
    "    print(missing_summary[missing_summary['Missing Percentage'] > 50])\n",
    "    \n",
    "    # 2. Analyse des doublons\n",
    "    print(f\"\\nNombre de lignes dupliquées: {train_df.duplicated().sum()}\")\n",
    "    \n",
    "    # 3. Analyse des colonnes constantes ou quasi-constantes\n",
    "    constant_cols = []\n",
    "    quasi_constant_cols = []\n",
    "    for col in train_df.columns:\n",
    "        unique_vals = train_df[col].nunique()\n",
    "        if unique_vals == 1:\n",
    "            constant_cols.append(col)\n",
    "        elif unique_vals == 2 and train_df[col].value_counts(normalize=True).iloc[0] > 0.99:\n",
    "            quasi_constant_cols.append(col)\n",
    "    \n",
    "    print(\"\\nColonnes constantes:\")\n",
    "    print(constant_cols)\n",
    "    print(\"\\nColonnes quasi-constantes (>99% même valeur):\")\n",
    "    print(quasi_constant_cols)\n",
    "    \n",
    "    # 4. Analyse des corrélations fortes\n",
    "    numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    corr_matrix = train_df[numeric_cols].corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            if abs(corr_matrix.iloc[i,j]) > 0.95:  # Seuil de corrélation à 0.95\n",
    "                high_corr_pairs.append((numeric_cols[i], numeric_cols[j], corr_matrix.iloc[i,j]))\n",
    "    \n",
    "    print(\"\\nPaires de variables fortement corrélées (>0.95):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "    \n",
    "    # 5. Recommandations pour le nettoyage\n",
    "    print(\"\\n=== Recommandations pour le nettoyage des données ===\")\n",
    "    \n",
    "    # Colonnes à potentiellement supprimer\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    # Colonnes avec trop de valeurs manquantes\n",
    "    high_missing_cols = missing_summary[missing_summary['Missing Percentage'] > 80].index.tolist()\n",
    "    if high_missing_cols:\n",
    "        print(\"\\nColonnes à considérer pour suppression (>80% manquantes):\")\n",
    "        print(high_missing_cols)\n",
    "        cols_to_drop.extend(high_missing_cols)\n",
    "    \n",
    "    # Colonnes constantes ou quasi-constantes\n",
    "    if constant_cols or quasi_constant_cols:\n",
    "        print(\"\\nColonnes à considérer pour suppression (constantes ou quasi-constantes):\")\n",
    "        print(constant_cols + quasi_constant_cols)\n",
    "        cols_to_drop.extend(constant_cols + quasi_constant_cols)\n",
    "    \n",
    "    return cols_to_drop, high_corr_pairs, missing_summary\n",
    "\n",
    "# Exécution de l'analyse\n",
    "cols_to_drop, high_corr_pairs, missing_summary = analyze_data_quality()\n",
    "\n",
    "# Visualisation des valeurs manquantes\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(range(len(missing_summary[:20])), missing_summary['Missing Percentage'][:20])\n",
    "plt.xticks(range(len(missing_summary[:20])), missing_summary.index[:20], rotation=45, ha='right')\n",
    "plt.title('Top 20 des colonnes avec valeurs manquantes')\n",
    "plt.ylabel('Pourcentage de valeurs manquantes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_cleaning_analysis():\n",
    "    print(\"=== Analyse détaillée pour le nettoyage ===\\n\")\n",
    "    \n",
    "    # 1. Analyse complète des valeurs manquantes\n",
    "    missing_pct = (train_df.isnull().sum() / len(train_df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': train_df.isnull().sum(),\n",
    "        'Missing Percentage': missing_pct\n",
    "    }).sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    # Catégorisation des colonnes selon le pourcentage de valeurs manquantes\n",
    "    extreme_missing = missing_summary[missing_summary['Missing Percentage'] > 90]\n",
    "    high_missing = missing_summary[(missing_summary['Missing Percentage'] > 70) & (missing_summary['Missing Percentage'] <= 90)]\n",
    "    moderate_missing = missing_summary[(missing_summary['Missing Percentage'] > 50) & (missing_summary['Missing Percentage'] <= 70)]\n",
    "    \n",
    "    print(\"1. Colonnes avec valeurs manquantes extrêmes (>90%):\")\n",
    "    print(extreme_missing)\n",
    "    print(f\"\\nNombre de colonnes: {len(extreme_missing)}\")\n",
    "    \n",
    "    print(\"\\n2. Colonnes avec beaucoup de valeurs manquantes (70-90%):\")\n",
    "    print(high_missing)\n",
    "    print(f\"\\nNombre de colonnes: {len(high_missing)}\")\n",
    "    \n",
    "    print(\"\\n3. Colonnes avec valeurs manquantes modérées (50-70%):\")\n",
    "    print(moderate_missing)\n",
    "    print(f\"\\nNombre de colonnes: {len(moderate_missing)}\")\n",
    "    \n",
    "    # 2. Analyse des colonnes constantes et quasi-constantes\n",
    "    constant_cols = [col for col in train_df.columns if train_df[col].nunique() == 1]\n",
    "    quasi_constant_cols = [col for col in train_df.columns if \n",
    "                         train_df[col].nunique() == 2 and \n",
    "                         train_df[col].value_counts(normalize=True).iloc[0] > 0.99]\n",
    "    \n",
    "    print(\"\\n4. Colonnes constantes:\", len(constant_cols))\n",
    "    print(constant_cols)\n",
    "    \n",
    "    print(\"\\n5. Colonnes quasi-constantes:\", len(quasi_constant_cols))\n",
    "    print(quasi_constant_cols)\n",
    "    \n",
    "    # 3. Recommandations\n",
    "    print(\"\\n=== Recommandations de nettoyage ===\")\n",
    "    \n",
    "    # Colonnes à supprimer automatiquement\n",
    "    auto_drop = constant_cols + list(extreme_missing.index)\n",
    "    print(f\"\\n1. À supprimer automatiquement ({len(auto_drop)} colonnes):\")\n",
    "    print(\"- Colonnes constantes et >90% manquantes\")\n",
    "    \n",
    "    # Colonnes à évaluer\n",
    "    evaluate = list(high_missing.index) + quasi_constant_cols\n",
    "    print(f\"\\n2. À évaluer ({len(evaluate)} colonnes):\")\n",
    "    print(\"- 70-90% manquantes\")\n",
    "    print(\"- Quasi-constantes\")\n",
    "    \n",
    "    # Colonnes à potentiellement imputer\n",
    "    impute = list(moderate_missing.index)\n",
    "    print(f\"\\n3. À potentiellement imputer ({len(impute)} colonnes):\")\n",
    "    print(\"- 50-70% manquantes\")\n",
    "    \n",
    "    return auto_drop, evaluate, impute\n",
    "\n",
    "# Exécution de l'analyse détaillée\n",
    "auto_drop, evaluate, impute = detailed_cleaning_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Définition de tous les groupes de variables\n",
    "WEATHER_VARS = {\n",
    "    'temp_max': [col for col in train_df.columns if any(x in col.upper() for x in \n",
    "                 ['NBJTX', 'TX', 'TXMAX', 'TXAB']) and 'TMMAX' not in col.upper()],\n",
    "    \n",
    "    'temp_min': [col for col in train_df.columns if any(x in col.upper() for x in \n",
    "                 ['NBJTN', 'TN', 'TNMIN', 'TNAB']) and 'TMMIN' not in col.upper()],\n",
    "    \n",
    "    'temp_moy': [col for col in train_df.columns if any(x in col.upper() for x in \n",
    "                 ['TM', 'TMM', 'TMMAX', 'TMMIN'])],\n",
    "    \n",
    "    'temp_amplitude': [col for col in train_df.columns if 'TAMPLI' in col.upper()],\n",
    "    \n",
    "    'wind': [col for col in train_df.columns if any(x in col.upper() for x in \n",
    "             ['FF', 'FX'])],\n",
    "    \n",
    "    'rain': [col for col in train_df.columns if any(x in col.upper() for x in \n",
    "             ['RR', 'RRAB'])]\n",
    "}\n",
    "\n",
    "# Redéfinition des variables bâtiment et activité\n",
    "BUILDING_VARS = {\n",
    "    # Surfaces\n",
    "    'surface': [f'SURFACE{i}' for i in range(1, 22)],\n",
    "    \n",
    "    # Nombre de bâtiments (attention au NBBAT12 manquant)\n",
    "    'buildings': [f'NBBAT{i}' for i in range(1, 15) if i != 12],\n",
    "    \n",
    "    # Caractéristiques du bâtiment\n",
    "    'characteristics': [f'CARACT{i}' for i in range(1, 6)],\n",
    "    \n",
    "    # Types de bâtiment\n",
    "    'building_type': ['TYPBAT1', 'TYPBAT2', 'ADOSS'],\n",
    "    \n",
    "    # Hauteurs et données BDTOPO\n",
    "    'height': [\n",
    "        'HAUTEUR', 'HAUTEUR_MAX',\n",
    "        'BDTOPO_BAT_MAX_HAUTEUR', 'BDTOPO_BAT_MAX_HAUTEUR_MAX'\n",
    "    ]\n",
    "}\n",
    "\n",
    "ACTIVITY_VARS = {\n",
    "    # Multi-équipement\n",
    "    'equipment': [f'EQUIPEMENT{i}' for i in range(1, 8)],\n",
    "    \n",
    "    # Activité et vocation\n",
    "    'activity': [\n",
    "        'ACTIVIT2',     # Activité\n",
    "        'VOCATION',     # Vocation de l'entité professionnelle\n",
    "        'TYPERS'        # Type de personne\n",
    "    ],\n",
    "    \n",
    "    # Catégorie\n",
    "    'category': ['COEFASS'],  # Retrait des variables TAILLE qui sont dans INSURANCE_VARS\n",
    "    \n",
    "}\n",
    "\n",
    "INSURANCE_VARS = {\n",
    "    # Indicateurs de risque\n",
    "    'risk_indicators': [f'RISK{i}' for i in range(1, 14)],\n",
    "    \n",
    "    # Données de capitaux\n",
    "    'capital': [f'KAPITAL{i}' for i in range(1, 44)],\n",
    "    \n",
    "    # Dérogations tarifaires\n",
    "    'derogation': [f'DEROG{i}' for i in range(1, 17)],\n",
    "    \n",
    "    # Franchises et indemnisations\n",
    "    'contract_terms': {\n",
    "        'franchise': ['FRCH1', 'FRCH2'],\n",
    "        'indemnisation': ['INDEM1', 'INDEM2']\n",
    "    },\n",
    "    \n",
    "    # Données d'activité et taille\n",
    "    'business_profile': {\n",
    "        'chiffre_affaires': ['CA1', 'CA2', 'CA3'],\n",
    "        'taille_risque': [f'TAILLE{i}' for i in range(1, 5)]  # Gardé ici car lié au profil business\n",
    "    },\n",
    "    \n",
    "    # Historique et sinistralité\n",
    "    'history': [\n",
    "        'NBSINCONJ',     # Sinistralité conjoncturelle\n",
    "        'NBSINSTRT',     # Sinistralité longue période\n",
    "        'ANCIENNETE',    # Ancienneté du contrat\n",
    "        'DUREE_REQANEUF' # Durée\n",
    "    ],\n",
    "    \n",
    "    # Données temporelles\n",
    "    'time': [\n",
    "        'AN_EXERC',        # Année d'exercice\n",
    "        'ANNEE_ASSURANCE'  # Nombre d'années assurées\n",
    "    ]\n",
    "}\n",
    "GEOGRAPHIC_VARS = {\n",
    "    # Distances par type d'environnement\n",
    "    'distances': {\n",
    "        'urban': [  # Zones urbaines\n",
    "            'DISTANCE_111',  # Tissu urbain continu\n",
    "            'DISTANCE_112',  # Tissu urbain discontinu\n",
    "            'DISTANCE_121',  # Zones industrielles/commerciales\n",
    "            'DISTANCE_122',  # Réseaux routier/ferroviaire\n",
    "            'DISTANCE_123',  # Zones portuaires\n",
    "            'DISTANCE_124',  # Aéroports\n",
    "            'DISTANCE_131',  # Extraction de matériaux\n",
    "            'DISTANCE_132',  # Décharges\n",
    "            'DISTANCE_133',  # Chantiers\n",
    "            'DISTANCE_141',  # Espaces verts urbains\n",
    "            'DISTANCE_142'   # Équipements sportifs et de loisirs\n",
    "        ],\n",
    "        'natural': [  # Zones naturelles\n",
    "            'DISTANCE_311',  # Forêts de feuillus\n",
    "            'DISTANCE_312',  # Forêts de conifères\n",
    "            'DISTANCE_313',  # Forêts mélangées\n",
    "            'DISTANCE_321',  # Pelouses et pâturages naturels\n",
    "            'DISTANCE_322',  # Landes et broussailles\n",
    "            'DISTANCE_323',  # Végétation sclérophylle\n",
    "            'DISTANCE_324',  # Forêt et végétation arbustive en mutation\n",
    "            'DISTANCE_331',  # Plages, dunes et sable\n",
    "            'DISTANCE_332',  # Roches nues\n",
    "            'DISTANCE_333',  # Végétation clairsemée\n",
    "            'DISTANCE_334',  # Zones incendiées\n",
    "            'DISTANCE_335'   # Glaciers et neiges éternelles\n",
    "        ],\n",
    "        'water': [  # Zones aquatiques\n",
    "            'DISTANCE_411',  # Marais intérieurs\n",
    "            'DISTANCE_412',  # Tourbières\n",
    "            'DISTANCE_421',  # Marais maritimes\n",
    "            'DISTANCE_422',  # Marais salants\n",
    "            'DISTANCE_423',  # Zones intertidales\n",
    "            'DISTANCE_511',  # Cours d'eau\n",
    "            'DISTANCE_512',  # Plans d'eau\n",
    "            'DISTANCE_521',  # Lagunes littorales\n",
    "            'DISTANCE_522',  # Estuaires\n",
    "            'DISTANCE_523'   # Mers et océans\n",
    "        ],\n",
    "        'other': [  # Autres distances\n",
    "            'DISTANCE_211',  # Terres arables hors périmètres d'irrigation\n",
    "            'DISTANCE_212',  # Périmètres irrigués en permanence\n",
    "            'DISTANCE_213',  # Rizières\n",
    "            'DISTANCE_221',  # Vignobles\n",
    "            'DISTANCE_222',  # Vergers et petits fruits\n",
    "            'DISTANCE_223',  # Oliveraies\n",
    "            'DISTANCE_231',  # Prairies et autres surfaces toujours en herbe\n",
    "            'DISTANCE_242',  # Systèmes culturaux et parcellaires complexes\n",
    "            'DISTANCE_243',  # Surfaces agricoles avec végétation naturelle\n",
    "            'DISTANCE_244'   # Territoires agroforestiers\n",
    "        ],\n",
    "        'general': [  # Distances générales\n",
    "            'DISTANCE_1',    # Distance générale 1\n",
    "            'DISTANCE_2'     # Distance générale 2\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Proportions par type d'environnement\n",
    "    'proportions': {\n",
    "        'urban': [  # Zones urbaines\n",
    "            'PROPORTION_11',  # Zones urbanisées\n",
    "            'PROPORTION_12',  # Zones industrielles et commerciales\n",
    "            'PROPORTION_13',  # Mines, décharges et chantiers\n",
    "            'PROPORTION_14'   # Espaces verts artificialisés\n",
    "        ],\n",
    "        'agricultural': [  # Zones agricoles\n",
    "            'PROPORTION_21',  # Terres arables\n",
    "            'PROPORTION_22',  # Cultures permanentes\n",
    "            'PROPORTION_23',  # Prairies\n",
    "            'PROPORTION_24'   # Zones agricoles hétérogènes\n",
    "        ],\n",
    "        'natural': [  # Zones naturelles\n",
    "            'PROPORTION_31',  # Forêts\n",
    "            'PROPORTION_32',  # Milieux à végétation arbustive et/ou herbacée\n",
    "            'PROPORTION_33'   # Espaces ouverts, sans ou avec peu de végétation\n",
    "        ],\n",
    "        'water': [  # Zones aquatiques\n",
    "            'PROPORTION_41',  # Zones humides intérieures\n",
    "            'PROPORTION_42',  # Zones humides maritimes\n",
    "            'PROPORTION_51',  # Eaux continentales\n",
    "            'PROPORTION_52'   # Eaux maritimes\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Données d'altitude et zones\n",
    "    'topography': {\n",
    "        'altitude': [f'ALTITUDE_{i}' for i in range(1, 6)],  # Différents niveaux d'altitude\n",
    "        'zone_info': [\n",
    "            'ZONE',          # Zone géographique\n",
    "            'ZONE_VENT',     # Zone de vent\n",
    "            'ESPINSEE'       # Espace INSEE\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Données d'urgence\n",
    "    'emergency': ['NB_CASERNES']  # Nombre de casernes de pompiers à proximité\n",
    "}\n",
    "DEMOGRAPHIC_VARS = {\n",
    "    # Variables ménages\n",
    "    'household': {\n",
    "        'general': ['MEN'],  # Nombre total de ménages\n",
    "        'composition': [\n",
    "            'MEN_1IND',      # Ménages d'un seul individu\n",
    "            'MEN_5IND',      # Ménages de 5 individus ou plus\n",
    "            'MEN_FMP'        # Ménages monoparentaux\n",
    "        ],\n",
    "        'housing_type': [\n",
    "            'MEN_COLL',      # Ménages en logements collectifs\n",
    "            'MEN_MAIS',      # Ménages en maison\n",
    "            'MEN_PROP'       # Ménages propriétaires\n",
    "        ],\n",
    "        'economic': [\n",
    "            'MEN_PAUV',      # Ménages pauvres\n",
    "            'MEN_SURF'       # Surface des logements du carreau\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Variables logement\n",
    "    'housing': {\n",
    "        'construction_period': [\n",
    "            'LOG_AVA1',      # Avant année A1\n",
    "            'LOG_A1_A2',     # Entre A1 et A2\n",
    "            'LOG_A2_A3',     # Entre A2 et A3\n",
    "            'LOG_APA3'       # Après A3\n",
    "        ],\n",
    "        'special_categories': [\n",
    "            'LOG_INC',       # Date de construction inconnue\n",
    "            'LOG_SOC'        # Logements sociaux\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Variables individus\n",
    "    'individual': {\n",
    "        'total': ['IND'],    # Nombre total d'individus\n",
    "        'age_groups': [\n",
    "            'IND_0_Y1',      # 0 à Y1 ans\n",
    "            'IND_Y1_Y2',     # Y1 à Y2 ans\n",
    "            'IND_Y2_Y3',     # Y2 à Y3 ans\n",
    "            'IND_Y3_Y4',     # Y3 à Y4 ans\n",
    "            'IND_Y4_Y5',     # Y4 à Y5 ans\n",
    "            'IND_Y5_Y6',     # Y5 à Y6 ans\n",
    "            'IND_Y6_Y7',     # Y6 à Y7 ans\n",
    "            'IND_Y7_Y8',     # Y7 à Y8 ans\n",
    "            'IND_Y8_Y9',     # Y8 à Y9 ans\n",
    "            'IND_Y9'         # Y9 ans ou plus\n",
    "        ],\n",
    "        'other': [\n",
    "            'IND_INC',       # Âge inconnu\n",
    "            'IND_SNV'        # Niveaux de vie winsorisés\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "TARGET_VARS = {\n",
    "    'primary': {\n",
    "        'frequency': ['FREQ'],      # Fréquence\n",
    "        'cost': ['CM'],            # Coût moyen\n",
    "        'total': ['CHARGE'],       # Charge totale\n",
    "        'period': ['ANNEE_ASSURANCE']  # Période d'assurance\n",
    "    },\n",
    "    'definitions': {\n",
    "        'FREQ': 'NOMBRE DE SINISTRES / ANNEE ASSURANCE',\n",
    "        'CM': 'CHARGE SINISTRES / NOMBRE DE SINISTRES',\n",
    "        'CHARGE': 'FREQ * CM * ANNEE_ASSURANCE',\n",
    "        'ANNEE_ASSURANCE': 'Durée de la période d\\'assurance en années'\n",
    "    }\n",
    "}\n",
    "ID_VARS = {\n",
    "    'identifiers': ['ID'],\n",
    "    'properties': {\n",
    "        'unique': True,          # L'ID doit être unique\n",
    "        'not_null': True,        # Pas de valeurs nulles autorisées\n",
    "        'type': 'str'           # Type attendu\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_weather_vars():\n",
    "    print(\"=== Vérification des variables météo ===\\n\")\n",
    "    \n",
    "    # 1. Vérifier les doublons globaux\n",
    "    all_weather_vars = []\n",
    "    for category, cols in WEATHER_VARS.items():\n",
    "        all_weather_vars.extend(cols)\n",
    "    \n",
    "    duplicates = [var for var in set(all_weather_vars) \n",
    "                 if all_weather_vars.count(var) > 1]\n",
    "    \n",
    "    if duplicates:\n",
    "        print(\"ATTENTION! Variables en double:\")\n",
    "        for var in duplicates:\n",
    "            categories = [cat for cat, cols in WEATHER_VARS.items() if var in cols]\n",
    "            print(f\"- {var} présent dans: {categories}\")\n",
    "    \n",
    "    # 2. Vérifier chaque catégorie\n",
    "    for category, cols in WEATHER_VARS.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"Nombre de variables: {len(cols)}\")\n",
    "        \n",
    "        # Vérifier les préfixes\n",
    "        prefixes = set([col.split('_')[0] for col in cols])\n",
    "        print(\"Préfixes trouvés:\", prefixes)\n",
    "        \n",
    "        # Exemples de variables\n",
    "        print(\"Exemples de variables:\", sorted(cols)[:3])\n",
    "        \n",
    "        # Vérifier l'existence dans le DataFrame\n",
    "        missing_in_df = [col for col in cols if col not in train_df.columns]\n",
    "        if missing_in_df:\n",
    "            print(f\"ATTENTION! Variables non trouvées dans le DataFrame:\")\n",
    "            print(missing_in_df)\n",
    "        \n",
    "        # Vérifier les valeurs manquantes\n",
    "        existing_cols = [col for col in cols if col in train_df.columns]\n",
    "        if existing_cols:\n",
    "            missing_pct = (train_df[existing_cols].isnull().mean() * 100).mean()\n",
    "            print(f\"Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "    \n",
    "    # 3. Vérifier si des variables météo n'ont pas été classées\n",
    "    weather_patterns = {\n",
    "        'temp': ['TX', 'TN', 'TM', 'TEMP'],\n",
    "        'wind': ['FF', 'FX'],\n",
    "        'rain': ['RR', 'RRAB'],\n",
    "        'amplitude': ['TAMPLI']\n",
    "    }\n",
    "    \n",
    "    unclassified = []\n",
    "    for pattern_type, patterns in weather_patterns.items():\n",
    "        for col in train_df.columns:\n",
    "            if any(pattern in col.upper() for pattern in patterns):\n",
    "                if col not in all_weather_vars:\n",
    "                    unclassified.append((col, pattern_type))\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables météo potentiellement non classées:\")\n",
    "        for col, type_ in unclassified:\n",
    "            print(f\"- {col} (type: {type_})\")\n",
    "    \n",
    "    # 4. Résumé\n",
    "    print(\"\\n=== Résumé des variables météo ===\")\n",
    "    print(f\"Total variables classées: {len(set(all_weather_vars))}\")\n",
    "    print(f\"Variables en double: {len(duplicates)}\")\n",
    "    print(f\"Variables non classées: {len(unclassified)}\")\n",
    "    \n",
    "    return {\n",
    "        'classified_vars': set(all_weather_vars),\n",
    "        'duplicates': duplicates,\n",
    "        'unclassified': unclassified,\n",
    "        'missing_in_df': {cat: [col for col in cols if col not in train_df.columns] \n",
    "                         for cat, cols in WEATHER_VARS.items()}\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "weather_results = verify_weather_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_building_and_activity_vars():\n",
    "    print(\"=== Vérification des variables bâtiment et activité ===\\n\")\n",
    "    \n",
    "    # Collecter toutes les variables pour vérification des doublons\n",
    "    all_building_vars = []\n",
    "    all_activity_vars = []\n",
    "    \n",
    "    # Vérifier les deux groupes\n",
    "    for group_name, group_vars, all_vars_list in [\n",
    "        (\"BUILDING\", BUILDING_VARS, all_building_vars), \n",
    "        (\"ACTIVITY\", ACTIVITY_VARS, all_activity_vars)\n",
    "    ]:\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        \n",
    "        for category, vars_list in group_vars.items():\n",
    "            print(f\"\\n  {category}:\")\n",
    "            print(f\"  Nombre de variables définies: {len(vars_list)}\")\n",
    "            print(\"  Variables:\", vars_list)\n",
    "            \n",
    "            # Vérifier l'existence\n",
    "            existing_vars = [var for var in vars_list if var in train_df.columns]\n",
    "            missing_vars = [var for var in vars_list if var not in train_df.columns]\n",
    "            \n",
    "            print(f\"  Variables trouvées: {len(existing_vars)}/{len(vars_list)}\")\n",
    "            \n",
    "            if missing_vars:\n",
    "                print(f\"  ATTENTION! Variables manquantes: {missing_vars}\")\n",
    "            \n",
    "            # Vérifier les valeurs manquantes pour les variables existantes\n",
    "            if existing_vars:\n",
    "                missing_pct = (train_df[existing_vars].isnull().mean() * 100).mean()\n",
    "                print(f\"  Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            \n",
    "            all_vars_list.extend(vars_list)\n",
    "    \n",
    "    # Vérifier les doublons entre les deux groupes\n",
    "    building_vars = set(all_building_vars)\n",
    "    activity_vars = set(all_activity_vars)\n",
    "    duplicates = building_vars.intersection(activity_vars)\n",
    "    \n",
    "    if duplicates:\n",
    "        print(\"\\nATTENTION! Variables en double entre BUILDING et ACTIVITY:\")\n",
    "        print(duplicates)\n",
    "    \n",
    "    # 3. Vérifier les variables non classées par type\n",
    "    patterns = {\n",
    "        'surface': 'SURFACE',\n",
    "        'batiment': 'NBBAT',\n",
    "        'caracteristique': 'CARACT',\n",
    "        'type_batiment': 'TYPBAT',\n",
    "        'hauteur': 'HAUTEUR',\n",
    "        'bdtopo': 'BDTOPO',\n",
    "        'equipement': 'EQUIPEMENT',\n",
    "        'activite': 'ACTIVIT',\n",
    "        'vocation': 'VOCATION',\n",
    "        'type_personne': 'TYPERS'\n",
    "    }\n",
    "    \n",
    "    unclassified = {}\n",
    "    all_defined_vars = building_vars.union(activity_vars)\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns if pattern in col \n",
    "                       and col not in all_defined_vars]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            print(f\"\\n{pattern_name.upper()}:\")\n",
    "            print(f\"Nombre: {len(vars_list)}\")\n",
    "            print(\"Variables:\", vars_list)\n",
    "    \n",
    "    # Résumé final\n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    \n",
    "    print(\"\\n=== Résumé ===\")\n",
    "    print(f\"Variables bâtiment: {len(building_vars)}\")\n",
    "    print(f\"Variables activité: {len(activity_vars)}\")\n",
    "    print(f\"Variables en double: {len(duplicates)}\")\n",
    "    print(f\"Variables non classées: {total_unclassified}\")\n",
    "    \n",
    "    return {\n",
    "        'building_vars': building_vars,\n",
    "        'activity_vars': activity_vars,\n",
    "        'duplicates': duplicates,\n",
    "        'unclassified': unclassified,\n",
    "        'stats': {\n",
    "            'building': {cat: len(vars_) for cat, vars_ in BUILDING_VARS.items()},\n",
    "            'activity': {cat: len(vars_) for cat, vars_ in ACTIVITY_VARS.items()},\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "building_activity_results = verify_building_and_activity_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Pour les expressions régulières\n",
    "def verify_insurance_vars():\n",
    "    \"\"\"\n",
    "    Vérifie la cohérence et la présence des variables d'assurance.\n",
    "    \"\"\"\n",
    "    print(\"=== Vérification des variables d'assurance ===\\n\")\n",
    "    \n",
    "    # 1. Collecter toutes les variables définies\n",
    "    all_defined_vars = []\n",
    "    \n",
    "    # 2. Vérifier chaque catégorie principale\n",
    "    for category, content in INSURANCE_VARS.items():\n",
    "        # 2.1 Traitement des catégories avec sous-catégories\n",
    "        if isinstance(content, dict):\n",
    "            print(f\"\\n{category.upper()}:\")\n",
    "            for subcat, vars_list in content.items():\n",
    "                print(f\"\\n  {subcat}:\")\n",
    "                verify_variable_group(vars_list, all_defined_vars)\n",
    "        \n",
    "        # 2.2 Traitement des catégories simples\n",
    "        else:\n",
    "            print(f\"\\n{category.upper()}:\")\n",
    "            verify_variable_group(content, all_defined_vars)\n",
    "    \n",
    "    # 3. Vérifier les doublons\n",
    "    duplicates = find_duplicates(all_defined_vars)\n",
    "    \n",
    "    # 4. Vérifier les variables non classées\n",
    "    unclassified = find_unclassified_vars(all_defined_vars)\n",
    "    \n",
    "    # 5. Générer le résumé\n",
    "    print_summary(all_defined_vars, duplicates, unclassified)\n",
    "    \n",
    "    return generate_results(all_defined_vars, duplicates, unclassified)\n",
    "\n",
    "def verify_variable_group(vars_list, all_vars):\n",
    "    \"\"\"\n",
    "    Vérifie un groupe de variables et met à jour la liste globale.\n",
    "    \"\"\"\n",
    "    print(f\"  Nombre de variables définies: {len(vars_list)}\")\n",
    "    print(\"  Variables:\", vars_list)\n",
    "    \n",
    "    # Vérifier l'existence\n",
    "    existing_vars = [var for var in vars_list if var in train_df.columns]\n",
    "    missing_vars = [var for var in vars_list if var not in train_df.columns]\n",
    "    \n",
    "    print(f\"  Variables trouvées: {len(existing_vars)}/{len(vars_list)}\")\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"  ATTENTION! Variables manquantes: {missing_vars}\")\n",
    "    \n",
    "    # Vérifier les valeurs manquantes\n",
    "    if existing_vars:\n",
    "        missing_pct = (train_df[existing_vars].isnull().mean() * 100).mean()\n",
    "        print(f\"  Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "    \n",
    "    all_vars.extend(vars_list)\n",
    "\n",
    "def find_duplicates(all_vars):\n",
    "    \"\"\"\n",
    "    Identifie les variables en double.\n",
    "    \"\"\"\n",
    "    return [var for var in set(all_vars) if all_vars.count(var) > 1]\n",
    "\n",
    "def find_unclassified_vars(all_vars):\n",
    "    \"\"\"\n",
    "    Identifie les variables d'assurance non classées.\n",
    "    \"\"\"\n",
    "    insurance_patterns = {\n",
    "        'risk': '^RISK[0-9]+$',\n",
    "        'capital': '^KAPITAL[0-9]+$',\n",
    "        'derogation': '^DEROG[0-9]+$',\n",
    "        'franchise': '^FRCH[0-9]+$',\n",
    "        'indemnisation': '^INDEM[0-9]+$',\n",
    "        'chiffre_affaires': '^CA[0-9]+$',\n",
    "        'sinistre': '^NBSIN',\n",
    "        'anciennete': '^ANCIEN',\n",
    "        'exercice': '^(AN_)?EXERC'\n",
    "    }\n",
    "    \n",
    "    # Exclure les patterns d'autres groupes\n",
    "    other_patterns = ['VOCATION', 'CARACT', 'NB_CASERNES', 'EQUIPEMENT', 'SURFACE']\n",
    "    \n",
    "    unclassified = {}\n",
    "    for pattern_name, pattern in insurance_patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns \n",
    "                       if re.match(pattern, col) and col not in all_vars\n",
    "                       and not any(other in col for other in other_patterns)]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    return {k: v for k, v in unclassified.items() if v}\n",
    "\n",
    "def print_summary(all_vars, duplicates, unclassified):\n",
    "    \"\"\"\n",
    "    Affiche le résumé de la vérification.\n",
    "    \"\"\"\n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            print(f\"\\n{pattern_name.upper()}:\")\n",
    "            print(f\"Nombre: {len(vars_list)}\")\n",
    "            print(\"Variables:\", vars_list)\n",
    "    \n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    \n",
    "    print(\"\\n=== Résumé ===\")\n",
    "    print(f\"Total variables définies: {len(all_vars)}\")\n",
    "    print(f\"Variables uniques: {len(set(all_vars))}\")\n",
    "    print(f\"Doublons: {len(duplicates)}\")\n",
    "    if duplicates:\n",
    "        print(\"Variables en double:\", duplicates)\n",
    "    print(f\"Variables non classées: {total_unclassified}\")\n",
    "\n",
    "def generate_results(all_vars, duplicates, unclassified):\n",
    "    \"\"\"\n",
    "    Génère le dictionnaire de résultats.\n",
    "    \"\"\"\n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    \n",
    "    return {\n",
    "        'all_vars': set(all_vars),\n",
    "        'duplicates': duplicates,\n",
    "        'unclassified': unclassified,\n",
    "        'missing_vars': {cat: [var for var in (content if isinstance(content, list) \n",
    "                                              else sum(content.values(), [])) \n",
    "                              if var not in train_df.columns] \n",
    "                        for cat, content in INSURANCE_VARS.items()},\n",
    "        'stats': {\n",
    "            'total_defined': len(all_vars),\n",
    "            'total_unique': len(set(all_vars)),\n",
    "            'total_duplicates': len(duplicates),\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "insurance_results = verify_insurance_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_geographic_vars():\n",
    "    print(\"=== Vérification des variables géographiques ===\\n\")\n",
    "    \n",
    "    # Aplatir la structure imbriquée\n",
    "    all_vars = []\n",
    "    category_vars = {}  # Pour suivre les variables par catégorie\n",
    "    \n",
    "    # 1. Collecter toutes les variables\n",
    "    for main_category, content in GEOGRAPHIC_VARS.items():\n",
    "        category_vars[main_category] = []\n",
    "        if isinstance(content, dict):\n",
    "            for subcategory, vars_list in content.items():\n",
    "                all_vars.extend(vars_list)\n",
    "                category_vars[main_category].extend(vars_list)\n",
    "        else:\n",
    "            all_vars.extend(content)\n",
    "            category_vars[main_category].extend(content)\n",
    "    \n",
    "    # 2. Vérifier chaque catégorie\n",
    "    for main_category, content in GEOGRAPHIC_VARS.items():\n",
    "        print(f\"\\n{main_category.upper()}:\")\n",
    "        \n",
    "        if isinstance(content, dict):\n",
    "            for subcategory, vars_list in content.items():\n",
    "                print(f\"\\n  {subcategory}:\")\n",
    "                print(f\"  Nombre de variables définies: {len(vars_list)}\")\n",
    "                if len(vars_list) > 3:\n",
    "                    print(f\"  Exemples de variables: {vars_list[:3]} ...\")\n",
    "                else:\n",
    "                    print(f\"  Variables: {vars_list}\")\n",
    "                \n",
    "                # Vérifier l'existence\n",
    "                existing_vars = [var for var in vars_list if var in train_df.columns]\n",
    "                missing_vars = [var for var in vars_list if var not in train_df.columns]\n",
    "                \n",
    "                print(f\"  Variables trouvées: {len(existing_vars)}/{len(vars_list)}\")\n",
    "                \n",
    "                if missing_vars:\n",
    "                    print(f\"  ATTENTION! Variables manquantes ({len(missing_vars)}):\")\n",
    "                    for var in missing_vars[:5]:  # Montrer les 5 premiers exemples\n",
    "                        print(f\"    - {var}\")\n",
    "                    if len(missing_vars) > 5:\n",
    "                        print(f\"    ... et {len(missing_vars)-5} autres\")\n",
    "                \n",
    "                # Vérifier les valeurs manquantes\n",
    "                if existing_vars:\n",
    "                    missing_pct = (train_df[existing_vars].isnull().mean() * 100).mean()\n",
    "                    print(f\"  Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Nombre de variables: {len(content)}\")\n",
    "            print(\"Variables:\", content)\n",
    "    \n",
    "    # 3. Vérifier les variables non classées\n",
    "    patterns = {\n",
    "        'distance': 'DISTANCE_',\n",
    "        'proportion': 'PROPORTION_',\n",
    "        'altitude': 'ALTITUDE_',\n",
    "        'zone': 'ZONE'\n",
    "    }\n",
    "    \n",
    "    unclassified = {}\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns if pattern in col \n",
    "                       and col not in all_vars]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            print(f\"\\n{pattern_name.upper()}:\")\n",
    "            print(f\"Nombre: {len(vars_list)}\")\n",
    "            print(\"Exemples:\", vars_list[:5], \"...\" if len(vars_list) > 5 else \"\")\n",
    "    \n",
    "    # 4. Résumé final\n",
    "    print(\"\\n=== Résumé de la vérification géographique ===\")\n",
    "    print(f\"Total variables définies: {len(all_vars)}\")\n",
    "    \n",
    "    existing_total = len([var for var in all_vars if var in train_df.columns])\n",
    "    print(f\"Variables existantes: {existing_total}\")\n",
    "    print(f\"Variables manquantes: {len(all_vars) - existing_total}\")\n",
    "    \n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    print(f\"Variables non classées: {total_unclassified}\")\n",
    "    \n",
    "    # 5. Statistiques par catégorie\n",
    "    print(\"\\nStatistiques par catégorie principale:\")\n",
    "    for category, vars_list in category_vars.items():\n",
    "        existing = len([var for var in vars_list if var in train_df.columns])\n",
    "        print(f\"{category}: {existing}/{len(vars_list)} variables trouvées\")\n",
    "    \n",
    "    return {\n",
    "        'all_vars': set(all_vars),\n",
    "        'by_category': category_vars,\n",
    "        'unclassified': unclassified,\n",
    "        'stats': {\n",
    "            'total_defined': len(all_vars),\n",
    "            'total_existing': existing_total,\n",
    "            'total_missing': len(all_vars) - existing_total,\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "geo_results = verify_geographic_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_demographic_vars():\n",
    "    print(\"=== Vérification des variables démographiques ===\\n\")\n",
    "    \n",
    "    # Aplatir la structure imbriquée\n",
    "    all_vars = []\n",
    "    for main_category, subcategories in DEMOGRAPHIC_VARS.items():\n",
    "        for subcat, vars_list in subcategories.items():\n",
    "            all_vars.extend(vars_list)\n",
    "    \n",
    "    # Vérifier chaque catégorie et sous-catégorie\n",
    "    for main_category, subcategories in DEMOGRAPHIC_VARS.items():\n",
    "        print(f\"\\n{main_category.upper()}:\")\n",
    "        \n",
    "        for subcat, vars_list in subcategories.items():\n",
    "            print(f\"\\n  {subcat}:\")\n",
    "            print(f\"  Nombre de variables définies: {len(vars_list)}\")\n",
    "            print(\"  Variables définies:\", vars_list)\n",
    "            \n",
    "            # Vérifier l'existence\n",
    "            existing_vars = [var for var in vars_list if var in train_df.columns]\n",
    "            missing_vars = [var for var in vars_list if var not in train_df.columns]\n",
    "            \n",
    "            print(f\"  Variables trouvées: {len(existing_vars)}/{len(vars_list)}\")\n",
    "            \n",
    "            if missing_vars:\n",
    "                print(f\"  ATTENTION! Variables manquantes: {missing_vars}\")\n",
    "            \n",
    "            # Vérifier les valeurs manquantes seulement pour les variables existantes\n",
    "            if existing_vars:\n",
    "                missing_pct = (train_df[existing_vars].isnull().mean() * 100).mean()\n",
    "                print(f\"  Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "    \n",
    "    # Patterns pour identifier les variables démographiques\n",
    "    demo_patterns = {\n",
    "        'menage': '^MEN_',\n",
    "        'logement': '^LOG_',\n",
    "        'individu': '^IND_'\n",
    "    }\n",
    "    \n",
    "    # Exclure les patterns d'autres groupes\n",
    "    other_patterns = ['INDEM', 'EQUIPEMENT', 'ACTIVIT', 'SURFACE', 'CARACT']\n",
    "    \n",
    "    # Vérifier les variables non classées\n",
    "    unclassified = {}\n",
    "    for pattern_name, pattern in demo_patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns \n",
    "                       if re.match(pattern, col) \n",
    "                       and col not in all_vars\n",
    "                       and not any(other in col for other in other_patterns)]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    # Afficher les variables non classées par type\n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            if vars_list:  # N'afficher que si la liste n'est pas vide\n",
    "                print(f\"\\n{pattern_name.upper()}:\")\n",
    "                print(f\"Nombre: {len(vars_list)}\")\n",
    "                print(\"Variables:\", vars_list)\n",
    "    \n",
    "    # Résumé final\n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    \n",
    "    print(\"\\n=== Résumé de la vérification démographique ===\")\n",
    "    existing_vars = [var for var in all_vars if var in train_df.columns]\n",
    "    missing_vars = [var for var in all_vars if var not in train_df.columns]\n",
    "    print(f\"Variables définies: {len(all_vars)}\")\n",
    "    print(f\"Variables existantes: {len(existing_vars)}\")\n",
    "    print(f\"Variables manquantes: {len(missing_vars)}\")\n",
    "    print(f\"Variables non classées: {total_unclassified}\")\n",
    "    \n",
    "    return {\n",
    "        'all_defined_vars': all_vars,\n",
    "        'existing_vars': existing_vars,\n",
    "        'missing_vars': missing_vars,\n",
    "        'unclassified': unclassified,\n",
    "        'stats': {\n",
    "            'total_defined': len(all_vars),\n",
    "            'total_existing': len(existing_vars),\n",
    "            'total_missing': len(missing_vars),\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "demo_results = verify_demographic_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_target_vars():\n",
    "    print(\"=== Vérification des variables cibles ===\\n\")\n",
    "    \n",
    "    # Aplatir la structure pour les variables\n",
    "    all_vars = []\n",
    "    for category in TARGET_VARS['primary'].values():\n",
    "        all_vars.extend(category)\n",
    "    \n",
    "    # Vérifier la présence des variables\n",
    "    for category, vars_list in TARGET_VARS['primary'].items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"Variables: {vars_list}\")\n",
    "        \n",
    "        # Vérifier l'existence\n",
    "        missing = [var for var in vars_list if var not in train_df.columns]\n",
    "        if missing:\n",
    "            print(f\"ATTENTION! Variables manquantes: {missing}\")\n",
    "        \n",
    "        # Vérifier les valeurs manquantes\n",
    "        if vars_list:\n",
    "            missing_pct = (train_df[vars_list].isnull().mean() * 100).mean()\n",
    "            print(f\"Pourcentage moyen de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "        \n",
    "        # Afficher la définition\n",
    "        for var in vars_list:\n",
    "            if var in TARGET_VARS['definitions']:\n",
    "                print(f\"Définition de {var}: {TARGET_VARS['definitions'][var]}\")\n",
    "    \n",
    "    # Vérifier les variables non classées\n",
    "    target_patterns = {\n",
    "        'frequence': '^FREQ',\n",
    "        'cout_moyen': '^CM',\n",
    "        'charge': '^CHARGE',\n",
    "        'annee': '^ANNEE_ASSURANCE'\n",
    "    }\n",
    "    \n",
    "    # Exclure les patterns d'autres groupes\n",
    "    other_patterns = ['FRCH', 'CARACT', 'SURFACE']\n",
    "    \n",
    "    unclassified = {}\n",
    "    for pattern_name, pattern in target_patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns \n",
    "                       if re.match(pattern, col) \n",
    "                       and col not in all_vars\n",
    "                       and not any(other in col for other in other_patterns)]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            if vars_list:\n",
    "                print(f\"\\n{pattern_name.upper()}:\")\n",
    "                print(f\"Nombre: {len(vars_list)}\")\n",
    "                print(\"Variables:\", vars_list)\n",
    "    \n",
    "    # Vérifier la cohérence des calculs\n",
    "    if all(var in train_df.columns for var in ['FREQ', 'CM', 'CHARGE', 'ANNEE_ASSURANCE']):\n",
    "        # Calculer CHARGE théorique\n",
    "        charge_calc = train_df['FREQ'] * train_df['CM'] * train_df['ANNEE_ASSURANCE']\n",
    "        \n",
    "        # Comparer avec CHARGE réelle\n",
    "        diff_pct = abs(charge_calc - train_df['CHARGE']).mean() / train_df['CHARGE'].mean() * 100\n",
    "        print(f\"\\nDifférence moyenne entre CHARGE calculée et réelle: {diff_pct:.2f}%\")\n",
    "    \n",
    "    # Statistiques descriptives\n",
    "    print(\"\\nStatistiques descriptives:\")\n",
    "    for var in all_vars:\n",
    "        if var in train_df.columns:\n",
    "            stats = train_df[var].describe()\n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Min: {stats['min']:.2f}\")\n",
    "            print(f\"  Max: {stats['max']:.2f}\")\n",
    "            print(f\"  Moyenne: {stats['mean']:.2f}\")\n",
    "            print(f\"  Écart-type: {stats['std']:.2f}\")\n",
    "    \n",
    "    # Résumé final\n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    print(\"\\n=== Résumé ===\")\n",
    "    print(f\"Variables définies: {len(all_vars)}\")\n",
    "    print(f\"Variables existantes: {len([var for var in all_vars if var in train_df.columns])}\")\n",
    "    print(f\"Variables manquantes: {len([var for var in all_vars if var not in train_df.columns])}\")\n",
    "    print(f\"Variables non classées: {total_unclassified}\")\n",
    "    \n",
    "    return {\n",
    "        'variables': all_vars,\n",
    "        'definitions': TARGET_VARS['definitions'],\n",
    "        'unclassified': unclassified,\n",
    "        'missing_stats': {var: train_df[var].isnull().mean() * 100 \n",
    "                         for var in all_vars if var in train_df.columns},\n",
    "        'stats': {\n",
    "            'total_defined': len(all_vars),\n",
    "            'total_existing': len([var for var in all_vars if var in train_df.columns]),\n",
    "            'total_missing': len([var for var in all_vars if var not in train_df.columns]),\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "target_results = verify_target_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_id_vars():\n",
    "    \"\"\"\n",
    "    Vérifie la cohérence et la présence des variables d'identification.\n",
    "    \"\"\"\n",
    "    print(\"=== Vérification des variables d'identification ===\\n\")\n",
    "    \n",
    "    id_vars = ID_VARS['identifiers']\n",
    "    properties = ID_VARS['properties']\n",
    "    \n",
    "    # 1. Vérifier la présence des variables\n",
    "    missing = [var for var in id_vars if var not in train_df.columns]\n",
    "    if missing:\n",
    "        print(f\"ATTENTION! Variables manquantes: {missing}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Vérifier chaque identifiant\n",
    "    for id_var in id_vars:\n",
    "        verify_single_id(id_var, properties)\n",
    "    \n",
    "    # 3. Vérifier les variables non classées\n",
    "    unclassified = find_unclassified_vars(id_vars)\n",
    "    \n",
    "    # 4. Afficher les statistiques générales\n",
    "    print_id_statistics(id_vars)\n",
    "    \n",
    "    # 5. Générer et afficher le résumé final\n",
    "    results = generate_results(id_vars, unclassified)\n",
    "    print_final_summary(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_single_id(id_var, properties):\n",
    "    \"\"\"Vérifie un identifiant spécifique.\"\"\"\n",
    "    print(f\"\\nVérification de {id_var}:\")\n",
    "    \n",
    "    # Vérifier l'unicité\n",
    "    if properties['unique']:\n",
    "        check_uniqueness(id_var)\n",
    "    \n",
    "    # Vérifier les valeurs nulles\n",
    "    if properties['not_null']:\n",
    "        check_null_values(id_var)\n",
    "    \n",
    "    # Vérifier le type de données\n",
    "    if properties['type'] == 'str':\n",
    "        check_data_type(id_var)\n",
    "    \n",
    "    # Vérifier la cohérence entre les ensembles\n",
    "    check_dataset_consistency(id_var)\n",
    "\n",
    "def check_uniqueness(id_var):\n",
    "    \"\"\"Vérifie l'unicité des identifiants.\"\"\"\n",
    "    n_unique = train_df[id_var].nunique()\n",
    "    n_total = len(train_df)\n",
    "    if n_unique != n_total:\n",
    "        print(f\"ATTENTION! {id_var} n'est pas unique:\")\n",
    "        print(f\"  {n_unique} valeurs uniques pour {n_total} lignes\")\n",
    "        duplicates = train_df[train_df[id_var].duplicated(keep=False)]\n",
    "        if not duplicates.empty:\n",
    "            print(\"  Exemples de doublons:\")\n",
    "            print(duplicates[id_var].head())\n",
    "\n",
    "def check_null_values(id_var):\n",
    "    \"\"\"Vérifie la présence de valeurs nulles.\"\"\"\n",
    "    n_null = train_df[id_var].isnull().sum()\n",
    "    if n_null > 0:\n",
    "        print(f\"ATTENTION! {n_null} valeurs nulles trouvées\")\n",
    "\n",
    "def check_data_type(id_var):\n",
    "    \"\"\"Vérifie le type de données.\"\"\"\n",
    "    if not pd.api.types.is_string_dtype(train_df[id_var]):\n",
    "        print(f\"ATTENTION! {id_var} n'est pas de type string\")\n",
    "        print(f\"Type actuel: {train_df[id_var].dtype}\")\n",
    "\n",
    "def check_dataset_consistency(id_var):\n",
    "    \"\"\"Vérifie la cohérence entre train, test et soumission.\"\"\"\n",
    "    train_ids = set(train_df[id_var])\n",
    "    test_ids = set(test_df[id_var])\n",
    "    submission_ids = set(data_results['submission_format'][id_var])\n",
    "    \n",
    "    # Vérifier l'intersection train/test\n",
    "    common_train_test = train_ids.intersection(test_ids)\n",
    "    if common_train_test:\n",
    "        print(f\"\\nATTENTION! IDs communs entre train et test: {len(common_train_test)}\")\n",
    "        print(\"Exemples:\", list(common_train_test)[:5])\n",
    "    \n",
    "    # Vérifier la cohérence test/soumission\n",
    "    check_submission_consistency(test_ids, submission_ids)\n",
    "    \n",
    "    return train_ids, test_ids, submission_ids, common_train_test\n",
    "\n",
    "def check_submission_consistency(test_ids, submission_ids):\n",
    "    \"\"\"Vérifie la cohérence avec le fichier de soumission.\"\"\"\n",
    "    missing_in_submission = test_ids - submission_ids\n",
    "    extra_in_submission = submission_ids - test_ids\n",
    "    if missing_in_submission or extra_in_submission:\n",
    "        print(\"\\nIncohérences entre test et soumission:\")\n",
    "        if missing_in_submission:\n",
    "            print(f\"IDs manquants dans soumission: {len(missing_in_submission)}\")\n",
    "        if extra_in_submission:\n",
    "            print(f\"IDs supplémentaires dans soumission: {len(extra_in_submission)}\")\n",
    "    return missing_in_submission, extra_in_submission\n",
    "\n",
    "def find_unclassified_vars(id_vars):\n",
    "    \"\"\"Identifie les variables d'identification non classées.\"\"\"\n",
    "    id_patterns = {\n",
    "        'identifiant': '^ID',\n",
    "        'reference': '^REF',\n",
    "        'numero': '^NUM'\n",
    "    }\n",
    "    \n",
    "    # Exclure les patterns d'autres groupes\n",
    "    other_patterns = ['INDEM', 'IND_', 'INDICE']\n",
    "    \n",
    "    unclassified = {}\n",
    "    for pattern_name, pattern in id_patterns.items():\n",
    "        pattern_vars = [col for col in train_df.columns \n",
    "                       if re.match(pattern, col) \n",
    "                       and col not in id_vars\n",
    "                       and not any(other in col for other in other_patterns)]\n",
    "        if pattern_vars:\n",
    "            unclassified[pattern_name] = pattern_vars\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées par type:\")\n",
    "        for pattern_name, vars_list in unclassified.items():\n",
    "            if vars_list:\n",
    "                print(f\"\\n{pattern_name.upper()}:\")\n",
    "                print(f\"Nombre: {len(vars_list)}\")\n",
    "                print(\"Variables:\", vars_list)\n",
    "    \n",
    "    return unclassified\n",
    "\n",
    "def print_id_statistics(id_vars):\n",
    "    \"\"\"Affiche les statistiques des identifiants.\"\"\"\n",
    "    print(\"\\n=== Statistiques des IDs ===\")\n",
    "    for id_var in id_vars:\n",
    "        print(f\"\\n{id_var}:\")\n",
    "        print(f\"Train: {train_df[id_var].nunique()} valeurs uniques\")\n",
    "        print(f\"Test: {test_df[id_var].nunique()} valeurs uniques\")\n",
    "        print(f\"Soumission: {data_results['submission_format'][id_var].nunique()} valeurs uniques\")\n",
    "        \n",
    "        if pd.api.types.is_string_dtype(train_df[id_var]):\n",
    "            train_lengths = train_df[id_var].str.len()\n",
    "            print(f\"Longueur min: {train_lengths.min()}\")\n",
    "            print(f\"Longueur max: {train_lengths.max()}\")\n",
    "            if train_lengths.min() != train_lengths.max():\n",
    "                print(\"ATTENTION! Longueurs variables\")\n",
    "\n",
    "def generate_results(id_vars, unclassified):\n",
    "    \"\"\"Génère le dictionnaire des résultats.\"\"\"\n",
    "    id_var = id_vars[0]  # Premier identifiant\n",
    "    train_ids = set(train_df[id_var])\n",
    "    test_ids = set(test_df[id_var])\n",
    "    submission_ids = set(data_results['submission_format'][id_var])\n",
    "    \n",
    "    total_unclassified = sum(len(vars_) for vars_ in unclassified.values())\n",
    "    \n",
    "    return {\n",
    "        'train_ids': train_ids,\n",
    "        'test_ids': test_ids,\n",
    "        'submission_ids': submission_ids,\n",
    "        'unclassified': unclassified,\n",
    "        'stats': {\n",
    "            'train_unique': len(train_ids),\n",
    "            'test_unique': len(test_ids),\n",
    "            'submission_unique': len(submission_ids),\n",
    "            'total_unclassified': total_unclassified\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_final_summary(results):\n",
    "    \"\"\"Affiche le résumé final.\"\"\"\n",
    "    print(\"\\n=== Résumé final ===\")\n",
    "    stats = results['stats']\n",
    "    print(f\"Train: {stats['train_unique']} IDs uniques\")\n",
    "    print(f\"Test: {stats['test_unique']} IDs uniques\")\n",
    "    print(f\"Soumission: {stats['submission_unique']} IDs uniques\")\n",
    "    print(f\"Variables non classées: {stats['total_unclassified']}\")\n",
    "\n",
    "# Exécuter la vérification\n",
    "id_results = verify_id_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_variable_classification():\n",
    "    print(\"=== Vérification globale de la classification des variables ===\")\n",
    "    \n",
    "    # 1. Collecter toutes les variables de chaque groupe\n",
    "    group_vars = {\n",
    "        'Weather': sum(WEATHER_VARS.values(), []),\n",
    "        'Building': sum(BUILDING_VARS.values(), []),\n",
    "        'Insurance': sum([v if isinstance(v, list) else sum(v.values(), []) \n",
    "                         for v in INSURANCE_VARS.values()], []),\n",
    "        'Geographic': sum([v if isinstance(v, list) else sum(v.values(), []) \n",
    "                          for v in GEOGRAPHIC_VARS.values()], []),\n",
    "        'Demographic': sum([sum(v.values(), []) for v in DEMOGRAPHIC_VARS.values()], []),\n",
    "        'Activity': sum(ACTIVITY_VARS.values(), []),\n",
    "        'Target': sum(TARGET_VARS['primary'].values(), []),\n",
    "        'ID': ID_VARS['identifiers']\n",
    "    }\n",
    "    \n",
    "    # 2. Vérifier les répétitions entre groupes\n",
    "    all_vars = set(train_df.columns)\n",
    "    classified_vars = set()\n",
    "    duplicates = {}\n",
    "    \n",
    "    for group, vars_list in group_vars.items():\n",
    "        # Vérifier les répétitions internes au groupe\n",
    "        internal_dupes = [v for v in vars_list if vars_list.count(v) > 1]\n",
    "        if internal_dupes:\n",
    "            print(f\"\\nRépétitions internes dans {group}:\")\n",
    "            print(set(internal_dupes))\n",
    "        \n",
    "        # Vérifier les répétitions avec d'autres groupes\n",
    "        for v in vars_list:\n",
    "            if v in classified_vars:\n",
    "                duplicates[v] = duplicates.get(v, []) + [group]\n",
    "            classified_vars.add(v)\n",
    "    \n",
    "    # 3. Identifier les variables non classées\n",
    "    unclassified = all_vars - classified_vars\n",
    "    \n",
    "    # 4. Afficher les résultats\n",
    "    print(\"\\n=== Résultats ===\")\n",
    "    print(f\"Total variables dans DataFrame: {len(all_vars)}\")\n",
    "    print(f\"Total variables classifiées: {len(classified_vars)}\")\n",
    "    print(f\"Variables non classées: {len(unclassified)}\")\n",
    "    \n",
    "    if duplicates:\n",
    "        print(\"\\nVariables présentes dans plusieurs groupes:\")\n",
    "        for var, groups in duplicates.items():\n",
    "            print(f\"- {var}: {groups}\")\n",
    "    \n",
    "    if unclassified:\n",
    "        print(\"\\nVariables non classées:\")\n",
    "        print(sorted(unclassified))\n",
    "    \n",
    "    return {\n",
    "        'group_stats': {group: len(vars_) for group, vars_ in group_vars.items()},\n",
    "        'duplicates': duplicates,\n",
    "        'unclassified': unclassified\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "classification_results = verify_variable_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1 : Fonction utilitaire\n",
    "def flatten_vars(var_dict):\n",
    "    \"\"\"\n",
    "    Aplatit un dictionnaire imbriqué de variables en une liste.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for k, v in var_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            for sub_v in v.values():\n",
    "                if isinstance(sub_v, list):\n",
    "                    flat_list.extend(sub_v)\n",
    "                elif isinstance(sub_v, dict):\n",
    "                    flat_list.extend(sum(sub_v.values(), []))\n",
    "        elif isinstance(v, list):\n",
    "            flat_list.extend(v)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 2 : Analyse initiale\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyse détaillée des valeurs manquantes par groupe et par tranche.\n",
    "    \"\"\"\n",
    "    groups = {\n",
    "        'Weather': sum(WEATHER_VARS.values(), []),\n",
    "        'Building': sum(BUILDING_VARS.values(), []),\n",
    "        'Activity': flatten_vars(ACTIVITY_VARS),\n",
    "        'Insurance': flatten_vars(INSURANCE_VARS),\n",
    "        'Geographic': flatten_vars(GEOGRAPHIC_VARS),\n",
    "        'Demographic': flatten_vars(DEMOGRAPHIC_VARS)\n",
    "    }\n",
    "    \n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': df.isnull().sum(),\n",
    "        'Missing Percentage': df.isnull().sum() / len(df) * 100\n",
    "    }).sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    print_missing_analysis(df, missing_summary, groups)\n",
    "    \n",
    "    return missing_summary, groups\n",
    "\n",
    "def print_missing_analysis(df, missing_summary, groups):\n",
    "    \"\"\"Affiche l'analyse des valeurs manquantes\"\"\"\n",
    "    ranges = {\n",
    "        '0%': (0, 0),\n",
    "        '< 10%': (0, 10),\n",
    "        '10-30%': (10, 30),\n",
    "        '30-50%': (30, 50),\n",
    "        '50-70%': (50, 70),\n",
    "        '70-90%': (70, 90),\n",
    "        '> 90%': (90, 100)\n",
    "    }\n",
    "    \n",
    "    total_cols = 0\n",
    "    print(\"=== Analyse des valeurs manquantes par tranche ===\\n\")\n",
    "    \n",
    "    for range_name, (min_val, max_val) in ranges.items():\n",
    "        range_cols = missing_summary[\n",
    "            missing_summary['Missing Percentage'].between(min_val, max_val, inclusive='right' if range_name != '0%' else 'both')\n",
    "        ]\n",
    "        \n",
    "        n_cols = len(range_cols)\n",
    "        total_cols += n_cols\n",
    "        \n",
    "        if n_cols > 0:\n",
    "            print(f\"\\n=== Tranche {range_name} ({n_cols} colonnes) ===\")\n",
    "            for group_name, cols in groups.items():\n",
    "                group_cols = [col for col in range_cols.index if col in cols]\n",
    "                if group_cols:\n",
    "                    print(f\"\\n{group_name}: {len(group_cols)} colonnes\")\n",
    "                    print(f\"Variables: {group_cols}\")\n",
    "    \n",
    "    print(f\"\\nTotal des colonnes analysées: {total_cols}\")\n",
    "    print(f\"Nombre total de colonnes: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analyse des valeurs manquantes par tranche ===\n",
      "\n",
      "\n",
      "=== Tranche 0% (96 colonnes) ===\n",
      "\n",
      "Building: 30 colonnes\n",
      "Variables: ['ADOSS', 'CARACT4', 'CARACT5', 'TYPBAT2', 'NBBAT14', 'NBBAT4', 'NBBAT5', 'NBBAT6', 'NBBAT7', 'NBBAT8', 'NBBAT9', 'NBBAT10', 'NBBAT11', 'NBBAT13', 'NBBAT3', 'NBBAT2', 'NBBAT1', 'SURFACE21', 'SURFACE1', 'SURFACE2', 'SURFACE4', 'SURFACE5', 'SURFACE6', 'SURFACE12', 'SURFACE13', 'SURFACE14', 'SURFACE15', 'SURFACE16', 'SURFACE17', 'SURFACE20']\n",
      "\n",
      "Activity: 9 colonnes\n",
      "Variables: ['ACTIVIT2', 'VOCATION', 'TYPERS', 'EQUIPEMENT7', 'COEFASS', 'EQUIPEMENT1', 'EQUIPEMENT3', 'EQUIPEMENT4', 'EQUIPEMENT6']\n",
      "\n",
      "Insurance: 55 colonnes\n",
      "Variables: ['AN_EXERC', 'KAPITAL7', 'CA1', 'CA2', 'CA3', 'KAPITAL1', 'KAPITAL2', 'KAPITAL3', 'KAPITAL4', 'KAPITAL5', 'KAPITAL6', 'KAPITAL8', 'KAPITAL34', 'KAPITAL9', 'KAPITAL10', 'KAPITAL12', 'KAPITAL13', 'KAPITAL14', 'KAPITAL25', 'KAPITAL26', 'KAPITAL27', 'KAPITAL28', 'TAILLE2', 'TAILLE1', 'DEROG15', 'DEROG11', 'ANCIENNETE', 'INDEM1', 'FRCH1', 'DEROG1', 'DEROG2', 'DEROG3', 'DEROG4', 'DEROG5', 'DEROG6', 'DEROG7', 'DEROG8', 'DEROG9', 'DEROG10', 'KAPITAL32', 'KAPITAL35', 'TAILLE3', 'KAPITAL36', 'TAILLE4', 'NBSINCONJ', 'NBSINSTRT', 'RISK11', 'KAPITAL37', 'KAPITAL38', 'KAPITAL39', 'KAPITAL40', 'KAPITAL41', 'KAPITAL42', 'KAPITAL43', 'ANNEE_ASSURANCE']\n",
      "\n",
      "Geographic: 1 colonnes\n",
      "Variables: ['ZONE']\n",
      "\n",
      "=== Tranche < 10% (84 colonnes) ===\n",
      "\n",
      "Weather: 4 colonnes\n",
      "Variables: ['FFM_VOR_COM_MM_A_Y', 'FFM_VOR_COM_MMAX_A_Y', 'FXI3SAB_VOR_COM_MM_A_Y', 'FXI3SAB_VOR_COM_MMAX_A_Y']\n",
      "\n",
      "Building: 4 colonnes\n",
      "Variables: ['SURFACE9', 'SURFACE10', 'CARACT1', 'SURFACE7']\n",
      "\n",
      "Insurance: 27 colonnes\n",
      "Variables: ['INDEM2', 'RISK1', 'RISK2', 'RISK4', 'RISK3', 'RISK12', 'RISK5', 'RISK6', 'FRCH2', 'RISK7', 'RISK8', 'RISK9', 'DUREE_REQANEUF', 'KAPITAL21', 'KAPITAL29', 'KAPITAL30', 'KAPITAL22', 'KAPITAL16', 'KAPITAL23', 'KAPITAL19', 'KAPITAL17', 'KAPITAL24', 'KAPITAL15', 'KAPITAL20', 'KAPITAL18', 'KAPITAL31', 'KAPITAL33']\n",
      "\n",
      "Geographic: 21 colonnes\n",
      "Variables: ['PROPORTION_21', 'PROPORTION_11', 'PROPORTION_42', 'NB_CASERNES', 'PROPORTION_41', 'ZONE_VENT', 'PROPORTION_13', 'PROPORTION_22', 'PROPORTION_23', 'PROPORTION_24', 'PROPORTION_31', 'PROPORTION_32', 'PROPORTION_33', 'PROPORTION_12', 'PROPORTION_51', 'ALTITUDE_3', 'PROPORTION_52', 'ALTITUDE_5', 'ALTITUDE_4', 'PROPORTION_14', 'ALTITUDE_2']\n",
      "\n",
      "Demographic: 28 colonnes\n",
      "Variables: ['MEN_MAIS', 'MEN_PROP', 'IND_Y7_Y8', 'MEN_SURF', 'IND_SNV', 'IND_INC', 'IND_Y9', 'IND_Y8_Y9', 'LOG_A1_A2', 'IND_Y6_Y7', 'IND_Y5_Y6', 'MEN', 'MEN_1IND', 'MEN_5IND', 'MEN_FMP', 'MEN_COLL', 'LOG_AVA1', 'LOG_A2_A3', 'LOG_APA3', 'LOG_INC', 'LOG_SOC', 'IND', 'IND_0_Y1', 'IND_Y1_Y2', 'IND_Y2_Y3', 'IND_Y3_Y4', 'IND_Y4_Y5', 'MEN_PAUV']\n",
      "\n",
      "=== Tranche 10-30% (6 colonnes) ===\n",
      "\n",
      "Building: 5 colonnes\n",
      "Variables: ['SURFACE8', 'SURFACE11', 'SURFACE3', 'SURFACE19', 'SURFACE18']\n",
      "\n",
      "Activity: 1 colonnes\n",
      "Variables: ['EQUIPEMENT2']\n",
      "\n",
      "=== Tranche 30-50% (5 colonnes) ===\n",
      "\n",
      "Activity: 1 colonnes\n",
      "Variables: ['EQUIPEMENT5']\n",
      "\n",
      "Insurance: 3 colonnes\n",
      "Variables: ['RISK13', 'RISK10', 'KAPITAL11']\n",
      "\n",
      "Geographic: 1 colonnes\n",
      "Variables: ['ESPINSEE']\n",
      "\n",
      "=== Tranche 50-70% (176 colonnes) ===\n",
      "\n",
      "Weather: 126 colonnes\n",
      "Variables: ['NBJTMS24_MM_A', 'NBJTNS20_MMAX_A', 'TMMAX_VOR_MM_A', 'TMM_VOR_MMAX_A', 'TMM_VOR_MM_A', 'TM_VOR_MMAX_A', 'TM_VOR_MM_A', 'TAMPLIM_VOR_MMAX_A', 'TAMPLIM_VOR_MM_A', 'TAMPLIAB_VOR_MMAX_A', 'TAMPLIAB_VOR_MM_A', 'NBJTMS24_MSOM_A', 'NBJTMS24_MMAX_A', 'NBJTX30_MMAX_A', 'NBJTX35_MM_A', 'NBJTX35_MMAX_A', 'NBJTX35_MSOM_A', 'NBJTN10_MM_A', 'NBJTN10_MMAX_A', 'NBJTN10_MSOM_A', 'NBJTNI10_MM_A', 'NBJTNI10_MMAX_A', 'NBJTNI10_MSOM_A', 'NBJTN5_MM_A', 'NBJTN5_MMAX_A', 'NBJTN5_MSOM_A', 'NBJTNS25_MM_A', 'NBJTNS25_MMAX_A', 'NBJTNS25_MSOM_A', 'NBJTNI15_MM_A', 'NBJTNI15_MMAX_A', 'NBJTNI15_MSOM_A', 'NBJTNI20_MM_A', 'NBJTNI20_MMAX_A', 'NBJTNI20_MSOM_A', 'NBJTX30_MSOM_A', 'NBJTX30_MM_A', 'NBJTXI20_MSOM_A', 'TMMIN_VOR_MM_A', 'NBJTNS20_MSOM_A', 'NBJTX25_MM_A', 'NBJTX25_MMAX_A', 'NBJTX25_MSOM_A', 'NBJTX0_MM_A', 'NBJTX0_MMAX_A', 'NBJTX0_MSOM_A', 'NBJTXI27_MM_A', 'NBJTXI27_MMAX_A', 'NBJTXI27_MSOM_A', 'NBJTXS32_MM_A', 'NBJTXS32_MMAX_A', 'NBJTXS32_MSOM_A', 'NBJTXI20_MM_A', 'NBJTXI20_MMAX_A', 'TMMAX_VOR_MMAX_A', 'NBJTNS20_MM_A', 'TMMIN_VOR_MMAX_A', 'NBJRR30_MM_A', 'NBJRR30_MSOM_A', 'NBJRR100_MM_A', 'NBJRR100_MMAX_A', 'FXI3SAB_VOR_MMAX_A', 'NBJFXY15_MM_A', 'NBJFXY10_MSOM_A', 'NBJFXY10_MMAX_A', 'NBJFXY10_MM_A', 'RR_VOR_MM_A', 'RR_VOR_MMAX_A', 'NBJFXY8_MSOM_A', 'NBJFXY8_MMAX_A', 'NBJFXY8_MM_A', 'NBJFXI3S28_MSOM_A', 'NBJFXI3S28_MMAX_A', 'NBJFXI3S28_MM_A', 'NBJFXI3S16_MSOM_A', 'NBJFXI3S16_MMAX_A', 'NBJFXI3S16_MM_A', 'NBJRR30_MMAX_A', 'NBJRR10_MSOM_A', 'NBJFXI3S10_MMAX_A', 'NBJRR10_MMAX_A', 'FXIAB_VOR_MM_A', 'FXIAB_VOR_MMAX_A', 'FFM_VOR_MMAX_A', 'FFM_VOR_MM_A', 'NBJFXY15_MSOM_A', 'FXYAB_VOR_MM_A', 'FXYAB_VOR_MMAX_A', 'NBJFXY15_MMAX_A', 'NBJRR50_MM_A', 'NBJRR50_MMAX_A', 'TN_VOR_MM_A', 'NBJRR50_MSOM_A', 'NBJRR1_MM_A', 'NBJRR1_MMAX_A', 'NBJRR1_MSOM_A', 'NBJRR5_MM_A', 'NBJRR5_MMAX_A', 'NBJRR5_MSOM_A', 'NBJRR10_MM_A', 'NBJFXI3S10_MSOM_A', 'NBJRR100_MSOM_A', 'NBJFXI3S10_MM_A', 'NBJFF10_MMAX_A', 'TN_VOR_MMAX_A', 'TNAB_VOR_MM_A', 'TNAB_VOR_MMAX_A', 'TNMAX_VOR_MM_A', 'TNMAX_VOR_MMAX_A', 'TX_VOR_MM_A', 'NBJFF28_MSOM_A', 'TX_VOR_MMAX_A', 'TXAB_VOR_MM_A', 'TXAB_VOR_MMAX_A', 'TXMIN_VOR_MM_A', 'TXMIN_VOR_MMAX_A', 'NBJFF10_MM_A', 'FXI3SAB_VOR_MM_A', 'NBJFF10_MSOM_A', 'NBJFF16_MMAX_A', 'NBJFF28_MM_A', 'NBJFF16_MSOM_A', 'RRAB_VOR_MM_A', 'NBJFF16_MM_A', 'NBJFF28_MMAX_A', 'RRAB_VOR_MMAX_A']\n",
      "\n",
      "Building: 4 colonnes\n",
      "Variables: ['HAUTEUR_MAX', 'HAUTEUR', 'BDTOPO_BAT_MAX_HAUTEUR_MAX', 'BDTOPO_BAT_MAX_HAUTEUR']\n",
      "\n",
      "Geographic: 46 colonnes\n",
      "Variables: ['DISTANCE_311', 'DISTANCE_313', 'DISTANCE_312', 'DISTANCE_324', 'DISTANCE_2', 'DISTANCE_244', 'DISTANCE_412', 'DISTANCE_222', 'DISTANCE_221', 'DISTANCE_223', 'DISTANCE_231', 'DISTANCE_242', 'DISTANCE_243', 'DISTANCE_411', 'DISTANCE_321', 'DISTANCE_322', 'DISTANCE_335', 'DISTANCE_213', 'DISTANCE_331', 'DISTANCE_332', 'DISTANCE_333', 'DISTANCE_334', 'DISTANCE_323', 'DISTANCE_212', 'DISTANCE_111', 'DISTANCE_422', 'DISTANCE_112', 'DISTANCE_121', 'DISTANCE_122', 'DISTANCE_123', 'DISTANCE_124', 'DISTANCE_131', 'DISTANCE_132', 'DISTANCE_133', 'DISTANCE_141', 'DISTANCE_142', 'DISTANCE_211', 'DISTANCE_421', 'DISTANCE_523', 'DISTANCE_423', 'DISTANCE_511', 'DISTANCE_512', 'DISTANCE_521', 'DISTANCE_522', 'DISTANCE_1', 'ALTITUDE_1']\n",
      "\n",
      "=== Tranche > 90% (7 colonnes) ===\n",
      "\n",
      "Building: 3 colonnes\n",
      "Variables: ['CARACT2', 'CARACT3', 'TYPBAT1']\n",
      "\n",
      "Insurance: 4 colonnes\n",
      "Variables: ['DEROG14', 'DEROG13', 'DEROG16', 'DEROG12']\n",
      "\n",
      "Total des colonnes analysées: 374\n",
      "Nombre total de colonnes: 374\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3 : Préparation des données\n",
    "exclude_cols = ['FREQ', 'CM', 'CHARGE']\n",
    "X_train = train_df.drop(exclude_cols, axis=1)\n",
    "y_train = train_df[['FREQ', 'CM', 'CHARGE']]\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# Analyse initiale\n",
    "missing_summary, groups = analyze_missing_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement des variables météo:\n",
      "\n",
      "Colonne NBJTX25_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "[5.0 7.0 '04. >= 7' 4.0]\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [4.0, 5.0, 7.0]\n",
      "\n",
      "Colonne NBJTX25_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 23' '04. >= 23' '02. <= 18' '01. <= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 23.0\n",
      "- Exemples de valeurs après traitement: [12.0, 18.0, 23.0]\n",
      "\n",
      "Colonne NBJTX25_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 65' '03. <= 88' '04. >= 88' '01. <= 44']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 65.0\n",
      "- Exemples de valeurs après traitement: [44.0, 65.0, 88.0]\n",
      "\n",
      "Colonne NBJTX0_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 1' '02. <= 1' '01. <= 0' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJTX0_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 7' '02. <= 4' '01. <= 2' '04. >= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [2.0, 4.0, 7.0]\n",
      "\n",
      "Colonne NBJTX0_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 18' '02. <= 10' '01. <= 4' '04. >= 18']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [4.0, 10.0, 18.0]\n",
      "\n",
      "Colonne NBJTXI27_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 27' '02. <= 26' '04. >= 27' '01. <= 24']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 27.0\n",
      "- Exemples de valeurs après traitement: [24.0, 26.0, 27.0]\n",
      "\n",
      "Colonne NBJTXI27_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 30' '02. <= 30' '04. >= 30' '01. <= 15']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 30.0\n",
      "- Exemples de valeurs après traitement: [15.0, 30.0]\n",
      "\n",
      "Colonne NBJTXI27_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 329' '02. <= 311' '04. >= 329' '01. <= 292']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 329.0\n",
      "- Exemples de valeurs après traitement: [292.0, 311.0, 329.0]\n",
      "\n",
      "Colonne NBJTXS32_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 1' '03. <= 2' '01. <= 1' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0]\n",
      "\n",
      "Colonne NBJTXS32_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 7' '03. <= 11' '01. <= 4' '04. >= 11']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 7.0\n",
      "- Exemples de valeurs après traitement: [4.0, 7.0, 11.0]\n",
      "\n",
      "Colonne NBJTXS32_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 16' '03. <= 27' '01. <= 9' '04. >= 27']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 16.0\n",
      "- Exemples de valeurs après traitement: [9.0, 16.0, 27.0]\n",
      "\n",
      "Colonne NBJTXI20_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 21' '02. <= 19' '01. <= 17' '04. >= 21']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 19.0\n",
      "- Exemples de valeurs après traitement: [17.0, 19.0, 21.0]\n",
      "\n",
      "Colonne NBJTXI20_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 30' '03. <= 30' '02. <= 30' '01. <= 15']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 30.0\n",
      "- Exemples de valeurs après traitement: [15.0, 30.0]\n",
      "\n",
      "Colonne NBJTXI20_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 256' '02. <= 232' '01. <= 207' '04. >= 256']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 232.0\n",
      "- Exemples de valeurs après traitement: [207.0, 232.0, 256.0]\n",
      "\n",
      "Colonne NBJTX30_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 2' '03. <= 4' '01. <= 1' '04. >= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 2.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0, 4.0]\n",
      "\n",
      "Colonne NBJTX30_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 11' '03. <= 17' '01. <= 6' '04. >= 17']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 11.0\n",
      "- Exemples de valeurs après traitement: [6.0, 11.0, 17.0]\n",
      "\n",
      "Colonne NBJTX30_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 28' '03. <= 44' '01. <= 15' '04. >= 44']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 28.0\n",
      "- Exemples de valeurs après traitement: [15.0, 28.0, 44.0]\n",
      "\n",
      "Colonne NBJTX35_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 0' '03. <= 1' '01. <= 0' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJTX35_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 3' '03. <= 5' '04. >= 5' '01. <= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 3.0\n",
      "- Exemples de valeurs après traitement: [2.0, 3.0, 5.0]\n",
      "\n",
      "Colonne NBJTX35_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 5' '03. <= 9' '01. <= 2' '04. >= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [2.0, 5.0, 9.0]\n",
      "\n",
      "Colonne TX_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 18' '02. <= 15' '04. >= 18' '01. <= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 18.0\n",
      "- Exemples de valeurs après traitement: [7.0, 15.0, 18.0]\n",
      "\n",
      "Colonne TX_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 27' '03. <= 27' '02. <= 24' '01. <= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 27.0\n",
      "- Exemples de valeurs après traitement: [12.0, 24.0, 27.0]\n",
      "\n",
      "Colonne TXAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 25' '04. >= 25' '02. <= 22' '01. <= 11']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 25.0\n",
      "- Exemples de valeurs après traitement: [11.0, 22.0, 25.0]\n",
      "\n",
      "Colonne TXAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 35' '04. >= 35' '02. <= 32' '01. <= 16']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 35.0\n",
      "- Exemples de valeurs après traitement: [16.0, 32.0, 35.0]\n",
      "\n",
      "Colonne TXMIN_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 9' '03. <= 11' '01. <= 6' '04. >= 11']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 9.0\n",
      "- Exemples de valeurs après traitement: [6.0, 9.0, 11.0]\n",
      "\n",
      "Colonne TXMIN_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 22' '02. <= 19' '04. >= 22' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 22.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 22.0]\n",
      "\n",
      "Colonne NBJTN10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 1' '02. <= 0' '01. <= 0' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJTN10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 5' '02. <= 2' '01. <= 1' '04. >= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0, 5.0]\n",
      "\n",
      "Colonne NBJTN10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 12' '02. <= 5' '01. <= 2' '04. >= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 2.0\n",
      "- Exemples de valeurs après traitement: [2.0, 5.0, 12.0]\n",
      "\n",
      "Colonne NBJTNI10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 22' '02. <= 19' '01. <= 18' '04. >= 22']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 22.0\n",
      "- Exemples de valeurs après traitement: [18.0, 19.0, 22.0]\n",
      "\n",
      "Colonne NBJTNI10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 30' '03. <= 30' '02. <= 29' '01. <= 15']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 30.0\n",
      "- Exemples de valeurs après traitement: [15.0, 29.0, 30.0]\n",
      "\n",
      "Colonne NBJTNI10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 260' '02. <= 233' '01. <= 210' '04. >= 260']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 260.0\n",
      "- Exemples de valeurs après traitement: [210.0, 233.0, 260.0]\n",
      "\n",
      "Colonne NBJTN5_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 3' '02. <= 2' '01. <= 1' '04. >= 3']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0, 3.0]\n",
      "\n",
      "Colonne NBJTN5_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 10' '02. <= 6' '04. >= 10' '01. <= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 6.0\n",
      "- Exemples de valeurs après traitement: [4.0, 6.0, 10.0]\n",
      "\n",
      "Colonne NBJTN5_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 33' '02. <= 19' '01. <= 9' '04. >= 33']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 9.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 33.0]\n",
      "\n",
      "Colonne NBJTNS25_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 0' '03. <= 0' '04. >= 0']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0]\n",
      "\n",
      "Colonne NBJTNS25_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 0' '03. <= 1' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJTNS25_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 0' '03. <= 1' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJTNI15_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 27' '03. <= 27' '02. <= 26' '01. <= 23']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 27.0\n",
      "- Exemples de valeurs après traitement: [23.0, 26.0, 27.0]\n",
      "\n",
      "Colonne NBJTNI15_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 30' '04. >= 30' '02. <= 30' '01. <= 15']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 30.0\n",
      "- Exemples de valeurs après traitement: [15.0, 30.0]\n",
      "\n",
      "Colonne NBJTNI15_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 326' '03. <= 326' '02. <= 309' '01. <= 282']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 326.0\n",
      "- Exemples de valeurs après traitement: [282.0, 309.0, 326.0]\n",
      "\n",
      "Colonne NBJTNI20_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 29' '03. <= 29' '02. <= 29' '01. <= 14']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 29.0\n",
      "- Exemples de valeurs après traitement: [14.0, 29.0]\n",
      "\n",
      "Colonne NBJTNI20_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 30' '03. <= 30' '02. <= 23' '01. <= 8']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 30.0\n",
      "- Exemples de valeurs après traitement: [8.0, 23.0, 30.0]\n",
      "\n",
      "Colonne NBJTNI20_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 350' '03. <= 350' '02. <= 344' '01. <= 169']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 350.0\n",
      "- Exemples de valeurs après traitement: [169.0, 344.0, 350.0]\n",
      "\n",
      "Colonne NBJTNS20_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 1' '03. <= 2' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0, 2.0]\n",
      "\n",
      "Colonne NBJTNS20_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 1' '02. <= 3' '03. <= 7' '04. >= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 3.0, 7.0]\n",
      "\n",
      "Colonne NBJTNS20_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 3' '02. <= 9' '03. <= 20' '04. >= 20']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 3.0\n",
      "- Exemples de valeurs après traitement: [3.0, 9.0, 20.0]\n",
      "\n",
      "Colonne TN_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 7' '03. <= 8' '04. >= 8' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [5.0, 7.0, 8.0]\n",
      "\n",
      "Colonne TN_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 15' '04. >= 15' '02. <= 13' '01. <= 6']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 15.0\n",
      "- Exemples de valeurs après traitement: [6.0, 13.0, 15.0]\n",
      "\n",
      "Colonne TNAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 1' '03. <= 2' '04. >= 2' '01. <= -1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 2.0\n",
      "- Exemples de valeurs après traitement: [-1.0, 1.0, 2.0]\n",
      "\n",
      "Colonne TNAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 9' '03. <= 11' '04. >= 11' '01. <= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 11.0\n",
      "- Exemples de valeurs après traitement: [7.0, 9.0, 11.0]\n",
      "\n",
      "Colonne TNMAX_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 11' '04. >= 13' '03. <= 13' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 13.0\n",
      "- Exemples de valeurs après traitement: [5.0, 11.0, 13.0]\n",
      "\n",
      "Colonne TNMAX_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 19' '04. >= 20' '03. <= 20' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 20.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 20.0]\n",
      "\n",
      "Colonne NBJTMS24_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 1' '03. <= 3' '01. <= 1' '04. >= 3']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 3.0]\n",
      "\n",
      "Colonne NBJTMS24_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 7' '03. <= 14' '01. <= 4' '04. >= 14']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 7.0\n",
      "- Exemples de valeurs après traitement: [4.0, 7.0, 14.0]\n",
      "\n",
      "Colonne NBJTMS24_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 16' '03. <= 30' '01. <= 8' '04. >= 30']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 16.0\n",
      "- Exemples de valeurs après traitement: [8.0, 16.0, 30.0]\n",
      "\n",
      "Colonne TM_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 12' '04. >= 12' '02. <= 11' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 12.0\n",
      "- Exemples de valeurs après traitement: [5.0, 11.0, 12.0]\n",
      "\n",
      "Colonne TM_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 21' '04. >= 21' '02. <= 19' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 21.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 21.0]\n",
      "\n",
      "Colonne TMM_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 12' '04. >= 12' '02. <= 9' '01. <= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 12.0\n",
      "- Exemples de valeurs après traitement: [4.0, 9.0, 12.0]\n",
      "\n",
      "Colonne TMM_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 21' '04. >= 21' '02. <= 18' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 21.0\n",
      "- Exemples de valeurs après traitement: [9.0, 18.0, 21.0]\n",
      "\n",
      "Colonne TMMAX_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 17' '03. <= 18' '04. >= 18' '01. <= 8']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 18.0\n",
      "- Exemples de valeurs après traitement: [8.0, 17.0, 18.0]\n",
      "\n",
      "Colonne TMMAX_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 26' '04. >= 26' '02. <= 25' '01. <= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 26.0\n",
      "- Exemples de valeurs après traitement: [12.0, 25.0, 26.0]\n",
      "\n",
      "Colonne TMMIN_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 6' '03. <= 8' '01. <= 4' '04. >= 8']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [4.0, 6.0, 8.0]\n",
      "\n",
      "Colonne TMMIN_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 17' '04. >= 17' '02. <= 15' '01. <= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 17.0\n",
      "- Exemples de valeurs après traitement: [7.0, 15.0, 17.0]\n",
      "\n",
      "Colonne TAMPLIAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 18' '03. <= 18' '02. <= 15' '01. <= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 18.0\n",
      "- Exemples de valeurs après traitement: [7.0, 15.0, 18.0]\n",
      "\n",
      "Colonne TAMPLIAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 22' '03. <= 22' '02. <= 19' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 22.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 22.0]\n",
      "\n",
      "Colonne TAMPLIM_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 10' '03. <= 10' '02. <= 9' '01. <= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 10.0\n",
      "- Exemples de valeurs après traitement: [4.0, 9.0, 10.0]\n",
      "\n",
      "Colonne TAMPLIM_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 14' '03. <= 14' '02. <= 12' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 14.0\n",
      "- Exemples de valeurs après traitement: [5.0, 12.0, 14.0]\n",
      "\n",
      "Colonne NBJFF10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 12' '02. <= 15' '03. <= 18' '04. >= 18']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 18.0\n",
      "- Exemples de valeurs après traitement: [12.0, 15.0, 18.0]\n",
      "\n",
      "Colonne NBJFF10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 19' '02. <= 21' '03. <= 24' '04. >= 24']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 24.0\n",
      "- Exemples de valeurs après traitement: [19.0, 21.0, 24.0]\n",
      "\n",
      "Colonne NBJFF10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 147' '02. <= 178' '03. <= 211' '04. >= 211']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 211.0\n",
      "- Exemples de valeurs après traitement: [147.0, 178.0, 211.0]\n",
      "\n",
      "Colonne NBJFF16_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 3' '02. <= 5' '03. <= 7' '04. >= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [3.0, 5.0, 7.0]\n",
      "\n",
      "Colonne NBJFF16_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 8' '02. <= 10' '03. <= 14' '04. >= 14']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 10.0\n",
      "- Exemples de valeurs après traitement: [8.0, 10.0, 14.0]\n",
      "\n",
      "Colonne NBJFF16_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 41' '02. <= 59' '03. <= 86' '04. >= 86']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 59.0\n",
      "- Exemples de valeurs après traitement: [41.0, 59.0, 86.0]\n",
      "\n",
      "Colonne NBJFF28_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 1' '01. <= 0' '03. <= 1' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJFF28_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 1' '02. <= 2' '03. <= 2' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0]\n",
      "\n",
      "Colonne NBJFF28_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 8' '01. <= 5' '03. <= 12' '04. >= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [5.0, 8.0, 12.0]\n",
      "\n",
      "Colonne NBJFXI3S10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 11' '02. <= 13' '03. <= 16' '04. >= 16']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 13.0\n",
      "- Exemples de valeurs après traitement: [11.0, 13.0, 16.0]\n",
      "\n",
      "Colonne NBJFXI3S10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 18' '02. <= 20' '03. <= 22' '04. >= 22']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 20.0\n",
      "- Exemples de valeurs après traitement: [18.0, 20.0, 22.0]\n",
      "\n",
      "Colonne NBJFXI3S10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 135' '02. <= 161' '03. <= 189' '04. >= 189']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 161.0\n",
      "- Exemples de valeurs après traitement: [135.0, 161.0, 189.0]\n",
      "\n",
      "Colonne NBJFXI3S16_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 3' '02. <= 4' '04. >= 6' '03. <= 6']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 3.0\n",
      "- Exemples de valeurs après traitement: [3.0, 4.0, 6.0]\n",
      "\n",
      "Colonne NBJFXI3S16_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 6' '02. <= 9' '03. <= 12' '04. >= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 9.0\n",
      "- Exemples de valeurs après traitement: [6.0, 9.0, 12.0]\n",
      "\n",
      "Colonne NBJFXI3S16_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 34' '02. <= 48' '04. >= 67' '03. <= 67']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 34.0\n",
      "- Exemples de valeurs après traitement: [34.0, 48.0, 67.0]\n",
      "\n",
      "Colonne NBJFXI3S28_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 1' '03. <= 1' '01. <= 0' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJFXI3S28_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 1' '03. <= 2' '02. <= 1' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0]\n",
      "\n",
      "Colonne NBJFXI3S28_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 7' '03. <= 10' '01. <= 4' '04. >= 10']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 7.0\n",
      "- Exemples de valeurs après traitement: [4.0, 7.0, 10.0]\n",
      "\n",
      "Colonne NBJFXY8_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 6' '02. <= 8' '03. <= 11' '04. >= 11']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [6.0, 8.0, 11.0]\n",
      "\n",
      "Colonne NBJFXY8_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 10' '02. <= 14' '03. <= 17' '04. >= 17']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 17.0\n",
      "- Exemples de valeurs après traitement: [10.0, 14.0, 17.0]\n",
      "\n",
      "Colonne NBJFXY8_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 70' '02. <= 99' '03. <= 136' '04. >= 136']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 99.0\n",
      "- Exemples de valeurs après traitement: [70.0, 99.0, 136.0]\n",
      "\n",
      "Colonne NBJFXY10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 3' '02. <= 4' '03. <= 7' '04. >= 7']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [3.0, 4.0, 7.0]\n",
      "\n",
      "Colonne NBJFXY10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 6' '02. <= 9' '03. <= 12' '04. >= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 9.0\n",
      "- Exemples de valeurs après traitement: [6.0, 9.0, 12.0]\n",
      "\n",
      "Colonne NBJFXY10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 34' '02. <= 52' '03. <= 81' '04. >= 81']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 52.0\n",
      "- Exemples de valeurs après traitement: [34.0, 52.0, 81.0]\n",
      "\n",
      "Colonne NBJFXY15_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 1' '02. <= 1' '03. <= 2' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0]\n",
      "\n",
      "Colonne NBJFXY15_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 2' '02. <= 3' '03. <= 5' '04. >= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 2.0\n",
      "- Exemples de valeurs après traitement: [2.0, 3.0, 5.0]\n",
      "\n",
      "Colonne NBJFXY15_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 8' '02. <= 15' '03. <= 25' '04. >= 25']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [8.0, 15.0, 25.0]\n",
      "\n",
      "Colonne FFM_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 1' '02. <= 3' '03. <= 4' '04. >= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [1.0, 3.0, 4.0]\n",
      "\n",
      "Colonne FFM_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 2' '02. <= 4' '03. <= 5' '04. >= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [2.0, 4.0, 5.0]\n",
      "\n",
      "Colonne FXI3SAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 8' '02. <= 17' '03. <= 24' '04. >= 24']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [8.0, 17.0, 24.0]\n",
      "\n",
      "Colonne FXI3SAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 11' '02. <= 24' '03. <= 32' '04. >= 32']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 11.0\n",
      "- Exemples de valeurs après traitement: [11.0, 24.0, 32.0]\n",
      "\n",
      "Colonne FXIAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 19' '03. <= 22' '04. >= 22' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 19.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 22.0]\n",
      "\n",
      "Colonne FXIAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 26' '03. <= 29' '04. >= 29' '01. <= 12']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 29.0\n",
      "- Exemples de valeurs après traitement: [12.0, 26.0, 29.0]\n",
      "\n",
      "Colonne FXYAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 10' '03. <= 12' '04. >= 12' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 12.0\n",
      "- Exemples de valeurs après traitement: [5.0, 10.0, 12.0]\n",
      "\n",
      "Colonne FXYAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 14' '03. <= 17' '04. >= 17' '01. <= 6']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 17.0\n",
      "- Exemples de valeurs après traitement: [6.0, 14.0, 17.0]\n",
      "\n",
      "Colonne FFM_VOR_COM_MM_A_Y:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 3' '03. <= 4' '01. <= 1' '04. >= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [1.0, 3.0, 4.0]\n",
      "\n",
      "Colonne FFM_VOR_COM_MMAX_A_Y:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 4' '01. <= 2' '03. <= 5' '04. >= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [2.0, 4.0, 5.0]\n",
      "\n",
      "Colonne FXI3SAB_VOR_COM_MM_A_Y:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 4' '03. <= 23' '02. <= 12' '04. >= 23']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 4.0\n",
      "- Exemples de valeurs après traitement: [4.0, 12.0, 23.0]\n",
      "\n",
      "Colonne FXI3SAB_VOR_COM_MMAX_A_Y:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 5' '03. <= 30' '02. <= 17' '04. >= 30']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 5.0\n",
      "- Exemples de valeurs après traitement: [5.0, 17.0, 30.0]\n",
      "\n",
      "Colonne NBJRR50_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 0' '02. <= 0' '01. <= 0' '04. >= 0']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0]\n",
      "\n",
      "Colonne NBJRR50_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 1' '01. <= 1' '02. <= 1' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0]\n",
      "\n",
      "Colonne NBJRR50_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 5' '02. <= 3' '01. <= 1' '04. >= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 3.0, 5.0]\n",
      "\n",
      "Colonne NBJRR1_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 11' '02. <= 10' '04. >= 11' '01. <= 8']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 11.0\n",
      "- Exemples de valeurs après traitement: [8.0, 10.0, 11.0]\n",
      "\n",
      "Colonne NBJRR1_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 19' '02. <= 17' '04. >= 19' '01. <= 13']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 19.0\n",
      "- Exemples de valeurs après traitement: [13.0, 17.0, 19.0]\n",
      "\n",
      "Colonne NBJRR1_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 132' '02. <= 117' '04. >= 132' '01. <= 90']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 132.0\n",
      "- Exemples de valeurs après traitement: [90.0, 117.0, 132.0]\n",
      "\n",
      "Colonne NBJRR5_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 6' '03. <= 6' '02. <= 5' '01. <= 4']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 6.0\n",
      "- Exemples de valeurs après traitement: [4.0, 5.0, 6.0]\n",
      "\n",
      "Colonne NBJRR5_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 12' '03. <= 12' '01. <= 9' '02. <= 10']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 10.0\n",
      "- Exemples de valeurs après traitement: [9.0, 10.0, 12.0]\n",
      "\n",
      "Colonne NBJRR5_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 71' '03. <= 71' '02. <= 58' '01. <= 46']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 71.0\n",
      "- Exemples de valeurs après traitement: [46.0, 58.0, 71.0]\n",
      "\n",
      "Colonne NBJRR10_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 4' '03. <= 4' '01. <= 2' '02. <= 3']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 3.0\n",
      "- Exemples de valeurs après traitement: [2.0, 3.0, 4.0]\n",
      "\n",
      "Colonne NBJRR10_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 8' '02. <= 7' '01. <= 5' '03. <= 8']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 7.0\n",
      "- Exemples de valeurs après traitement: [5.0, 7.0, 8.0]\n",
      "\n",
      "Colonne NBJRR10_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 45' '03. <= 45' '01. <= 29' '02. <= 36']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 36.0\n",
      "- Exemples de valeurs après traitement: [29.0, 36.0, 45.0]\n",
      "\n",
      "Colonne NBJRR30_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 1' '03. <= 1' '02. <= 1' '01. <= 0']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJRR30_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 3' '02. <= 2' '04. >= 3' '01. <= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 1.0\n",
      "- Exemples de valeurs après traitement: [1.0, 2.0, 3.0]\n",
      "\n",
      "Colonne NBJRR30_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 11' '03. <= 11' '02. <= 8' '01. <= 5']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 8.0\n",
      "- Exemples de valeurs après traitement: [5.0, 8.0, 11.0]\n",
      "\n",
      "Colonne NBJRR100_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 0' '03. <= 0' '04. >= 0']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0]\n",
      "\n",
      "Colonne NBJRR100_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 0' '03. <= 1' '04. >= 1']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0]\n",
      "\n",
      "Colonne NBJRR100_MSOM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['01. <= 0' '02. <= 1' '03. <= 2' '04. >= 2']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 0.0\n",
      "- Exemples de valeurs après traitement: [0.0, 1.0, 2.0]\n",
      "\n",
      "Colonne RR_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 102' '01. <= 63' '04. >= 102' '02. <= 79']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 79.0\n",
      "- Exemples de valeurs après traitement: [63.0, 79.0, 102.0]\n",
      "\n",
      "Colonne RR_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['03. <= 232' '01. <= 137' '02. <= 176' '04. >= 232']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 176.0\n",
      "- Exemples de valeurs après traitement: [137.0, 176.0, 232.0]\n",
      "\n",
      "Colonne RRAB_VOR_MM_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['04. >= 25' '03. <= 25' '02. <= 19' '01. <= 9']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 19.0\n",
      "- Exemples de valeurs après traitement: [9.0, 19.0, 25.0]\n",
      "\n",
      "Colonne RRAB_VOR_MMAX_A:\n",
      "- Pourcentage de valeurs manquantes: 56.75%\n",
      "- Type initial: object\n",
      "- Exemples de valeurs avant conversion:\n",
      "['02. <= 57' '01. <= 41' '03. <= 82' '04. >= 82']\n",
      "- Conversion en numérique\n",
      "- Type après conversion: float64\n",
      "- Imputation avec la médiane: 41.0\n",
      "- Exemples de valeurs après traitement: [41.0, 57.0, 82.0]\n"
     ]
    }
   ],
   "source": [
    "# Cellule 4 : Traitement Weather\n",
    "def clean_weather_value(value):\n",
    "    \"\"\"\n",
    "    Nettoie une valeur météo en extrayant le nombre après '<=' ou '>='\n",
    "    Ex: '02. <= 65' -> 65\n",
    "    Ex: '04. >= 7' -> 7\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    try:\n",
    "        # Enlever le préfixe (ex: \"02. \")\n",
    "        value = value.split('.')[1].strip()\n",
    "        # Extraire le nombre après '<=' ou '>='\n",
    "        if '<=' in value:\n",
    "            return float(value.split('<=')[1].strip())\n",
    "        elif '>=' in value:\n",
    "            return float(value.split('>=')[1].strip())\n",
    "        return value\n",
    "    except:\n",
    "        return value\n",
    "\n",
    "def  clean_weather_vars(X_train, X_test, missing_summary, weather_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables météo avec conversion des valeurs catégorielles en numériques\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables météo:\")\n",
    "    \n",
    "    for col in weather_cols:\n",
    "        if col in X_train.columns:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            print(f\"\\nColonne {col}:\")\n",
    "            print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "            \n",
    "            # Afficher quelques valeurs avant conversion\n",
    "            if X_train[col].dtype == 'object':\n",
    "                print(\"- Exemples de valeurs avant conversion:\")\n",
    "                print(X_train[col].dropna().unique()[:5])\n",
    "            \n",
    "            # Créer indicateur de valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                # Convertir les valeurs catégorielles en numériques\n",
    "                if X_train[col].dtype == 'object':\n",
    "                    print(\"- Conversion en numérique\")\n",
    "                    X_train[col] = X_train[col].apply(clean_weather_value)\n",
    "                    X_test[col] = X_test[col].apply(clean_weather_value)\n",
    "                    print(f\"- Type après conversion: {X_train[col].dtype}\")\n",
    "                \n",
    "                # Imputation avec la médiane\n",
    "                median_val = X_train[col].median()\n",
    "                X_train[col].fillna(median_val, inplace=True)\n",
    "                X_test[col].fillna(median_val, inplace=True)\n",
    "                print(f\"- Imputation avec la médiane: {median_val}\")\n",
    "                \n",
    "                # Vérification des valeurs uniques après traitement\n",
    "                unique_vals = sorted(X_train[col].unique())[:5]\n",
    "                print(f\"- Exemples de valeurs après traitement: {unique_vals}\")\n",
    "            else:\n",
    "                print(\"- Colonne ignorée (>70% manquant)\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "# Exécution\n",
    "X_train, X_test, weather_indicators = handle_weather_vars(\n",
    "    X_train, X_test, missing_summary, groups['Weather']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5 : Traitement Building\n",
    "def handle_building_vars(X_train, X_test, missing_summary, building_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables bâtiment avec vérification détaillée des types\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables bâtiment:\")\n",
    "    \n",
    "    for col in building_cols:\n",
    "        if col in X_train.columns:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            print(f\"\\nColonne {col}:\")\n",
    "            print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "            \n",
    "            # Créer indicateur de valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    # Pour les variables numériques\n",
    "                    median_val = X_train[col].median()\n",
    "                    X_train[col].fillna(median_val, inplace=True)\n",
    "                    X_test[col].fillna(median_val, inplace=True)\n",
    "                    print(f\"- Variable numérique, imputation avec la médiane: {median_val}\")\n",
    "                    print(f\"- Plage de valeurs: [{X_train[col].min()}, {X_train[col].max()}]\")\n",
    "                else:\n",
    "                    # Pour les variables catégorielles\n",
    "                    # Filtrer les valeurs non-nulles avant de les afficher\n",
    "                    unique_vals = X_train[col].dropna().unique()\n",
    "                    print(\"- Variable catégorielle, valeurs uniques:\")\n",
    "                    print(unique_vals[:5])\n",
    "                    \n",
    "                    # Si peu de valeurs uniques, on pourrait considérer une conversion en numérique\n",
    "                    if len(unique_vals) < 10:\n",
    "                        print(f\"- Nombre de catégories: {len(unique_vals)}\")\n",
    "                        print(\"- Distribution des valeurs:\")\n",
    "                        print(X_train[col].value_counts().head())\n",
    "                    \n",
    "                    mode_val = X_train[col].mode()[0]\n",
    "                    X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                    X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "                    print(f\"- Imputation avec 'UNKNOWN'\")\n",
    "            else:\n",
    "                print(\"- Colonne ignorée (>70% manquant)\")\n",
    "            \n",
    "            # Vérification après traitement\n",
    "            missing_after = X_train[col].isnull().sum()\n",
    "            if missing_after > 0:\n",
    "                print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "# Exécution\n",
    "X_train, X_test, building_indicators = handle_building_vars(\n",
    "    X_train, X_test, missing_summary, groups['Building']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6 : Traitement Activity\n",
    "def handle_activity_vars(X_train, X_test, missing_summary, activity_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables d'activité avec vérification détaillée des types et valeurs\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables d'activité:\")\n",
    "    \n",
    "    for col in activity_cols:\n",
    "        if col in X_train.columns:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            print(f\"\\nColonne {col}:\")\n",
    "            print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "            \n",
    "            # Créer indicateur de valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    # Pour les variables numériques\n",
    "                    median_val = X_train[col].median()\n",
    "                    X_train[col].fillna(median_val, inplace=True)\n",
    "                    X_test[col].fillna(median_val, inplace=True)\n",
    "                    print(f\"- Variable numérique, imputation avec la médiane: {median_val}\")\n",
    "                    print(f\"- Plage de valeurs: [{X_train[col].min()}, {X_train[col].max()}]\")\n",
    "                    print(f\"- Distribution: \\n{X_train[col].value_counts().head()}\")\n",
    "                else:\n",
    "                    # Pour les variables catégorielles\n",
    "                    unique_vals = X_train[col].unique()\n",
    "                    print(f\"- Variable catégorielle\")\n",
    "                    print(f\"- Nombre de catégories uniques: {len(unique_vals)}\")\n",
    "                    print(f\"- Top 5 catégories les plus fréquentes:\")\n",
    "                    print(X_train[col].value_counts().head())\n",
    "                    \n",
    "                    mode_val = X_train[col].mode()[0]\n",
    "                    X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                    X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "                    print(f\"- Imputation avec 'UNKNOWN'\")\n",
    "            else:\n",
    "                print(\"- Colonne ignorée (>70% manquant)\")\n",
    "            \n",
    "            # Vérification après traitement\n",
    "            missing_after = X_train[col].isnull().sum()\n",
    "            if missing_after > 0:\n",
    "                print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "            \n",
    "            # Vérification de la cohérence entre train et test\n",
    "            train_unique = set(X_train[col].unique())\n",
    "            test_unique = set(X_test[col].unique())\n",
    "            diff_cats = test_unique - train_unique\n",
    "            if len(diff_cats) > 0:\n",
    "                print(f\"⚠️ ATTENTION: Catégories présentes dans test mais pas dans train: {diff_cats}\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "# Exécution\n",
    "X_train, X_test, activity_indicators = handle_activity_vars(\n",
    "    X_train, X_test, missing_summary, groups['Activity']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement des variables d'assurance:\n",
      "\n",
      "Colonne RISK1:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 3.0\n",
      "  * Moyenne: 447.41\n",
      "  * Écart-type: 495.72\n",
      "  * Plage: [-1.0, 1000.0]\n",
      "\n",
      "Colonne RISK2:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 35.0\n",
      "  * Moyenne: 461.45\n",
      "  * Écart-type: 483.14\n",
      "  * Plage: [3.0, 1000.0]\n",
      "\n",
      "Colonne RISK3:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 13.0\n",
      "  * Moyenne: 451.24\n",
      "  * Écart-type: 492.29\n",
      "  * Plage: [-3.0, 1000.0]\n",
      "\n",
      "Colonne RISK4:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 5.0\n",
      "  * Moyenne: 447.76\n",
      "  * Écart-type: 495.40\n",
      "  * Plage: [1.0, 1000.0]\n",
      "\n",
      "Colonne RISK5:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 5.0\n",
      "  * Moyenne: 448.33\n",
      "  * Écart-type: 494.89\n",
      "  * Plage: [0.0, 1000.0]\n",
      "\n",
      "Colonne RISK6:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 4\n",
      "  * Distribution des valeurs:\n",
      "O    197833\n",
      "N    155982\n",
      "A      3222\n",
      "Name: RISK6, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK7:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 100.0\n",
      "  * Moyenne: 92.96\n",
      "  * Écart-type: 8.90\n",
      "  * Plage: [75.0, 500.0]\n",
      "⚠️ 2 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne RISK8:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 3\n",
      "  * Distribution des valeurs:\n",
      "O    241056\n",
      "N    115981\n",
      "Name: RISK8, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK9:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 4\n",
      "  * Distribution des valeurs:\n",
      "N    323586\n",
      "R     22962\n",
      "O     10489\n",
      "Name: RISK9, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK10:\n",
      "- Pourcentage de valeurs manquantes: 36.69%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 4\n",
      "  * Distribution des valeurs:\n",
      "R    87340\n",
      "O    79901\n",
      "N    75625\n",
      "Name: RISK10, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK11:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 3\n",
      "  * Distribution des valeurs:\n",
      "O    156979\n",
      "N    140728\n",
      "R     85903\n",
      "Name: RISK11, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK12:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 4\n",
      "  * Distribution des valeurs:\n",
      "N    321170\n",
      "O     19016\n",
      "R     16851\n",
      "Name: RISK12, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne RISK13:\n",
      "- Pourcentage de valeurs manquantes: 40.79%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 4\n",
      "  * Distribution des valeurs:\n",
      "N    126750\n",
      "R     73635\n",
      "O     26759\n",
      "Name: RISK13, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.02\n",
      "  * Écart-type: 0.15\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 9246 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL2:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.00\n",
      "  * Écart-type: 0.05\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 887 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL3:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.05\n",
      "  * Écart-type: 0.21\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 17898 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL4:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.00\n",
      "  * Écart-type: 0.02\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 168 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL5:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.00\n",
      "  * Écart-type: 0.06\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 1598 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL6:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.43\n",
      "  * Écart-type: 0.49\n",
      "  * Plage: [0, 1]\n",
      "\n",
      "Colonne KAPITAL7:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.17\n",
      "  * Écart-type: 0.37\n",
      "  * Plage: [0, 1]\n",
      "\n",
      "Colonne KAPITAL8:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.04\n",
      "  * Écart-type: 0.21\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 16970 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL9:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.10\n",
      "  * Écart-type: 0.30\n",
      "  * Plage: [0, 1]\n",
      "⚠️ 37471 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL10:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 67639.98\n",
      "  * Écart-type: 123142.25\n",
      "  * Plage: [0, 500000]\n",
      "⚠️ 16578 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL11:\n",
      "- Pourcentage de valeurs manquantes: 31.02%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 40000.0\n",
      "  * Moyenne: 98060.78\n",
      "  * Écart-type: 137843.73\n",
      "  * Plage: [0.0, 500000.0]\n",
      "\n",
      "Colonne KAPITAL12:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 125000.0\n",
      "  * Moyenne: 120546.09\n",
      "  * Écart-type: 124946.66\n",
      "  * Plage: [0, 500000]\n",
      "⚠️ 10974 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL13:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 122.56\n",
      "  * Écart-type: 786.28\n",
      "  * Plage: [0, 10000]\n",
      "⚠️ 9246 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL14:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 1331.33\n",
      "  * Écart-type: 4182.20\n",
      "  * Plage: [0, 50000]\n",
      "⚠️ 5570 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL15:\n",
      "- Pourcentage de valeurs manquantes: 0.16%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 15.79\n",
      "  * Écart-type: 220.87\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 2450 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL16:\n",
      "- Pourcentage de valeurs manquantes: 0.41%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 44.96\n",
      "  * Écart-type: 410.04\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 4674 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL17:\n",
      "- Pourcentage de valeurs manquantes: 0.23%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 25.79\n",
      "  * Écart-type: 298.27\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 3419 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL18:\n",
      "- Pourcentage de valeurs manquantes: 0.01%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 7.43\n",
      "  * Écart-type: 127.77\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 1616 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL19:\n",
      "- Pourcentage de valeurs manquantes: 0.29%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 82.38\n",
      "  * Écart-type: 752.61\n",
      "  * Plage: [0.0, 10000.0]\n",
      "⚠️ 4838 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL20:\n",
      "- Pourcentage de valeurs manquantes: 0.07%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 10.14\n",
      "  * Écart-type: 179.15\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 1527 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL21:\n",
      "- Pourcentage de valeurs manquantes: 1.40%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 2086.07\n",
      "  * Écart-type: 2868.76\n",
      "  * Plage: [0.0, 10000.0]\n",
      "\n",
      "Colonne KAPITAL22:\n",
      "- Pourcentage de valeurs manquantes: 0.68%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 35.10\n",
      "  * Écart-type: 498.55\n",
      "  * Plage: [0.0, 10000.0]\n",
      "⚠️ 2308 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL23:\n",
      "- Pourcentage de valeurs manquantes: 0.30%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 468.09\n",
      "  * Écart-type: 1339.45\n",
      "  * Plage: [0.0, 10000.0]\n",
      "⚠️ 12137 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL24:\n",
      "- Pourcentage de valeurs manquantes: 0.22%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 156.25\n",
      "  * Écart-type: 569.39\n",
      "  * Plage: [0.0, 5000.0]\n",
      "⚠️ 16278 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL25:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 1000.0\n",
      "  * Moyenne: 3379.88\n",
      "  * Écart-type: 4973.58\n",
      "  * Plage: [0, 20000]\n",
      "⚠️ 19555 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL26:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 2470.32\n",
      "  * Écart-type: 13967.15\n",
      "  * Plage: [0, 100000]\n",
      "⚠️ 10378 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL27:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 5699.67\n",
      "  * Écart-type: 14205.23\n",
      "  * Plage: [0, 50000]\n",
      "⚠️ 28167 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL28:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 931.68\n",
      "  * Écart-type: 5426.72\n",
      "  * Plage: [0, 50000]\n",
      "⚠️ 12140 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL29:\n",
      "- Pourcentage de valeurs manquantes: 0.99%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 227.22\n",
      "  * Écart-type: 2961.72\n",
      "  * Plage: [0.0, 50000.0]\n",
      "⚠️ 2497 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL30:\n",
      "- Pourcentage de valeurs manquantes: 0.99%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 165.41\n",
      "  * Écart-type: 2568.90\n",
      "  * Plage: [0.0, 50000.0]\n",
      "⚠️ 1756 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL31:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 557.21\n",
      "  * Écart-type: 4264.39\n",
      "  * Plage: [0.0, 50000.0]\n",
      "⚠️ 7149 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL32:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 22500.0\n",
      "  * Moyenne: 74441.68\n",
      "  * Écart-type: 94312.94\n",
      "  * Plage: [0, 300000]\n",
      "\n",
      "Colonne KAPITAL33:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 7080.80\n",
      "  * Écart-type: 40506.08\n",
      "  * Plage: [0.0, 300000.0]\n",
      "⚠️ 8245 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne KAPITAL34:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    369780\n",
      "O     13830\n",
      "Name: KAPITAL34, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL35:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    374364\n",
      "O      9246\n",
      "Name: KAPITAL35, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL36:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    382723\n",
      "O       887\n",
      "Name: KAPITAL36, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL37:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    365712\n",
      "O     17898\n",
      "Name: KAPITAL37, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL38:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    383442\n",
      "O       168\n",
      "Name: KAPITAL38, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL39:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    382012\n",
      "O      1598\n",
      "Name: KAPITAL39, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL40:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    220059\n",
      "O    163551\n",
      "Name: KAPITAL40, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL41:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    319270\n",
      "O     64340\n",
      "Name: KAPITAL41, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL42:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    366640\n",
      "O     16970\n",
      "Name: KAPITAL42, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne KAPITAL43:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    346139\n",
      "O     37471\n",
      "Name: KAPITAL43, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    383529\n",
      "O        81\n",
      "Name: DEROG1, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG2:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    370561\n",
      "O     13049\n",
      "Name: DEROG2, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG3:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    375624\n",
      "O      7986\n",
      "Name: DEROG3, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG4:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    359142\n",
      "O     24468\n",
      "Name: DEROG4, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG5:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    365019\n",
      "O     18591\n",
      "Name: DEROG5, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG6:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    381612\n",
      "O      1998\n",
      "Name: DEROG6, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG7:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    382831\n",
      "O       779\n",
      "Name: DEROG7, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG8:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    375707\n",
      "O      7903\n",
      "Name: DEROG8, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG9:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    381410\n",
      "O      2200\n",
      "Name: DEROG9, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG10:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    382150\n",
      "O      1460\n",
      "Name: DEROG10, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG11:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    378858\n",
      "O      4752\n",
      "Name: DEROG11, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne DEROG12:\n",
      "- Pourcentage de valeurs manquantes: 91.00%\n",
      "- Type initial: object\n",
      "- Colonne ignorée (>70% manquant)\n",
      "⚠️ ATTENTION: 349078 valeurs manquantes restantes!\n",
      "\n",
      "Colonne DEROG13:\n",
      "- Pourcentage de valeurs manquantes: 99.66%\n",
      "- Type initial: object\n",
      "- Colonne ignorée (>70% manquant)\n",
      "⚠️ ATTENTION: 382317 valeurs manquantes restantes!\n",
      "\n",
      "Colonne DEROG14:\n",
      "- Pourcentage de valeurs manquantes: 100.00%\n",
      "- Type initial: object\n",
      "- Colonne ignorée (>70% manquant)\n",
      "⚠️ ATTENTION: 383602 valeurs manquantes restantes!\n",
      "\n",
      "Colonne DEROG15:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 100.0\n",
      "  * Moyenne: 100.00\n",
      "  * Écart-type: 0.29\n",
      "  * Plage: [70.0, 100.0]\n",
      "⚠️ 35 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne DEROG16:\n",
      "- Pourcentage de valeurs manquantes: 99.15%\n",
      "- Type initial: object\n",
      "- Colonne ignorée (>70% manquant)\n",
      "⚠️ ATTENTION: 380358 valeurs manquantes restantes!\n",
      "\n",
      "Colonne FRCH1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 5\n",
      "  * Distribution des valeurs:\n",
      "0    242299\n",
      "1    126845\n",
      "2     11793\n",
      "3      2609\n",
      "i        64\n",
      "Name: FRCH1, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne FRCH2:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 7\n",
      "  * Distribution des valeurs:\n",
      "1    161789\n",
      "2    123820\n",
      "A     55578\n",
      "3     14969\n",
      "4       651\n",
      "Name: FRCH2, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne INDEM1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 2\n",
      "  * Distribution des valeurs:\n",
      "N    365728\n",
      "O     17882\n",
      "Name: INDEM1, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne INDEM2:\n",
      "- Pourcentage de valeurs manquantes: 7.14%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 13\n",
      "  * Distribution des valeurs:\n",
      "CLASS5    215687\n",
      "CLASS8     67948\n",
      "CLASS6     28696\n",
      "CLASS9     18713\n",
      "CLASS1     15467\n",
      "Name: INDEM2, dtype: int64\n",
      "\n",
      "Colonne CA1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 1047.40\n",
      "  * Écart-type: 4777.35\n",
      "  * Plage: [0, 30000]\n",
      "⚠️ 9125 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne CA2:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 4.11\n",
      "  * Écart-type: 329.38\n",
      "  * Plage: [0, 30000]\n",
      "⚠️ 65 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne CA3:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 1228.03\n",
      "  * Écart-type: 6995.88\n",
      "  * Plage: [0, 50000]\n",
      "⚠️ 11392 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne TAILLE1:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 10\n",
      "  * Distribution des valeurs:\n",
      "01 - [0   -250k]    126652\n",
      "02 - [250k-500k]     79214\n",
      "03 - [500k-750k]     50552\n",
      "05 - [1M - 1.5M]     42991\n",
      "04 - [750k-  1M]     34424\n",
      "Name: TAILLE1, dtype: int64\n",
      "\n",
      "Colonne TAILLE2:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 10\n",
      "  * Distribution des valeurs:\n",
      "03 - [250k-500k]    95799\n",
      "02 - [100k-250k]    86531\n",
      "01 - [0   -100k]    66538\n",
      "04 - [500k-750k]    55253\n",
      "05 - [750k-  1M]    32244\n",
      "Name: TAILLE2, dtype: int64\n",
      "\n",
      "Colonne TAILLE3:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.36\n",
      "  * Écart-type: 1.17\n",
      "  * Plage: [0, 16]\n",
      "⚠️ 7938 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne TAILLE4:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.10\n",
      "  * Écart-type: 0.74\n",
      "  * Plage: [0, 11]\n",
      "⚠️ 5009 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne NBSINCONJ:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.11\n",
      "  * Écart-type: 0.30\n",
      "  * Plage: [0.0, 6.2]\n",
      "⚠️ 5178 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne NBSINSTRT:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.0\n",
      "  * Moyenne: 0.27\n",
      "  * Écart-type: 0.55\n",
      "  * Plage: [0.0, 10.0]\n",
      "⚠️ 9873 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne ANCIENNETE:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: int64\n",
      "- Variable numérique:\n",
      "  * Médiane: 5.0\n",
      "  * Moyenne: 5.18\n",
      "  * Écart-type: 3.82\n",
      "  * Plage: [0, 11]\n",
      "\n",
      "Colonne DUREE_REQANEUF:\n",
      "- Pourcentage de valeurs manquantes: 6.93%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 2.0\n",
      "  * Moyenne: 2.20\n",
      "  * Écart-type: 1.30\n",
      "  * Plage: [0.0, 10.0]\n",
      "⚠️ 7809 valeurs aberrantes détectées (>3σ)\n",
      "\n",
      "Colonne AN_EXERC:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: object\n",
      "- Variable catégorielle:\n",
      "  * Nombre de catégories: 9\n",
      "  * Distribution des valeurs:\n",
      "ANNEE6    77819\n",
      "ANNEE1    67334\n",
      "ANNEE7    42378\n",
      "ANNEE8    39491\n",
      "ANNEE9    37084\n",
      "Name: AN_EXERC, dtype: int64\n",
      "  * Candidat potentiel pour encodage catégoriel\n",
      "\n",
      "Colonne ANNEE_ASSURANCE:\n",
      "- Pourcentage de valeurs manquantes: 0.00%\n",
      "- Type initial: float64\n",
      "- Variable numérique:\n",
      "  * Médiane: 0.8821917808\n",
      "  * Moyenne: 0.70\n",
      "  * Écart-type: 0.35\n",
      "  * Plage: [0.0027322404, 2.0]\n",
      "⚠️ 707 valeurs aberrantes détectées (>3σ)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 7 : Traitement Insurance\n",
    "def handle_insurance_vars(X_train, X_test, missing_summary, insurance_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables d'assurance avec vérification détaillée des types et valeurs\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables d'assurance:\")\n",
    "    \n",
    "    for col in insurance_cols:\n",
    "        if col in X_train.columns:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            print(f\"\\nColonne {col}:\")\n",
    "            print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "            \n",
    "            # Créer indicateur de valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    # Pour les variables numériques\n",
    "                    median_val = X_train[col].median()\n",
    "                    mean_val = X_train[col].mean()\n",
    "                    std_val = X_train[col].std()\n",
    "                    \n",
    "                    print(f\"- Variable numérique:\")\n",
    "                    print(f\"  * Médiane: {median_val}\")\n",
    "                    print(f\"  * Moyenne: {mean_val:.2f}\")\n",
    "                    print(f\"  * Écart-type: {std_val:.2f}\")\n",
    "                    print(f\"  * Plage: [{X_train[col].min()}, {X_train[col].max()}]\")\n",
    "                    \n",
    "                    # Détection des valeurs aberrantes\n",
    "                    outliers = X_train[col][(X_train[col] > mean_val + 3*std_val) | \n",
    "                                         (X_train[col] < mean_val - 3*std_val)].count()\n",
    "                    if outliers > 0:\n",
    "                        print(f\"⚠️ {outliers} valeurs aberrantes détectées (>3σ)\")\n",
    "                    \n",
    "                    X_train[col].fillna(median_val, inplace=True)\n",
    "                    X_test[col].fillna(median_val, inplace=True)\n",
    "                    \n",
    "                else:\n",
    "                    # Pour les variables catégorielles\n",
    "                    unique_vals = X_train[col].unique()\n",
    "                    print(f\"- Variable catégorielle:\")\n",
    "                    print(f\"  * Nombre de catégories: {len(unique_vals)}\")\n",
    "                    print(\"  * Distribution des valeurs:\")\n",
    "                    print(X_train[col].value_counts().head())\n",
    "                    \n",
    "                    # Si la variable a peu de catégories, on pourrait envisager un encodage\n",
    "                    if len(unique_vals) < 10:\n",
    "                        print(\"  * Candidat potentiel pour encodage catégoriel\")\n",
    "                    \n",
    "                    X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                    X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "            else:\n",
    "                print(\"- Colonne ignorée (>70% manquant)\")\n",
    "            \n",
    "            # Vérifications post-traitement\n",
    "            missing_after = X_train[col].isnull().sum()\n",
    "            if missing_after > 0:\n",
    "                print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "            \n",
    "            # Vérification de la cohérence train/test\n",
    "            if X_train[col].dtype == 'object':\n",
    "                train_cats = set(X_train[col].unique())\n",
    "                test_cats = set(X_test[col].unique())\n",
    "                new_cats = test_cats - train_cats\n",
    "                if new_cats:\n",
    "                    print(f\"⚠️ Nouvelles catégories dans test: {new_cats}\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "# Exécution\n",
    "X_train, X_test, insurance_indicators = handle_insurance_vars(\n",
    "    X_train, X_test, missing_summary, groups['Insurance']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_geographic_vars(X_train, X_test, missing_summary, geographic_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables géographiques avec vérification détaillée des types et valeurs\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables géographiques:\")\n",
    "    \n",
    "    # Regrouper les colonnes par type\n",
    "    distance_cols = [col for col in geographic_cols if 'DISTANCE' in col]\n",
    "    proportion_cols = [col for col in geographic_cols if 'PROPORTION' in col]\n",
    "    altitude_cols = [col for col in geographic_cols if 'ALTITUDE' in col]\n",
    "    other_cols = [col for col in geographic_cols if col not in distance_cols + proportion_cols + altitude_cols]\n",
    "    \n",
    "    for col_type, cols in [\n",
    "        (\"Distance\", distance_cols),\n",
    "        (\"Proportion\", proportion_cols),\n",
    "        (\"Altitude\", altitude_cols),\n",
    "        (\"Autres\", other_cols)\n",
    "    ]:\n",
    "        print(f\"\\n=== Variables de {col_type} ===\")\n",
    "        \n",
    "        for col in cols:\n",
    "            if col in X_train.columns:\n",
    "                missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "                print(f\"\\nColonne {col}:\")\n",
    "                print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "                print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "                \n",
    "                # Créer indicateur de valeurs manquantes\n",
    "                if missing_pct > 0:\n",
    "                    missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "                \n",
    "                if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                    if X_train[col].dtype in ['int64', 'float64']:\n",
    "                        # Pour les variables numériques\n",
    "                        stats = X_train[col].describe()\n",
    "                        print(\"- Statistiques descriptives:\")\n",
    "                        print(f\"  * Moyenne: {stats['mean']:.2f}\")\n",
    "                        print(f\"  * Écart-type: {stats['std']:.2f}\")\n",
    "                        print(f\"  * Min: {stats['min']:.2f}\")\n",
    "                        print(f\"  * 25%: {stats['25%']:.2f}\")\n",
    "                        print(f\"  * Médiane: {stats['50%']:.2f}\")\n",
    "                        print(f\"  * 75%: {stats['75%']:.2f}\")\n",
    "                        print(f\"  * Max: {stats['max']:.2f}\")\n",
    "                        \n",
    "                        # Vérification des valeurs négatives pour les distances\n",
    "                        if 'DISTANCE' in col and (X_train[col] < 0).any():\n",
    "                            print(\"⚠️ ATTENTION: Valeurs négatives détectées pour une distance!\")\n",
    "                        \n",
    "                        # Vérification des proportions\n",
    "                        if 'PROPORTION' in col:\n",
    "                            if (X_train[col] < 0).any() or (X_train[col] > 1).any():\n",
    "                                print(\"⚠️ ATTENTION: Proportions hors de l'intervalle [0,1]!\")\n",
    "                        \n",
    "                        median_val = stats['50%']\n",
    "                        X_train[col].fillna(median_val, inplace=True)\n",
    "                        X_test[col].fillna(median_val, inplace=True)\n",
    "                        print(f\"- Imputation avec la médiane: {median_val}\")\n",
    "                        \n",
    "                    else:\n",
    "                        # Pour les variables catégorielles\n",
    "                        # Extraire le nombre après '<=' ou '>='\n",
    "                        def extract_number(x):\n",
    "                            if pd.isna(x):\n",
    "                                return x\n",
    "                            try:\n",
    "                                if '<=' in str(x):\n",
    "                                    return float(str(x).split('<=')[1].strip())\n",
    "                                elif '>=' in str(x):\n",
    "                                    return float(str(x).split('>=')[1].strip())\n",
    "                                return float(x)\n",
    "                            except:\n",
    "                                return x\n",
    "                        \n",
    "                        print(\"- Conversion en numérique...\")\n",
    "                        X_train[col] = X_train[col].apply(extract_number)\n",
    "                        X_test[col] = X_test[col].apply(extract_number)\n",
    "                        \n",
    "                        # Imputation avec la médiane après conversion\n",
    "                        median_val = X_train[col].median()\n",
    "                        X_train[col].fillna(median_val, inplace=True)\n",
    "                        X_test[col].fillna(median_val, inplace=True)\n",
    "                        print(f\"- Imputation avec la médiane: {median_val}\")\n",
    "                        \n",
    "                else:\n",
    "                    print(\"- Colonne ignorée (>70% manquant)\")\n",
    "                \n",
    "                # Vérifications post-traitement\n",
    "                missing_after = X_train[col].isnull().sum()\n",
    "                if missing_after > 0:\n",
    "                    print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement des variables démographiques:\n",
      "\n",
      "=== Variables Ménages ===\n",
      "\n",
      "Colonne MEN:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 17204     362312\n",
      "02. <= 153098      2592\n",
      "03. <= 670263       116\n",
      "Name: MEN, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne MEN_1IND:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "03. <= 30    160610\n",
      "04. <= 40    147462\n",
      "05. <= 50     37410\n",
      "02. <= 20     18298\n",
      "06. <= 60      1221\n",
      "Name: MEN_1IND, dtype: int64\n",
      "- Nombre total de catégories: 7\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_5IND:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    350980\n",
      "02. <= 20     14015\n",
      "03. <= 30        25\n",
      "Name: MEN_5IND, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_FMP:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    252432\n",
      "02. <= 20    112528\n",
      "03. <= 30        60\n",
      "Name: MEN_FMP, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_COLL:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    236355\n",
      "02. <= 20     69346\n",
      "03. <= 30     29215\n",
      "04. <= 40     13585\n",
      "05. <= 50      8350\n",
      "Name: MEN_COLL, dtype: int64\n",
      "- Nombre total de catégories: 10\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_MAIS:\n",
      "- Pourcentage de valeurs manquantes: 7.16%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "10. > 90     227485\n",
      "09. <= 90     69346\n",
      "08. <= 80     29215\n",
      "07. <= 70     13585\n",
      "06. <= 60      8350\n",
      "Name: MEN_MAIS, dtype: int64\n",
      "- Nombre total de catégories: 10\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_PROP:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "08. <= 80    156440\n",
      "09. <= 90    124689\n",
      "07. <= 70     57230\n",
      "06. <= 60     17298\n",
      "05. <= 50      4838\n",
      "Name: MEN_PROP, dtype: int64\n",
      "- Nombre total de catégories: 8\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_PAUV:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 20    207001\n",
      "01. <= 10    115306\n",
      "03. <= 30     40062\n",
      "04. <= 40      2400\n",
      "05. <= 50       240\n",
      "Name: MEN_PAUV, dtype: int64\n",
      "- Nombre total de catégories: 6\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "Colonne MEN_SURF:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 106    151993\n",
      "03. <= 118    128509\n",
      "04. >= 118     45863\n",
      "01. <= 93      38655\n",
      "Name: MEN_SURF, dtype: int64\n",
      "- Nombre total de catégories: 4\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\n",
      "\n",
      "=== Variables Individus ===\n",
      "\n",
      "Colonne IND:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 33995      362143\n",
      "02. <= 318560       2761\n",
      "03. <= 1340814       116\n",
      "Name: IND, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne IND_0_Y1:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    365017\n",
      "02. <= 20         3\n",
      "Name: IND_0_Y1, dtype: int64\n",
      "- Nombre total de catégories: 2\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y1_Y2:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    365020\n",
      "Name: IND_Y1_Y2, dtype: int64\n",
      "- Nombre total de catégories: 1\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne IND_Y2_Y3:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    364268\n",
      "02. <= 20       752\n",
      "Name: IND_Y2_Y3, dtype: int64\n",
      "- Nombre total de catégories: 2\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y3_Y4:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    304855\n",
      "02. <= 20     60165\n",
      "Name: IND_Y3_Y4, dtype: int64\n",
      "- Nombre total de catégories: 2\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y4_Y5:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    362926\n",
      "02. <= 20      2094\n",
      "Name: IND_Y4_Y5, dtype: int64\n",
      "- Nombre total de catégories: 2\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y5_Y6:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 20    334272\n",
      "03. <= 30     16785\n",
      "01. <= 10     13948\n",
      "04. <= 40        15\n",
      "Name: IND_Y5_Y6, dtype: int64\n",
      "- Nombre total de catégories: 4\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y6_Y7:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 20    191428\n",
      "03. <= 30    173488\n",
      "01. <= 10        59\n",
      "04. <= 40        45\n",
      "Name: IND_Y6_Y7, dtype: int64\n",
      "- Nombre total de catégories: 4\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y7_Y8:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 20    349457\n",
      "03. <= 30      8336\n",
      "01. <= 10      7227\n",
      "Name: IND_Y7_Y8, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y8_Y9:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "02. <= 20    273630\n",
      "03. <= 30     78317\n",
      "01. <= 10     11070\n",
      "04. <= 40      1994\n",
      "05. <= 50         5\n",
      "Name: IND_Y8_Y9, dtype: int64\n",
      "- Nombre total de catégories: 6\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_Y9:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    320197\n",
      "02. <= 20     44812\n",
      "03. <= 30        11\n",
      "Name: IND_Y9, dtype: int64\n",
      "- Nombre total de catégories: 3\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "Colonne IND_INC:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    365020\n",
      "Name: IND_INC, dtype: int64\n",
      "- Nombre total de catégories: 1\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne IND_SNV:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 21873    156783\n",
      "02. <= 24733    148819\n",
      "03. <= 29681     53263\n",
      "04. >= 29681      6155\n",
      "Name: IND_SNV, dtype: int64\n",
      "- Nombre total de catégories: 4\n",
      "- Imputation avec 'UNKNOWN'\n",
      "⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\n",
      "\n",
      "=== Variables Logements ===\n",
      "\n",
      "Colonne LOG_AVA1:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "04. <= 40    76170\n",
      "05. <= 50    73293\n",
      "03. <= 30    66230\n",
      "06. <= 60    54065\n",
      "02. <= 20    34920\n",
      "Name: LOG_AVA1, dtype: int64\n",
      "- Nombre total de catégories: 10\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne LOG_A1_A2:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    250558\n",
      "02. <= 20     95641\n",
      "03. <= 30     15939\n",
      "04. <= 40      2412\n",
      "05. <= 50       411\n",
      "Name: LOG_A1_A2, dtype: int64\n",
      "- Nombre total de catégories: 7\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne LOG_A2_A3:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "03. <= 30    164016\n",
      "02. <= 20    132692\n",
      "04. <= 40     40920\n",
      "01. <= 10     21335\n",
      "05. <= 50      5378\n",
      "Name: LOG_A2_A3, dtype: int64\n",
      "- Nombre total de catégories: 9\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne LOG_APA3:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "03. <= 30    114582\n",
      "04. <= 40     94401\n",
      "02. <= 20     76993\n",
      "05. <= 50     48159\n",
      "01. <= 10     14476\n",
      "Name: LOG_APA3, dtype: int64\n",
      "- Nombre total de catégories: 10\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne LOG_INC:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    365002\n",
      "02. <= 20        18\n",
      "Name: LOG_INC, dtype: int64\n",
      "- Nombre total de catégories: 2\n",
      "- Imputation avec 'UNKNOWN'\n",
      "\n",
      "Colonne LOG_SOC:\n",
      "- Pourcentage de valeurs manquantes: 4.85%\n",
      "- Type initial: object\n",
      "- Distribution des valeurs:\n",
      "01. <= 10    319281\n",
      "02. <= 20     36778\n",
      "03. <= 30      7096\n",
      "04. <= 40      1390\n",
      "05. <= 50       397\n",
      "Name: LOG_SOC, dtype: int64\n",
      "- Nombre total de catégories: 7\n",
      "- Imputation avec 'UNKNOWN'\n"
     ]
    }
   ],
   "source": [
    "# Cellule 9 : Traitement Demographic\n",
    "def handle_demographic_vars(X_train, X_test, missing_summary, demographic_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables démographiques avec vérification détaillée des types et valeurs\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nTraitement des variables démographiques:\")\n",
    "    \n",
    "    # Regrouper les colonnes par type\n",
    "    household_cols = [col for col in demographic_cols if col.startswith('MEN')]\n",
    "    individual_cols = [col for col in demographic_cols if col.startswith('IND')]\n",
    "    housing_cols = [col for col in demographic_cols if col.startswith('LOG')]\n",
    "    \n",
    "    for col_type, cols in [\n",
    "        (\"Ménages\", household_cols),\n",
    "        (\"Individus\", individual_cols),\n",
    "        (\"Logements\", housing_cols)\n",
    "    ]:\n",
    "        print(f\"\\n=== Variables {col_type} ===\")\n",
    "        \n",
    "        for col in cols:\n",
    "            if col in X_train.columns:\n",
    "                missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "                print(f\"\\nColonne {col}:\")\n",
    "                print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "                print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "                \n",
    "                # Créer indicateur de valeurs manquantes\n",
    "                if missing_pct > 0:\n",
    "                    missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "                \n",
    "                if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                    if X_train[col].dtype in ['int64', 'float64']:\n",
    "                        # Pour les variables numériques\n",
    "                        stats = X_train[col].describe()\n",
    "                        print(\"- Statistiques descriptives:\")\n",
    "                        print(f\"  * Moyenne: {stats['mean']:.2f}\")\n",
    "                        print(f\"  * Écart-type: {stats['std']:.2f}\")\n",
    "                        print(f\"  * Min: {stats['min']:.2f}\")\n",
    "                        print(f\"  * 25%: {stats['25%']:.2f}\")\n",
    "                        print(f\"  * Médiane: {stats['50%']:.2f}\")\n",
    "                        print(f\"  * 75%: {stats['75%']:.2f}\")\n",
    "                        print(f\"  * Max: {stats['max']:.2f}\")\n",
    "                        \n",
    "                        # Vérifications spécifiques selon le type de variable\n",
    "                        if col.startswith('MEN') or col.startswith('IND'):\n",
    "                            if (X_train[col] < 0).any():\n",
    "                                print(\"⚠️ ATTENTION: Valeurs négatives détectées pour un comptage!\")\n",
    "                        \n",
    "                        # Vérification de la cohérence des proportions\n",
    "                        if 'PROP' in col or 'PAUV' in col:\n",
    "                            if (X_train[col] < 0).any() or (X_train[col] > 1).any():\n",
    "                                print(\"⚠️ ATTENTION: Proportions hors de l'intervalle [0,1]!\")\n",
    "                        \n",
    "                        median_val = stats['50%']\n",
    "                        X_train[col].fillna(median_val, inplace=True)\n",
    "                        X_test[col].fillna(median_val, inplace=True)\n",
    "                        print(f\"- Imputation avec la médiane: {median_val}\")\n",
    "                        \n",
    "                    else:\n",
    "                        # Pour les variables catégorielles\n",
    "                        value_counts = X_train[col].value_counts()\n",
    "                        print(\"- Distribution des valeurs:\")\n",
    "                        print(value_counts.head())\n",
    "                        print(f\"- Nombre total de catégories: {len(value_counts)}\")\n",
    "                        \n",
    "                        X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                        X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "                        print(\"- Imputation avec 'UNKNOWN'\")\n",
    "                else:\n",
    "                    print(\"- Colonne ignorée (>70% manquant)\")\n",
    "                \n",
    "                # Vérifications post-traitement\n",
    "                missing_after = X_train[col].isnull().sum()\n",
    "                if missing_after > 0:\n",
    "                    print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "                \n",
    "                # Vérification de la cohérence des données\n",
    "                if col.startswith('MEN'):\n",
    "                    total_men = X_train['MEN'] if 'MEN' in X_train.columns else None\n",
    "                    if total_men is not None and col != 'MEN':\n",
    "                        if (X_train[col] > total_men).any():\n",
    "                            print(\"⚠️ ATTENTION: Sous-catégorie de ménages supérieure au total!\")\n",
    "                \n",
    "                if col.startswith('IND'):\n",
    "                    total_ind = X_train['IND'] if 'IND' in X_train.columns else None\n",
    "                    if total_ind is not None and col != 'IND':\n",
    "                        if (X_train[col] > total_ind).any():\n",
    "                            print(\"⚠️ ATTENTION: Sous-catégorie d'individus supérieure au total!\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "# Exécution\n",
    "X_train, X_test, demographic_indicators = handle_demographic_vars(\n",
    "    X_train, X_test, missing_summary, groups['Demographic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajout de 130 indicateurs du groupe weather\n",
      "Ajout de 16 indicateurs du groupe building\n",
      "Ajout de 2 indicateurs du groupe activity\n",
      "Ajout de 34 indicateurs du groupe insurance\n",
      "⚠️ Attention: Les indicateurs geographic sont manquants ou vides\n",
      "Ajout de 28 indicateurs du groupe demographic\n",
      "\n",
      "=== Début de la finalisation ===\n",
      "\n",
      "État initial:\n",
      "X_train: (383610, 374)\n",
      "X_test: (95852, 374)\n",
      "Nombre d'indicateurs: 210\n",
      "\n",
      "Vérification des index:\n",
      "Index train unique: True\n",
      "Index test unique: True\n",
      "\n",
      "Création des DataFrames d'indicateurs...\n",
      "Shape indicateurs train: (383610, 210)\n",
      "Shape indicateurs test: (95852, 210)\n",
      "\n",
      "Concaténation avec les données principales...\n",
      "\n",
      "Suppression de 7 colonnes avec >70% de valeurs manquantes:\n",
      "['DEROG14', 'DEROG13', 'DEROG16', 'CARACT2', 'CARACT3', 'TYPBAT1', 'DEROG12']\n",
      "\n",
      "=== État final des données ===\n",
      "Shape après traitement:\n",
      "X_train: (383610, 577)\n",
      "X_test: (95852, 577)\n",
      "\n",
      "Colonnes avec valeurs manquantes restantes dans train:\n",
      "DISTANCE_112    217712\n",
      "DISTANCE_121    217712\n",
      "DISTANCE_122    217712\n",
      "DISTANCE_123    217712\n",
      "DISTANCE_124    217712\n",
      "                 ...  \n",
      "ALTITUDE_4       18582\n",
      "ALTITUDE_5       18582\n",
      "ZONE_VENT        18582\n",
      "NB_CASERNES      18582\n",
      "ESPINSEE        151542\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "Colonnes avec valeurs manquantes restantes dans test:\n",
      "DISTANCE_112    54577\n",
      "DISTANCE_121    54577\n",
      "DISTANCE_122    54577\n",
      "DISTANCE_123    54577\n",
      "DISTANCE_124    54577\n",
      "                ...  \n",
      "ALTITUDE_4       4670\n",
      "ALTITUDE_5       4670\n",
      "ZONE_VENT        4670\n",
      "NB_CASERNES      4670\n",
      "ESPINSEE        38026\n",
      "Length: 67, dtype: int64\n",
      "\n",
      "Types de données dans le jeu final:\n",
      "int32      210\n",
      "float64    165\n",
      "object     144\n",
      "int64       58\n",
      "dtype: int64\n",
      "\n",
      "Colonnes cohérentes entre train et test\n"
     ]
    }
   ],
   "source": [
    "# Cellule 10 : Finalisation\n",
    "def finalize_processing(X_train, X_test, all_indicators, missing_summary):\n",
    "    \"\"\"\n",
    "    Finalise le traitement des données avec vérifications détaillées\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Début de la finalisation ===\")\n",
    "    \n",
    "    # État initial\n",
    "    print(\"\\nÉtat initial:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"Nombre d'indicateurs: {len(all_indicators)}\")\n",
    "    \n",
    "    # Vérification des index\n",
    "    print(\"\\nVérification des index:\")\n",
    "    print(f\"Index train unique: {X_train.index.is_unique}\")\n",
    "    print(f\"Index test unique: {X_test.index.is_unique}\")\n",
    "    \n",
    "    # Combiner les indicateurs\n",
    "    print(\"\\nCréation des DataFrames d'indicateurs...\")\n",
    "    indicators_df_train = pd.DataFrame(all_indicators, index=X_train.index)\n",
    "    indicators_df_test = pd.DataFrame(all_indicators, index=X_test.index)\n",
    "    \n",
    "    print(f\"Shape indicateurs train: {indicators_df_train.shape}\")\n",
    "    print(f\"Shape indicateurs test: {indicators_df_test.shape}\")\n",
    "    \n",
    "    # Concaténation\n",
    "    print(\"\\nConcaténation avec les données principales...\")\n",
    "    X_train = pd.concat([X_train, indicators_df_train], axis=1)\n",
    "    X_test = pd.concat([X_test, indicators_df_test], axis=1)\n",
    "    \n",
    "    # Suppression des colonnes avec beaucoup de valeurs manquantes\n",
    "    high_missing = missing_summary[missing_summary['Missing Percentage'] > 70].index\n",
    "    print(f\"\\nSuppression de {len(high_missing)} colonnes avec >70% de valeurs manquantes:\")\n",
    "    print(high_missing.tolist())\n",
    "    \n",
    "    X_train.drop(high_missing, axis=1, inplace=True)\n",
    "    X_test.drop(high_missing, axis=1, inplace=True)\n",
    "    \n",
    "    # Vérifications finales\n",
    "    print(\"\\n=== État final des données ===\")\n",
    "    print(f\"Shape après traitement:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Vérification des valeurs manquantes\n",
    "    final_missing_train = X_train.isnull().sum()\n",
    "    final_missing_test = X_test.isnull().sum()\n",
    "    \n",
    "    if final_missing_train.sum() > 0:\n",
    "        print(\"\\nColonnes avec valeurs manquantes restantes dans train:\")\n",
    "        print(final_missing_train[final_missing_train > 0])\n",
    "    else:\n",
    "        print(\"\\nAucune valeur manquante restante dans train\")\n",
    "        \n",
    "    if final_missing_test.sum() > 0:\n",
    "        print(\"\\nColonnes avec valeurs manquantes restantes dans test:\")\n",
    "        print(final_missing_test[final_missing_test > 0])\n",
    "    else:\n",
    "        print(\"\\nAucune valeur manquante restante dans test\")\n",
    "    \n",
    "    # Vérification des types de données\n",
    "    print(\"\\nTypes de données dans le jeu final:\")\n",
    "    print(X_train.dtypes.value_counts())\n",
    "    \n",
    "    # Vérification de la cohérence train/test\n",
    "    train_cols = set(X_train.columns)\n",
    "    test_cols = set(X_test.columns)\n",
    "    \n",
    "    if train_cols != test_cols:\n",
    "        print(\"\\n⚠️ ATTENTION: Différences dans les colonnes train/test!\")\n",
    "        print(\"Colonnes uniquement dans train:\", train_cols - test_cols)\n",
    "        print(\"Colonnes uniquement dans test:\", test_cols - train_cols)\n",
    "    else:\n",
    "        print(\"\\nColonnes cohérentes entre train et test\")\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Initialiser le dictionnaire des indicateurs\n",
    "all_indicators = {}\n",
    "\n",
    "# Liste des groupes d'indicateurs à vérifier\n",
    "indicator_groups = {\n",
    "    'weather': ('weather_indicators', weather_indicators if 'weather_indicators' in globals() else {}),\n",
    "    'building': ('building_indicators', building_indicators if 'building_indicators' in globals() else {}),\n",
    "    'activity': ('activity_indicators', activity_indicators if 'activity_indicators' in globals() else {}),\n",
    "    'insurance': ('insurance_indicators', insurance_indicators if 'insurance_indicators' in globals() else {}),\n",
    "    'geographic': ('geographic_indicators', geographic_indicators if 'geographic_indicators' in globals() else {}),\n",
    "    'demographic': ('demographic_indicators', demographic_indicators if 'demographic_indicators' in globals() else {})\n",
    "}\n",
    "\n",
    "# Ajouter chaque groupe d'indicateurs disponible\n",
    "for group_name, (var_name, indicators) in indicator_groups.items():\n",
    "    if indicators:\n",
    "        print(f\"Ajout de {len(indicators)} indicateurs du groupe {group_name}\")\n",
    "        all_indicators.update(indicators)\n",
    "    else:\n",
    "        print(f\"⚠️ Attention: Les indicateurs {group_name} sont manquants ou vides\")\n",
    "\n",
    "# Traitement final\n",
    "X_train_processed, X_test_processed = finalize_processing(\n",
    "    X_train, X_test, all_indicators, missing_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def flatten_vars(var_dict):\n",
    "    \"\"\"\n",
    "    Aplatit un dictionnaire imbriqué de variables en une liste.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for k, v in var_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            for sub_v in v.values():\n",
    "                if isinstance(sub_v, list):\n",
    "                    flat_list.extend(sub_v)\n",
    "                elif isinstance(sub_v, dict):\n",
    "                    flat_list.extend(sum(sub_v.values(), []))\n",
    "        elif isinstance(v, list):\n",
    "            flat_list.extend(v)\n",
    "    return flat_list\n",
    "\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyse détaillée des valeurs manquantes par groupe et par tranche.\n",
    "    \"\"\"\n",
    "    # Définir les groupes de variables\n",
    "    groups = {\n",
    "        'Weather': sum(WEATHER_VARS.values(), []),\n",
    "        'Building': sum(BUILDING_VARS.values(), []),\n",
    "        'Insurance': flatten_vars(INSURANCE_VARS),\n",
    "        'Activity': flatten_vars(ACTIVITY_VARS), \n",
    "        'Geographic': flatten_vars(GEOGRAPHIC_VARS),\n",
    "        'Demographic': flatten_vars(DEMOGRAPHIC_VARS)\n",
    "    }\n",
    "    \n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': df.isnull().sum(),\n",
    "        'Missing Percentage': df.isnull().sum() / len(df) * 100\n",
    "    }).sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    # Définir les tranches\n",
    "    ranges = {\n",
    "        '0%': (0, 0),\n",
    "        '< 10%': (0, 10),\n",
    "        '10-30%': (10, 30),\n",
    "        '30-50%': (30, 50),\n",
    "        '50-70%': (50, 70),\n",
    "        '70-90%': (70, 90),\n",
    "        '> 90%': (90, 100)\n",
    "    }\n",
    "    \n",
    "    # Analyser par tranche et par groupe\n",
    "    total_cols = 0\n",
    "    print(\"=== Analyse des valeurs manquantes par tranche ===\\n\")\n",
    "    \n",
    "    for range_name, (min_val, max_val) in ranges.items():\n",
    "        if range_name == '0%':\n",
    "            range_cols = missing_summary[missing_summary['Missing Percentage'] == 0]\n",
    "        else:\n",
    "            range_cols = missing_summary[\n",
    "                (missing_summary['Missing Percentage'] > min_val) & \n",
    "                (missing_summary['Missing Percentage'] <= max_val)\n",
    "            ]\n",
    "        \n",
    "        n_cols = len(range_cols)\n",
    "        total_cols += n_cols\n",
    "        \n",
    "        if n_cols > 0:\n",
    "            print(f\"\\n=== Tranche {range_name} ({n_cols} colonnes) ===\")\n",
    "            # Analyser par groupe\n",
    "            for group_name, cols in groups.items():\n",
    "                group_cols = [col for col in range_cols.index if col in cols]\n",
    "                if group_cols:\n",
    "                    print(f\"\\n{group_name}: {len(group_cols)} colonnes\")\n",
    "                    print(f\"Variables: {group_cols}\")\n",
    "    \n",
    "    print(f\"\\nTotal des colonnes analysées: {total_cols}\")\n",
    "    print(f\"Nombre total de colonnes: {len(df.columns)}\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "def handle_missing_values(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Traite les valeurs manquantes avec une approche optimisée.\n",
    "    \"\"\"\n",
    "    print(\"=== État initial des données ===\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    # Analyse initiale\n",
    "    missing_summary = analyze_missing_values(X_train)\n",
    "    \n",
    "    # Identifier les types de colonnes\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Définir les groupes de variables\n",
    "    groups = {\n",
    "        'Weather': sum(WEATHER_VARS.values(), []),\n",
    "        'Building': sum(BUILDING_VARS.values(), []),\n",
    "        'Activity': flatten_vars(ACTIVITY_VARS), \n",
    "        'Insurance': flatten_vars(INSURANCE_VARS),\n",
    "        'Geographic': flatten_vars(GEOGRAPHIC_VARS),\n",
    "        'Demographic': flatten_vars(DEMOGRAPHIC_VARS)\n",
    "    }\n",
    "    \n",
    "    # Créer des DataFrames pour stocker les indicateurs de valeurs manquantes\n",
    "    missing_indicators_train = pd.DataFrame()\n",
    "    missing_indicators_test = pd.DataFrame()\n",
    "    \n",
    "    # Traitement par groupe\n",
    "    print(\"\\n=== Traitement par groupe ===\")\n",
    "    for group_name, cols in groups.items():\n",
    "        present_cols = [col for col in cols if col in X_train.columns]\n",
    "        print(f\"\\nTraitement {group_name}: {len(present_cols)} colonnes\")\n",
    "        \n",
    "        for col in present_cols:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            \n",
    "            # Créer indicateur pour toutes les colonnes avec valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators_train[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "                missing_indicators_test[f'{col}_is_missing'] = X_test[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct > 70:\n",
    "                continue\n",
    "                \n",
    "            if col in numeric_cols:\n",
    "                # Imputation simple avec la médiane\n",
    "                median_value = X_train[col].median()\n",
    "                X_train[col].fillna(median_value, inplace=True)\n",
    "                X_test[col].fillna(median_value, inplace=True)\n",
    "            \n",
    "            elif col in categorical_cols:\n",
    "                X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "    \n",
    "    # Ajouter les indicateurs de valeurs manquantes\n",
    "    X_train = pd.concat([X_train, missing_indicators_train], axis=1)\n",
    "    X_test = pd.concat([X_test, missing_indicators_test], axis=1)\n",
    "    \n",
    "    # Supprimer les colonnes avec plus de 70% de valeurs manquantes\n",
    "    high_missing = missing_summary[missing_summary['Missing Percentage'] > 70].index\n",
    "    X_train.drop(high_missing, axis=1, inplace=True)\n",
    "    X_test.drop(high_missing, axis=1, inplace=True)\n",
    "    \n",
    "    # Vérification finale\n",
    "    print(\"\\n=== État final des données ===\")\n",
    "    print(f\"Shape après traitement:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    final_missing = X_train.isnull().sum()\n",
    "    if final_missing.sum() > 0:\n",
    "        print(\"\\nColonnes avec valeurs manquantes restantes:\")\n",
    "        print(final_missing[final_missing > 0])\n",
    "    else:\n",
    "        print(\"\\nAucune valeur manquante restante\")\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Application\n",
    "exclude_cols = ['FREQ', 'CM', 'CHARGE']\n",
    "X_train = train_df.drop(exclude_cols, axis=1)\n",
    "y_train = train_df[['FREQ', 'CM', 'CHARGE']]\n",
    "X_test = test_df.copy()\n",
    "\n",
    "X_train_processed, X_test_processed = handle_missing_values(X_train, X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def analyze_missing_values():\n",
    "    \"\"\"Analyse les valeurs manquantes par tranches et par groupes.\"\"\"\n",
    "    \n",
    "    # 1. Calculer le pourcentage de valeurs manquantes pour chaque colonne\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': train_df.isnull().sum(),\n",
    "        'Missing Percentage': train_df.isnull().sum() / len(train_df) * 100\n",
    "    }).sort_values('Missing Percentage', ascending=False)\n",
    "    \n",
    "    # 2. Définir les tranches de valeurs manquantes\n",
    "    ranges = {\n",
    "        '< 10%': (0, 10),\n",
    "        '10-30%': (10, 30),\n",
    "        '30-50%': (30, 50),\n",
    "        '50-70%': (50, 70),\n",
    "        '70-90%': (70, 90),\n",
    "        '> 90%': (90, 100)\n",
    "    }\n",
    "    \n",
    "    # 3. Fonction helper pour aplatir les dictionnaires imbriqués\n",
    "    def flatten_vars(var_dict):\n",
    "        flat_list = []\n",
    "        for k, v in var_dict.items():\n",
    "            if isinstance(v, dict):\n",
    "                for sub_v in v.values():\n",
    "                    if isinstance(sub_v, list):\n",
    "                        flat_list.extend(sub_v)\n",
    "                    elif isinstance(sub_v, dict):\n",
    "                        flat_list.extend(sum(sub_v.values(), []))\n",
    "            elif isinstance(v, list):\n",
    "                flat_list.extend(v)\n",
    "        return flat_list\n",
    "    \n",
    "    # 4. Analyser par groupe et par tranche\n",
    "    print(\"=== Analyse des valeurs manquantes par groupe et par tranche ===\\n\")\n",
    "    \n",
    "    for range_name, (min_val, max_val) in ranges.items():\n",
    "        print(f\"\\n=== Tranche {range_name} ===\")\n",
    "        \n",
    "        # Filtrer les colonnes dans cette tranche\n",
    "        range_missing = missing_summary[\n",
    "            (missing_summary['Missing Percentage'] > min_val) & \n",
    "            (missing_summary['Missing Percentage'] <= max_val)\n",
    "        ]\n",
    "        \n",
    "        if range_missing.empty:\n",
    "            print(\"Aucune variable dans cette tranche\")\n",
    "            continue\n",
    "        \n",
    "        # Analyser par groupe\n",
    "        missing_by_group = {\n",
    "            'Weather': [col for col in range_missing.index if col in \n",
    "                       sum(WEATHER_VARS.values(), [])],\n",
    "            'Building': [col for col in range_missing.index if col in \n",
    "                        flatten_vars(BUILDING_VARS)],\n",
    "            'Insurance': [col for col in range_missing.index if col in \n",
    "                         flatten_vars(INSURANCE_VARS)],\n",
    "            'Geographic': [col for col in range_missing.index if col in \n",
    "                          flatten_vars(GEOGRAPHIC_VARS)],\n",
    "            'Demographic': [col for col in range_missing.index if col in \n",
    "                           flatten_vars(DEMOGRAPHIC_VARS)],\n",
    "            'Activity': [col for col in range_missing.index if col in \n",
    "                        flatten_vars(ACTIVITY_VARS)],\n",
    "            'Target': [col for col in range_missing.index if col in \n",
    "                      flatten_vars(TARGET_VARS)],\n",
    "            'ID': [col for col in range_missing.index if col in \n",
    "                   ID_VARS['identifiers']],\n",
    "            'Others': []\n",
    "        }\n",
    "        \n",
    "        # Ajouter les colonnes non classées dans 'Others'\n",
    "        all_grouped = sum(missing_by_group.values(), [])\n",
    "        missing_by_group['Others'] = [col for col in range_missing.index \n",
    "                                    if col not in all_grouped]\n",
    "        \n",
    "        # Afficher les résultats pour cette tranche\n",
    "        for group, cols in missing_by_group.items():\n",
    "            if cols:  # N'afficher que les groupes non vides\n",
    "                print(f\"\\n{group}: {len(cols)} colonnes\")\n",
    "                print(f\"Pourcentage moyen de valeurs manquantes: \"\n",
    "                      f\"{missing_summary.loc[cols, 'Missing Percentage'].mean():.2f}%\")\n",
    "                print(f\"Variables: {cols}\")\n",
    "    \n",
    "    # 5. Résumé global\n",
    "    print(\"\\n=== Résumé global ===\")\n",
    "    print(\"Distribution des valeurs manquantes par tranche:\")\n",
    "    for range_name, (min_val, max_val) in ranges.items():\n",
    "        n_cols = len(missing_summary[\n",
    "            (missing_summary['Missing Percentage'] > min_val) & \n",
    "            (missing_summary['Missing Percentage'] <= max_val)\n",
    "        ])\n",
    "        print(f\"- {range_name}: {n_cols} colonnes\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "# Exécuter l'analyse\n",
    "missing_summary = analyze_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def flatten_vars(var_dict):\n",
    "    \"\"\"\n",
    "    Aplatit un dictionnaire imbriqué de variables en une liste.\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for k, v in var_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            for sub_v in v.values():\n",
    "                if isinstance(sub_v, list):\n",
    "                    flat_list.extend(sub_v)\n",
    "                elif isinstance(sub_v, dict):\n",
    "                    flat_list.extend(sum(sub_v.values(), []))\n",
    "        elif isinstance(v, list):\n",
    "            flat_list.extend(v)\n",
    "    return flat_list\n",
    "\n",
    "def handle_missing_values(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Traite les valeurs manquantes avec vérifications détaillées à chaque étape.\n",
    "    \"\"\"\n",
    "    print(\"=== État initial des données ===\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    # 1. Analyse initiale des valeurs manquantes\n",
    "    print(\"\\n=== Analyse initiale des valeurs manquantes ===\")\n",
    "    missing_pct = (X_train.isnull().sum() / len(X_train) * 100).sort_values(ascending=False)\n",
    "    print(\"\\nDistribution des valeurs manquantes:\")\n",
    "    for threshold in [90, 70, 50, 30, 10, 0]:\n",
    "        n_cols = sum(missing_pct > threshold)\n",
    "        print(f\"> {threshold}%: {n_cols} colonnes\")\n",
    "    \n",
    "    # 2. Identifier les types de colonnes\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nTypes de colonnes:\")\n",
    "    print(f\"Numériques: {len(numeric_cols)}\")\n",
    "    print(f\"Catégorielles: {len(categorical_cols)}\")\n",
    "    \n",
    "    # 3. Traitement par groupe de variables\n",
    "    groups = {\n",
    "        'Weather': sum(WEATHER_VARS.values(), []),\n",
    "        'Building': sum(BUILDING_VARS.values(), []),\n",
    "        'Insurance': flatten_vars(INSURANCE_VARS),\n",
    "        'Geographic': flatten_vars(GEOGRAPHIC_VARS),\n",
    "        'Demographic': flatten_vars(DEMOGRAPHIC_VARS)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Traitement par groupe ===\")\n",
    "    for group_name, cols in groups.items():\n",
    "        print(f\"\\nTraitement {group_name}:\")\n",
    "        present_cols = [col for col in cols if col in X_train.columns]\n",
    "        print(f\"Colonnes trouvées: {len(present_cols)}/{len(cols)}\")\n",
    "        \n",
    "        for col in present_cols:\n",
    "            if col in numeric_cols:\n",
    "                # Créer indicateur de valeur manquante\n",
    "                X_train[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "                X_test[f'{col}_is_missing'] = X_test[col].isnull().astype(int)\n",
    "                \n",
    "                # Imputation selon le groupe\n",
    "                if group_name == 'Weather':\n",
    "                    group_col = 'ZONE'\n",
    "                elif group_name == 'Building':\n",
    "                    group_col = 'TYPBAT2'\n",
    "                elif group_name == 'Insurance':\n",
    "                    group_col = ['RISK1', 'RISK2']\n",
    "                else:\n",
    "                    group_col = 'ZONE'\n",
    "                \n",
    "                # Imputation avec vérification\n",
    "                try:\n",
    "                    if isinstance(group_col, list):\n",
    "                        X_train[col] = X_train.groupby(group_col)[col].transform(\n",
    "                            lambda x: x.fillna(x.median()))\n",
    "                        X_test[col] = X_test.groupby(group_col)[col].transform(\n",
    "                            lambda x: x.fillna(x.median()))\n",
    "                    else:\n",
    "                        X_train[col] = X_train.groupby(group_col)[col].transform(\n",
    "                            lambda x: x.fillna(x.median()))\n",
    "                        X_test[col] = X_test.groupby(group_col)[col].transform(\n",
    "                            lambda x: x.fillna(x.median()))\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur pour {col}: {str(e)}\")\n",
    "                    # Fallback à la médiane globale\n",
    "                    X_train[col] = X_train[col].fillna(X_train[col].median())\n",
    "                    X_test[col] = X_test[col].fillna(X_train[col].median())\n",
    "            \n",
    "            elif col in categorical_cols:\n",
    "                X_train[col] = X_train[col].fillna('UNKNOWN')\n",
    "                X_test[col] = X_test[col].fillna('UNKNOWN')\n",
    "    \n",
    "    # 4. Vérification des colonnes très manquantes\n",
    "    print(\"\\n=== Colonnes avec beaucoup de valeurs manquantes ===\")\n",
    "    high_missing_pct = (X_train.isnull().sum() / len(X_train) * 100)\n",
    "    high_missing = high_missing_pct[high_missing_pct > 70].index.tolist()\n",
    "    print(f\"Colonnes >70% manquantes avant suppression:\")\n",
    "    for col in high_missing:\n",
    "        print(f\"{col}: {high_missing_pct[col]:.2f}%\")\n",
    "    \n",
    "    # 5. Suppression des colonnes\n",
    "    X_train = X_train.drop(high_missing, axis=1)\n",
    "    X_test = X_test.drop(high_missing, axis=1)\n",
    "    \n",
    "    # 6. Vérification finale\n",
    "    print(\"\\n=== État final des données ===\")\n",
    "    print(f\"Shape après traitement:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    missing_final = X_train.isnull().sum()\n",
    "    if missing_final.sum() > 0:\n",
    "        print(\"\\nValeurs manquantes restantes:\")\n",
    "        print(missing_final[missing_final > 0])\n",
    "    else:\n",
    "        print(\"\\nAucune valeur manquante restante\")\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# Application\n",
    "exclude_cols = ['FREQ', 'CM', 'CHARGE']\n",
    "X_train = train_df.drop(exclude_cols, axis=1)\n",
    "y_train = train_df[['FREQ', 'CM', 'CHARGE']]\n",
    "X_test = test_df.copy()\n",
    "\n",
    "X_train_processed, X_test_processed = handle_missing_values(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_strategies():\n",
    "    print(\"=== Stratégies proposées par groupe ===\\n\")\n",
    "    \n",
    "    strategies = {\n",
    "        'Weather': {\n",
    "            'description': \"Variables météorologiques (température, vent, pluie)\",\n",
    "            'options': [\n",
    "                \"Imputation par la moyenne/médiane par zone géographique\",\n",
    "                \"Création d'indicateurs de données manquantes\",\n",
    "                \"Agrégation en variables de synthèse (ex: moyenne par saison)\",\n",
    "                \"Utilisation des stations météo les plus proches\"\n",
    "            ]\n",
    "        },\n",
    "        'Building': {\n",
    "            'description': \"Caractéristiques physiques des bâtiments\",\n",
    "            'options': [\n",
    "                \"Imputation par mode pour les catégories similaires\",\n",
    "                \"Création de catégories 'UNKNOWN'\",\n",
    "                \"Utilisation des corrélations entre surface/hauteur/nombre de bâtiments\",\n",
    "                \"Agrégation des caractéristiques similaires\"\n",
    "            ]\n",
    "        },\n",
    "        'Insurance': {\n",
    "            'description': \"Variables d'assurance et risque\",\n",
    "            'options': [\n",
    "                \"Conservation stricte - pas d'imputation pour les variables critiques\",\n",
    "                \"Imputation basée sur les profils de risque similaires\",\n",
    "                \"Utilisation des règles métier pour l'imputation\",\n",
    "                \"Création de catégories de risque agrégées\"\n",
    "            ]\n",
    "        },\n",
    "        'Geographic': {\n",
    "            'description': \"Variables de localisation et environnement\",\n",
    "            'options': [\n",
    "                \"Imputation par plus proche voisin géographique\",\n",
    "                \"Agrégation à un niveau géographique plus large\",\n",
    "                \"Création de clusters géographiques\",\n",
    "                \"Utilisation des données d'occupation des sols\"\n",
    "            ]\n",
    "        },\n",
    "        'Demographic': {\n",
    "            'description': \"Variables socio-démographiques\",\n",
    "            'options': [\n",
    "                \"Imputation par la moyenne du quartier/zone\",\n",
    "                \"Utilisation des données INSEE du niveau supérieur\",\n",
    "                \"Création d'indices composites\",\n",
    "                \"Agrégation par profil démographique\"\n",
    "            ]\n",
    "        },\n",
    "        'Activity': {\n",
    "            'description': \"Variables d'activité et équipement\",\n",
    "            'options': [\n",
    "                \"Imputation basée sur l'activité principale\",\n",
    "                \"Création de groupes d'activité simplifiés\",\n",
    "                \"Utilisation des corrélations activité/équipement\",\n",
    "                \"Conservation uniquement des équipements principaux\"\n",
    "            ]\n",
    "        },\n",
    "        'Target': {\n",
    "            'description': \"Variables cibles (FREQ, CM, CHARGE)\",\n",
    "            'options': [\n",
    "                \"Aucune imputation - suppression des lignes\",\n",
    "                \"Analyse séparée pour chaque variable cible\",\n",
    "                \"Création d'indicateurs de fiabilité\",\n",
    "                \"Modélisation spécifique pour les valeurs manquantes\"\n",
    "            ]\n",
    "        },\n",
    "        'ID': {\n",
    "            'description': \"Variables d'identification\",\n",
    "            'options': [\n",
    "                \"Aucune imputation - suppression des lignes\",\n",
    "                \"Création de nouveaux identifiants\",\n",
    "                \"Vérification des doublons et incohérences\",\n",
    "                \"Traçabilité des modifications\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Affichage des stratégies\n",
    "    for group, info in strategies.items():\n",
    "        print(f\"\\n{group} - {info['description']}:\")\n",
    "        print(\"Stratégies proposées:\")\n",
    "        for i, opt in enumerate(info['options'], 1):\n",
    "            print(f\"{i}. {opt}\")\n",
    "        \n",
    "        # Afficher les variables concernées\n",
    "        if group == 'Weather':\n",
    "            vars_list = sum(WEATHER_VARS.values(), [])\n",
    "        elif group == 'Building':\n",
    "            vars_list = sum(BUILDING_VARS.values(), [])\n",
    "        elif group == 'Insurance':\n",
    "            vars_list = sum([v if isinstance(v, list) else sum(v.values(), []) \n",
    "                           for v in INSURANCE_VARS.values()], [])\n",
    "        elif group == 'Geographic':\n",
    "            vars_list = sum([v if isinstance(v, list) else sum(v.values(), []) \n",
    "                           for v in GEOGRAPHIC_VARS.values()], [])\n",
    "        elif group == 'Demographic':\n",
    "            vars_list = sum([sum(v.values(), []) for v in DEMOGRAPHIC_VARS.values()], [])\n",
    "        elif group == 'Activity':\n",
    "            vars_list = sum(ACTIVITY_VARS.values(), [])\n",
    "        elif group == 'Target':\n",
    "            vars_list = sum([v for v in TARGET_VARS['primary'].values()], [])\n",
    "        elif group == 'ID':\n",
    "            vars_list = ID_VARS['identifiers']\n",
    "            \n",
    "        print(f\"\\nNombre de variables: {len(vars_list)}\")\n",
    "        print(f\"Exemples: {vars_list[:3]}...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "# Exécuter l'analyse\n",
    "strategies = propose_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weather_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables météorologiques ===\")\n",
    "    \n",
    "    # Utiliser la structure existante de WEATHER_VARS\n",
    "    weather_groups = {\n",
    "        'temp_max': [col for col in train_df.columns if any(x in col for x in ['NBJTX', 'TX', 'TMMAX', 'TXMAX'])],\n",
    "        'temp_min': [col for col in train_df.columns if any(x in col for x in ['NBJTN', 'TN', 'TMMIN', 'TNMIN'])],\n",
    "        'temp_moy': [col for col in train_df.columns if any(x in col for x in ['TM', 'TMM'])],\n",
    "        'temp_amplitude': [col for col in train_df.columns if 'TAMPLI' in col],\n",
    "        'wind': [col for col in train_df.columns if any(x in col for x in [\n",
    "            'NBJFF', 'NBJFXI3S', 'NBJFXY', 'FFM_VOR', 'FXI3SAB_VOR', \n",
    "            'FXIAB_VOR', 'FXYAB_VOR', 'FFM_vor_com', 'FXI3SAB_vor_com'\n",
    "        ])],\n",
    "        'rain': [col for col in train_df.columns if any(x in col for x in [\n",
    "            'NBJRR', 'RR_VOR', 'RRAB_VOR'\n",
    "        ])]\n",
    "    }\n",
    "    \n",
    "    # Types de mesures\n",
    "    measure_types = {\n",
    "        'moyenne': 'MM_A',    # Moyenne annuelle\n",
    "        'maximum': 'MMAX_A',  # Maximum annuel\n",
    "        'somme': 'MSOM_A'    # Somme annuelle\n",
    "    }\n",
    "    \n",
    "    new_weather_cols = []\n",
    "    \n",
    "    # Pour chaque groupe de variables météo\n",
    "    for weather_type, cols in weather_groups.items():\n",
    "        print(f\"\\n{weather_type.capitalize()} - {len(cols)} variables\")\n",
    "        print(\"Exemples de colonnes:\", cols[:3])\n",
    "        \n",
    "        # Pour chaque type de mesure\n",
    "        for measure_name, measure_suffix in measure_types.items():\n",
    "            measure_cols = [col for col in cols if measure_suffix in col]\n",
    "            \n",
    "            if measure_cols:\n",
    "                # Créer différentes agrégations\n",
    "                aggregations = {\n",
    "                    'mean': f'{weather_type}_{measure_name}_mean',\n",
    "                    'min': f'{weather_type}_{measure_name}_min',\n",
    "                    'max': f'{weather_type}_{measure_name}_max'\n",
    "                }\n",
    "                \n",
    "                for agg_type, col_name in aggregations.items():\n",
    "                    if agg_type == 'mean':\n",
    "                        train_df[col_name] = train_df[measure_cols].mean(axis=1)\n",
    "                        test_df[col_name] = test_df[measure_cols].mean(axis=1)\n",
    "                    elif agg_type == 'min':\n",
    "                        train_df[col_name] = train_df[measure_cols].min(axis=1)\n",
    "                        test_df[col_name] = test_df[measure_cols].min(axis=1)\n",
    "                    elif agg_type == 'max':\n",
    "                        train_df[col_name] = train_df[measure_cols].max(axis=1)\n",
    "                        test_df[col_name] = test_df[measure_cols].max(axis=1)\n",
    "                    \n",
    "                    new_weather_cols.append(col_name)\n",
    "    \n",
    "    # Créer des ratios et variations\n",
    "    for weather_type in ['temp_max', 'temp_min', 'wind', 'rain']:\n",
    "        mean_cols = [col for col in new_weather_cols if weather_type in col and 'mean' in col]\n",
    "        max_cols = [col for col in new_weather_cols if weather_type in col and 'max' in col]\n",
    "        \n",
    "        if mean_cols and max_cols:\n",
    "            # Ratio max/mean\n",
    "            ratio_col = f'{weather_type}_max_mean_ratio'\n",
    "            train_df[ratio_col] = train_df[max_cols[0]] / train_df[mean_cols[0]]\n",
    "            test_df[ratio_col] = test_df[max_cols[0]] / test_df[mean_cols[0]]\n",
    "            new_weather_cols.append(ratio_col)\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    weather_cols_to_drop = sum(weather_groups.values(), [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables météo ===\")\n",
    "    print(\"\\nNombre de variables créées:\", len(new_weather_cols))\n",
    "    print(\"\\nExemples de nouvelles variables:\", new_weather_cols[:5])\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives:\")\n",
    "    print(train_df[new_weather_cols].describe())\n",
    "    \n",
    "    print(\"\\nValeurs manquantes dans les nouvelles variables:\")\n",
    "    missing_pct = (train_df[new_weather_cols].isnull().mean() * 100).round(2)\n",
    "    print(missing_pct[missing_pct > 0])\n",
    "    \n",
    "    return {\n",
    "        'to_drop': weather_cols_to_drop,\n",
    "        'new_cols': new_weather_cols,\n",
    "        'stats': {\n",
    "            'n_original': len(weather_cols_to_drop),\n",
    "            'n_new': len(new_weather_cols),\n",
    "            'missing_pct': missing_pct.to_dict()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "weather_results = handle_weather_vars(train_df, test_df)\n",
    "\n",
    "# Afficher un résumé détaillé\n",
    "print(\"\\n=== Résumé des modifications ===\")\n",
    "print(f\"Variables originales: {weather_results['stats']['n_original']}\")\n",
    "print(f\"Nouvelles variables: {weather_results['stats']['n_new']}\")\n",
    "print(\"\\nRatio de réduction:\", \n",
    "      round(weather_results['stats']['n_new'] / weather_results['stats']['n_original'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_building_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables bâtiment ===\")\n",
    "    \n",
    "    new_building_cols = []\n",
    "    \n",
    "    # 1. Consolider les hauteurs\n",
    "    height_cols = BUILDING_VARS['height']\n",
    "    if height_cols:\n",
    "        print(\"\\nConsolidation des hauteurs:\")\n",
    "        height_aggs = {\n",
    "            'height_mean': 'mean',\n",
    "            'height_max': 'max',\n",
    "            'height_min': 'min'\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in height_aggs.items():\n",
    "            train_df[col_name] = train_df[height_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[height_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(height_aggs)} variables de hauteur agrégées\")\n",
    "    \n",
    "    # 2. Consolider les surfaces\n",
    "    surface_cols = BUILDING_VARS['surface']\n",
    "    if surface_cols:\n",
    "        print(\"\\nConsolidation des surfaces:\")\n",
    "        surface_aggs = {\n",
    "            'surface_total': 'sum',\n",
    "            'surface_mean': 'mean',\n",
    "            'surface_max': 'max',\n",
    "            'n_surfaces': lambda x: x.notna().sum()  # Nombre de surfaces renseignées\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in surface_aggs.items():\n",
    "            train_df[col_name] = train_df[surface_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[surface_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(surface_aggs)} variables de surface agrégées\")\n",
    "    \n",
    "    # 3. Consolider le nombre de bâtiments\n",
    "    building_cols = BUILDING_VARS['buildings']\n",
    "    if building_cols:\n",
    "        print(\"\\nConsolidation du nombre de bâtiments:\")\n",
    "        building_aggs = {\n",
    "            'buildings_total': 'sum',\n",
    "            'buildings_count': lambda x: x.notna().sum(),  # Nombre de types de bâtiments\n",
    "            'buildings_types': lambda x: (x > 0).sum()     # Nombre de types avec bâtiments\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in building_aggs.items():\n",
    "            train_df[col_name] = train_df[building_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[building_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(building_aggs)} variables de comptage de bâtiments\")\n",
    "    \n",
    "    # 4. Créer des ratios et indicateurs composites\n",
    "    print(\"\\nCréation d'indicateurs composites:\")\n",
    "    \n",
    "    # Ratio surface/bâtiment\n",
    "    if 'surface_total' in new_building_cols and 'buildings_total' in new_building_cols:\n",
    "        train_df['surface_per_building'] = (train_df['surface_total'] / \n",
    "                                          train_df['buildings_total'].replace(0, np.nan))\n",
    "        test_df['surface_per_building'] = (test_df['surface_total'] / \n",
    "                                         test_df['buildings_total'].replace(0, np.nan))\n",
    "        new_building_cols.append('surface_per_building')\n",
    "    \n",
    "    # Ratio hauteur/surface\n",
    "    if 'height_mean' in new_building_cols and 'surface_total' in new_building_cols:\n",
    "        train_df['height_surface_ratio'] = (train_df['height_mean'] / \n",
    "                                          train_df['surface_total'].replace(0, np.nan))\n",
    "        test_df['height_surface_ratio'] = (test_df['height_mean'] / \n",
    "                                         test_df['surface_total'].replace(0, np.nan))\n",
    "        new_building_cols.append('height_surface_ratio')\n",
    "    \n",
    "    # 5. Encoder les caractéristiques catégorielles\n",
    "    char_cols = BUILDING_VARS['characteristics']\n",
    "    if char_cols:\n",
    "        print(\"\\nEncodage des caractéristiques:\")\n",
    "        for col in char_cols:\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=f'char_{col}')\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=f'char_{col}')\n",
    "            \n",
    "            # Assurer les mêmes colonnes dans train et test\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_building_cols.extend(dummies.columns)\n",
    "        \n",
    "        print(f\"Créé {len(dummies.columns)} variables dummy pour les caractéristiques\")\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    building_cols_to_drop = sum([\n",
    "        BUILDING_VARS['height'],\n",
    "        BUILDING_VARS['surface'],\n",
    "        BUILDING_VARS['buildings'],\n",
    "        BUILDING_VARS['characteristics']\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables bâtiment ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_building_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(building_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_building_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': building_cols_to_drop,\n",
    "        'new_cols': new_building_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "building_results = handle_building_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = building_results['train_df']\n",
    "test_df = building_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_geographic_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables géographiques ===\")\n",
    "    \n",
    "    new_geo_cols = []\n",
    "    \n",
    "    # 1. Traiter les distances\n",
    "    if 'distances' in GEOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des distances:\")\n",
    "        \n",
    "        # Pour chaque type d'environnement\n",
    "        for env_type, cols in GEOGRAPHIC_VARS['distances'].items():\n",
    "            if cols:\n",
    "                print(f\"\\n{env_type.capitalize()} - {len(cols)} variables\")\n",
    "                \n",
    "                # Créer différentes agrégations\n",
    "                aggs = {\n",
    "                    f'distance_{env_type}_mean': 'mean',\n",
    "                    f'distance_{env_type}_min': 'min',  # Distance la plus proche\n",
    "                    f'distance_{env_type}_max': 'max'   # Distance la plus éloignée\n",
    "                }\n",
    "                \n",
    "                for col_name, agg_func in aggs.items():\n",
    "                    train_df[col_name] = train_df[cols].agg(agg_func, axis=1)\n",
    "                    test_df[col_name] = test_df[cols].agg(agg_func, axis=1)\n",
    "                    new_geo_cols.append(col_name)\n",
    "                \n",
    "                # Nombre d'éléments proches (distance < seuil)\n",
    "                thresholds = [1000, 5000]  # 1km, 5km\n",
    "                for threshold in thresholds:\n",
    "                    col_name = f'n_{env_type}_within_{threshold}m'\n",
    "                    train_df[col_name] = (train_df[cols] < threshold).sum(axis=1)\n",
    "                    test_df[col_name] = (test_df[cols] < threshold).sum(axis=1)\n",
    "                    new_geo_cols.append(col_name)\n",
    "    \n",
    "    # 2. Traiter les proportions\n",
    "    if 'proportions' in GEOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des proportions:\")\n",
    "        \n",
    "        for env_type, cols in GEOGRAPHIC_VARS['proportions'].items():\n",
    "            if cols:\n",
    "                print(f\"\\n{env_type.capitalize()} - {len(cols)} variables\")\n",
    "                \n",
    "                # Somme des proportions par type\n",
    "                col_name = f'proportion_{env_type}_total'\n",
    "                train_df[col_name] = train_df[cols].sum(axis=1)\n",
    "                test_df[col_name] = test_df[cols].sum(axis=1)\n",
    "                new_geo_cols.append(col_name)\n",
    "                \n",
    "                # Proportion dominante\n",
    "                col_name = f'proportion_{env_type}_max'\n",
    "                train_df[col_name] = train_df[cols].max(axis=1)\n",
    "                test_df[col_name] = test_df[cols].max(axis=1)\n",
    "                new_geo_cols.append(col_name)\n",
    "                \n",
    "                # Diversité (nombre de types > seuil)\n",
    "                col_name = f'diversity_{env_type}'\n",
    "                threshold = 0.1  # 10%\n",
    "                train_df[col_name] = (train_df[cols] > threshold).sum(axis=1)\n",
    "                test_df[col_name] = (test_df[cols] > threshold).sum(axis=1)\n",
    "                new_geo_cols.append(col_name)\n",
    "    \n",
    "    # 3. Traiter la topographie\n",
    "    if 'topography' in GEOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement de la topographie:\")\n",
    "        \n",
    "        # Altitude\n",
    "        altitude_cols = GEOGRAPHIC_VARS['topography']['altitude']\n",
    "        if altitude_cols:\n",
    "            altitude_aggs = {\n",
    "                'altitude_mean': 'mean',\n",
    "                'altitude_min': 'min',\n",
    "                'altitude_max': 'max',\n",
    "                'altitude_range': lambda x: x.max() - x.min()\n",
    "            }\n",
    "            \n",
    "            for col_name, agg_func in altitude_aggs.items():\n",
    "                train_df[col_name] = train_df[altitude_cols].agg(agg_func, axis=1)\n",
    "                test_df[col_name] = test_df[altitude_cols].agg(agg_func, axis=1)\n",
    "                new_geo_cols.append(col_name)\n",
    "    \n",
    "    # 4. Traiter les services d'urgence\n",
    "    if 'emergency' in GEOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des services d'urgence:\")\n",
    "        emergency_cols = GEOGRAPHIC_VARS['emergency']\n",
    "        # Garder ces variables telles quelles car déjà agrégées\n",
    "        new_geo_cols.extend(emergency_cols)\n",
    "    \n",
    "    # 5. Créer des indicateurs composites\n",
    "    print(\"\\nCréation d'indicateurs composites:\")\n",
    "    \n",
    "    # Ratio urbain/naturel\n",
    "    if 'proportion_urban_total' in new_geo_cols and 'proportion_natural_total' in new_geo_cols:\n",
    "        col_name = 'urban_natural_ratio'\n",
    "        train_df[col_name] = (train_df['proportion_urban_total'] / \n",
    "                             train_df['proportion_natural_total'].replace(0, np.nan))\n",
    "        test_df[col_name] = (test_df['proportion_urban_total'] / \n",
    "                            test_df['proportion_natural_total'].replace(0, np.nan))\n",
    "        new_geo_cols.append(col_name)\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    geo_cols_to_drop = sum([\n",
    "        sum(GEOGRAPHIC_VARS['distances'].values(), []),\n",
    "        sum(GEOGRAPHIC_VARS['proportions'].values(), []),\n",
    "        GEOGRAPHIC_VARS['topography']['altitude']\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables géographiques ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_geo_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(geo_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives:\")\n",
    "    print(train_df[new_geo_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': geo_cols_to_drop,\n",
    "        'new_cols': new_geo_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "geo_results = handle_geographic_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = geo_results['train_df']\n",
    "test_df = geo_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_insurance_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables d'assurance ===\")\n",
    "    \n",
    "    new_insurance_cols = []\n",
    "    \n",
    "    # 1. Traitement des indicateurs de risque\n",
    "    if 'risk_indicators' in INSURANCE_VARS:\n",
    "        print(\"\\nTraitement des indicateurs de risque:\")\n",
    "        risk_cols = INSURANCE_VARS['risk_indicators']\n",
    "        \n",
    "        # Agrégations des indicateurs de risque\n",
    "        risk_aggs = {\n",
    "            'risk_mean': 'mean',\n",
    "            'risk_max': 'max',\n",
    "            'risk_sum': 'sum',\n",
    "            'n_risks': lambda x: (x > 0).sum()  # Nombre de risques actifs\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in risk_aggs.items():\n",
    "            train_df[col_name] = train_df[risk_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[risk_cols].agg(agg_func, axis=1)\n",
    "            new_insurance_cols.append(col_name)\n",
    "    \n",
    "    # 2. Traitement des capitaux\n",
    "    if 'capital' in INSURANCE_VARS:\n",
    "        print(\"\\nTraitement des capitaux:\")\n",
    "        capital_cols = INSURANCE_VARS['capital']\n",
    "        \n",
    "        # Agrégations des capitaux\n",
    "        capital_aggs = {\n",
    "            'capital_total': 'sum',\n",
    "            'capital_mean': 'mean',\n",
    "            'capital_max': 'max',\n",
    "            'n_capitals': lambda x: x.notna().sum()  # Nombre de capitaux renseignés\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in capital_aggs.items():\n",
    "            train_df[col_name] = train_df[capital_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[capital_cols].agg(agg_func, axis=1)\n",
    "            new_insurance_cols.append(col_name)\n",
    "    \n",
    "    # 3. Traitement des dérogations\n",
    "    if 'derogation' in INSURANCE_VARS:\n",
    "        print(\"\\nTraitement des dérogations:\")\n",
    "        derog_cols = INSURANCE_VARS['derogation']\n",
    "        \n",
    "        # Nombre total de dérogations\n",
    "        train_df['n_derogations'] = (train_df[derog_cols] > 0).sum(axis=1)\n",
    "        test_df['n_derogations'] = (test_df[derog_cols] > 0).sum(axis=1)\n",
    "        new_insurance_cols.append('n_derogations')\n",
    "        \n",
    "        # Encodage one-hot des dérogations\n",
    "        for col in derog_cols:\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=f'derog_{col}')\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=f'derog_{col}')\n",
    "            \n",
    "            # Assurer les mêmes colonnes dans train et test\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_insurance_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 4. Traitement des termes du contrat\n",
    "    if 'contract_terms' in INSURANCE_VARS:\n",
    "        print(\"\\nTraitement des termes du contrat:\")\n",
    "        \n",
    "        # Encoder les franchises et indemnisations\n",
    "        for term_type, cols in INSURANCE_VARS['contract_terms'].items():\n",
    "            for col in cols:\n",
    "                dummies = pd.get_dummies(train_df[col], prefix=f'{term_type}_{col}')\n",
    "                test_dummies = pd.get_dummies(test_df[col], prefix=f'{term_type}_{col}')\n",
    "                \n",
    "                # Assurer les mêmes colonnes\n",
    "                for dummy_col in dummies.columns:\n",
    "                    if dummy_col not in test_dummies.columns:\n",
    "                        test_dummies[dummy_col] = 0\n",
    "                \n",
    "                train_df = pd.concat([train_df, dummies], axis=1)\n",
    "                test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "                new_insurance_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 5. Traitement de l'historique\n",
    "    if 'history' in INSURANCE_VARS:\n",
    "        print(\"\\nTraitement de l'historique:\")\n",
    "        history_cols = INSURANCE_VARS['history']\n",
    "        \n",
    "        # Garder ces variables telles quelles\n",
    "        new_insurance_cols.extend(history_cols)\n",
    "        \n",
    "        # Créer des ratios\n",
    "        if 'NBSINCONJ' in history_cols and 'NBSINSTRT' in history_cols:\n",
    "            train_df['ratio_sinistres'] = (train_df['NBSINCONJ'] / \n",
    "                                         train_df['NBSINSTRT'].replace(0, np.nan))\n",
    "            test_df['ratio_sinistres'] = (test_df['NBSINCONJ'] / \n",
    "                                        test_df['NBSINSTRT'].replace(0, np.nan))\n",
    "            new_insurance_cols.append('ratio_sinistres')\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    insurance_cols_to_drop = sum([\n",
    "        INSURANCE_VARS['risk_indicators'],\n",
    "        INSURANCE_VARS['capital'],\n",
    "        INSURANCE_VARS['derogation'],\n",
    "        sum(INSURANCE_VARS['contract_terms'].values(), [])\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables d'assurance ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_insurance_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(insurance_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_insurance_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': insurance_cols_to_drop,\n",
    "        'new_cols': new_insurance_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "insurance_results = handle_insurance_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = insurance_results['train_df']\n",
    "test_df = insurance_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_demographic_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables démographiques ===\")\n",
    "    \n",
    "    new_demo_cols = []\n",
    "    \n",
    "    # 1. Traitement des ménages\n",
    "    if 'household' in DEMOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des ménages:\")\n",
    "        \n",
    "        # Variables de base des ménages\n",
    "        men_total = DEMOGRAPHIC_VARS['household']['general'][0]  # 'MEN'\n",
    "        \n",
    "        # Ratios pour la composition des ménages\n",
    "        if 'composition' in DEMOGRAPHIC_VARS['household']:\n",
    "            comp_cols = DEMOGRAPHIC_VARS['household']['composition']\n",
    "            for col in comp_cols:\n",
    "                ratio_name = f'ratio_{col}'\n",
    "                train_df[ratio_name] = train_df[col] / train_df[men_total]\n",
    "                test_df[ratio_name] = test_df[col] / test_df[men_total]\n",
    "                new_demo_cols.append(ratio_name)\n",
    "        \n",
    "        # Ratios pour le type de logement\n",
    "        if 'housing_type' in DEMOGRAPHIC_VARS['household']:\n",
    "            house_cols = DEMOGRAPHIC_VARS['household']['housing_type']\n",
    "            for col in house_cols:\n",
    "                ratio_name = f'ratio_{col}'\n",
    "                train_df[ratio_name] = train_df[col] / train_df[men_total]\n",
    "                test_df[ratio_name] = test_df[col] / test_df[men_total]\n",
    "                new_demo_cols.append(ratio_name)\n",
    "        \n",
    "        # Indicateurs économiques\n",
    "        if 'economic' in DEMOGRAPHIC_VARS['household']:\n",
    "            eco_cols = DEMOGRAPHIC_VARS['household']['economic']\n",
    "            \n",
    "            # Ratio de ménages pauvres\n",
    "            if 'men_pauv' in eco_cols:\n",
    "                train_df['ratio_pauvrete'] = train_df['men_pauv'] / train_df[men_total]\n",
    "                test_df['ratio_pauvrete'] = test_df['men_pauv'] / test_df[men_total]\n",
    "                new_demo_cols.append('ratio_pauvrete')\n",
    "            \n",
    "            # Surface moyenne par ménage\n",
    "            if 'men_surf' in eco_cols:\n",
    "                train_df['surface_per_menage'] = train_df['men_surf'] / train_df[men_total]\n",
    "                test_df['surface_per_menage'] = test_df['men_surf'] / test_df[men_total]\n",
    "                new_demo_cols.append('surface_per_menage')\n",
    "    \n",
    "    # 2. Traitement des logements\n",
    "    if 'housing' in DEMOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des logements:\")\n",
    "        \n",
    "        # Périodes de construction\n",
    "        if 'construction_period' in DEMOGRAPHIC_VARS['housing']:\n",
    "            period_cols = DEMOGRAPHIC_VARS['housing']['construction_period']\n",
    "            total_log = train_df[period_cols].sum(axis=1)\n",
    "            \n",
    "            # Ratios par période\n",
    "            for col in period_cols:\n",
    "                ratio_name = f'ratio_{col}'\n",
    "                train_df[ratio_name] = train_df[col] / total_log\n",
    "                test_df[ratio_name] = test_df[col] / total_log\n",
    "                new_demo_cols.append(ratio_name)\n",
    "            \n",
    "            # Âge moyen du parc (pondéré)\n",
    "            # Supposons: avA1 = avant 1950, A1_A2 = 1950-1975, A2_A3 = 1975-2000, apA3 = après 2000\n",
    "            weights = {'log_avA1': 1940, 'log_A1_A2': 1962.5, 'log_A2_A3': 1987.5, 'log_apA3': 2010}\n",
    "            weighted_sum = sum(train_df[col] * weight for col, weight in weights.items())\n",
    "            train_df['age_moyen_logements'] = weighted_sum / total_log\n",
    "            test_df['age_moyen_logements'] = sum(test_df[col] * weight for col, weight in weights.items()) / total_log\n",
    "            new_demo_cols.append('age_moyen_logements')\n",
    "    \n",
    "    # 3. Traitement des individus\n",
    "    if 'individual' in DEMOGRAPHIC_VARS:\n",
    "        print(\"\\nTraitement des individus:\")\n",
    "        \n",
    "        # Total des individus\n",
    "        ind_total = DEMOGRAPHIC_VARS['individual']['total'][0]  # 'IND'\n",
    "        \n",
    "        # Traitement des groupes d'âge\n",
    "        if 'age_groups' in DEMOGRAPHIC_VARS['individual']:\n",
    "            age_cols = DEMOGRAPHIC_VARS['individual']['age_groups']\n",
    "            \n",
    "            # Ratios par groupe d'âge\n",
    "            for col in age_cols:\n",
    "                ratio_name = f'ratio_{col}'\n",
    "                train_df[ratio_name] = train_df[col] / train_df[ind_total]\n",
    "                test_df[ratio_name] = test_df[col] / test_df[ind_total]\n",
    "                new_demo_cols.append(ratio_name)\n",
    "            \n",
    "            # Indices démographiques\n",
    "            # Indice de jeunesse (0-Y4 / Y5-Y9)\n",
    "            young_cols = [col for col in age_cols if any(f'_{i}_' in col for i in range(5))]\n",
    "            old_cols = [col for col in age_cols if any(f'_{i}_' in col for i in range(5, 10))]\n",
    "            \n",
    "            train_df['indice_jeunesse'] = train_df[young_cols].sum(axis=1) / train_df[old_cols].sum(axis=1)\n",
    "            test_df['indice_jeunesse'] = test_df[young_cols].sum(axis=1) / test_df[old_cols].sum(axis=1)\n",
    "            new_demo_cols.append('indice_jeunesse')\n",
    "        \n",
    "        # Autres indicateurs\n",
    "        if 'other' in DEMOGRAPHIC_VARS['individual']:\n",
    "            other_cols = DEMOGRAPHIC_VARS['individual']['other']\n",
    "            \n",
    "            # Niveau de vie moyen\n",
    "            if 'ind_snv' in other_cols:\n",
    "                train_df['niveau_vie_moyen'] = train_df['ind_snv'] / train_df[ind_total]\n",
    "                test_df['niveau_vie_moyen'] = test_df['ind_snv'] / test_df[ind_total]\n",
    "                new_demo_cols.append('niveau_vie_moyen')\n",
    "    \n",
    "    # 4. Indicateurs composites\n",
    "    print(\"\\nCréation d'indicateurs composites:\")\n",
    "    \n",
    "    # Densité de population\n",
    "    if 'MEN' in train_df.columns and 'men_surf' in train_df.columns:\n",
    "        train_df['densite_population'] = train_df['IND'] / train_df['men_surf']\n",
    "        test_df['densite_population'] = test_df['IND'] / test_df['men_surf']\n",
    "        new_demo_cols.append('densite_population')\n",
    "    \n",
    "    # Taille moyenne des ménages\n",
    "    train_df['taille_moyenne_menage'] = train_df['IND'] / train_df['MEN']\n",
    "    test_df['taille_moyenne_menage'] = test_df['IND'] / test_df['MEN']\n",
    "    new_demo_cols.append('taille_moyenne_menage')\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    demo_cols_to_drop = sum([\n",
    "        sum([cols for cols in DEMOGRAPHIC_VARS['household'].values()], []),\n",
    "        sum([cols for cols in DEMOGRAPHIC_VARS['housing'].values()], []),\n",
    "        sum([cols for cols in DEMOGRAPHIC_VARS['individual'].values()], [])\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables démographiques ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_demo_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(demo_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives:\")\n",
    "    print(train_df[new_demo_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': demo_cols_to_drop,\n",
    "        'new_cols': new_demo_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "demographic_results = handle_demographic_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = demographic_results['train_df']\n",
    "test_df = demographic_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_activity_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables d'activité ===\")\n",
    "    \n",
    "    new_activity_cols = []\n",
    "    \n",
    "    # 1. Traitement des équipements\n",
    "    if 'equipment' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement des équipements:\")\n",
    "        equip_cols = ACTIVITY_VARS['equipment']\n",
    "        \n",
    "        # Nombre total d'équipements\n",
    "        train_df['n_equipements_total'] = (train_df[equip_cols] > 0).sum(axis=1)\n",
    "        test_df['n_equipements_total'] = (test_df[equip_cols] > 0).sum(axis=1)\n",
    "        new_activity_cols.append('n_equipements_total')\n",
    "        \n",
    "        # Score de complexité des équipements (pondéré par le numéro d'équipement)\n",
    "        equip_weights = {col: int(col.replace('EQUIPEMENT', '')) for col in equip_cols}\n",
    "        train_df['score_complexite_equip'] = sum(train_df[col] * weight \n",
    "                                               for col, weight in equip_weights.items())\n",
    "        test_df['score_complexite_equip'] = sum(test_df[col] * weight \n",
    "                                              for col, weight in equip_weights.items())\n",
    "        new_activity_cols.append('score_complexite_equip')\n",
    "        \n",
    "        # Encodage one-hot des équipements\n",
    "        for col in equip_cols:\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=f'equip_{col}')\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=f'equip_{col}')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 2. Traitement de l'activité et vocation\n",
    "    if 'activity' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement de l'activité et vocation:\")\n",
    "        activity_cols = ACTIVITY_VARS['activity']\n",
    "        \n",
    "        # Encodage des variables catégorielles\n",
    "        for col in activity_cols:\n",
    "            # Encodage avec gestion de la fréquence\n",
    "            encoding_map = (train_df[col].value_counts() / len(train_df)).to_dict()\n",
    "            \n",
    "            # Appliquer l'encodage\n",
    "            train_df[f'{col}_freq'] = train_df[col].map(encoding_map)\n",
    "            test_df[f'{col}_freq'] = test_df[col].map(encoding_map)\n",
    "            new_activity_cols.append(f'{col}_freq')\n",
    "            \n",
    "            # Encodage one-hot classique\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=col)\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=col)\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "        \n",
    "        # Créer des combinaisons d'activité-vocation si disponibles\n",
    "        if 'ACTIVIT2' in activity_cols and 'VOCATION' in activity_cols:\n",
    "            train_df['activ_voc'] = train_df['ACTIVIT2'] + '_' + train_df['VOCATION']\n",
    "            test_df['activ_voc'] = test_df['ACTIVIT2'] + '_' + test_df['VOCATION']\n",
    "            \n",
    "            # Encoder cette nouvelle combinaison\n",
    "            dummies = pd.get_dummies(train_df['activ_voc'], prefix='activ_voc')\n",
    "            test_dummies = pd.get_dummies(test_df['activ_voc'], prefix='activ_voc')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 3. Traitement de la taille et catégorie\n",
    "    if 'size_category' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement de la taille et catégorie:\")\n",
    "        size_cols = [col for col in ACTIVITY_VARS['size_category'] if col.startswith('TAILLE')]\n",
    "        \n",
    "        # Score de taille global\n",
    "        if size_cols:\n",
    "            train_df['score_taille'] = train_df[size_cols].mean(axis=1)\n",
    "            test_df['score_taille'] = test_df[size_cols].mean(axis=1)\n",
    "            new_activity_cols.append('score_taille')\n",
    "            \n",
    "            # Variabilité de la taille\n",
    "            train_df['var_taille'] = train_df[size_cols].std(axis=1)\n",
    "            test_df['var_taille'] = test_df[size_cols].std(axis=1)\n",
    "            new_activity_cols.append('var_taille')\n",
    "        \n",
    "        # Traitement de COEFASS\n",
    "        if 'COEFASS' in ACTIVITY_VARS['size_category']:\n",
    "            # Encodage fréquentiel\n",
    "            coef_map = (train_df['COEFASS'].value_counts() / len(train_df)).to_dict()\n",
    "            train_df['coefass_freq'] = train_df['COEFASS'].map(coef_map)\n",
    "            test_df['coefass_freq'] = test_df['COEFASS'].map(coef_map)\n",
    "            new_activity_cols.append('coefass_freq')\n",
    "            \n",
    "            # Encodage one-hot\n",
    "            dummies = pd.get_dummies(train_df['COEFASS'], prefix='coefass')\n",
    "            test_dummies = pd.get_dummies(test_df['COEFASS'], prefix='coefass')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 4. Traitement des services d'urgence\n",
    "    if 'emergency' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement des services d'urgence:\")\n",
    "        emergency_cols = ACTIVITY_VARS['emergency']\n",
    "        # Garder ces variables telles quelles\n",
    "        new_activity_cols.extend(emergency_cols)\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    activity_cols_to_drop = sum([\n",
    "        ACTIVITY_VARS['equipment'],\n",
    "        ACTIVITY_VARS['activity'],\n",
    "        ACTIVITY_VARS['size_category']\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables d'activité ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_activity_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(activity_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_activity_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': activity_cols_to_drop,\n",
    "        'new_cols': new_activity_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "activity_results = handle_activity_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = activity_results['train_df']\n",
    "test_df = activity_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_target_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables cibles ===\")\n",
    "    \n",
    "    new_target_cols = []\n",
    "    target_stats = {}\n",
    "    \n",
    "    # Récupérer les variables cibles\n",
    "    freq_var = TARGET_VARS['primary']['frequency'][0]  # 'FREQ'\n",
    "    cost_var = TARGET_VARS['primary']['cost'][0]       # 'CM'\n",
    "    charge_var = TARGET_VARS['primary']['total'][0]    # 'CHARGE'\n",
    "    \n",
    "    # 1. Vérification de la cohérence des calculs\n",
    "    print(\"\\nVérification de la cohérence des calculs:\")\n",
    "    \n",
    "    if all(var in train_df.columns for var in [freq_var, cost_var, charge_var, 'ANNEE_ASSURANCE']):\n",
    "        # Calculer CHARGE théorique\n",
    "        train_df['CHARGE_calc'] = (train_df[freq_var] * \n",
    "                                  train_df[cost_var] * \n",
    "                                  train_df['ANNEE_ASSURANCE'])\n",
    "        \n",
    "        # Calculer la différence relative\n",
    "        train_df['charge_diff_pct'] = abs(train_df['CHARGE_calc'] - \n",
    "                                        train_df[charge_var]) / train_df[charge_var] * 100\n",
    "        \n",
    "        # Statistiques sur les différences\n",
    "        diff_stats = train_df['charge_diff_pct'].describe()\n",
    "        print(\"\\nDifférences relatives dans le calcul de CHARGE:\")\n",
    "        print(diff_stats)\n",
    "        \n",
    "        # Marquer les incohérences importantes\n",
    "        threshold = diff_stats['75%'] + 1.5 * (diff_stats['75%'] - diff_stats['25%'])  # IQR method\n",
    "        train_df['charge_incoherent'] = train_df['charge_diff_pct'] > threshold\n",
    "        new_target_cols.extend(['charge_diff_pct', 'charge_incoherent'])\n",
    "        \n",
    "        target_stats['charge_coherence'] = {\n",
    "            'mean_diff_pct': diff_stats['mean'],\n",
    "            'median_diff_pct': diff_stats['50%'],\n",
    "            'n_incoherent': train_df['charge_incoherent'].sum()\n",
    "        }\n",
    "    \n",
    "    # 2. Traitement des valeurs extrêmes\n",
    "    print(\"\\nTraitement des valeurs extrêmes:\")\n",
    "    \n",
    "    for var, var_type in zip([freq_var, cost_var, charge_var], ['freq', 'cost', 'charge']):\n",
    "        if var in train_df.columns:\n",
    "            # Calculer les quantiles\n",
    "            q1 = train_df[var].quantile(0.25)\n",
    "            q3 = train_df[var].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Identifier les outliers\n",
    "            outlier_col = f'{var_type}_outlier'\n",
    "            train_df[outlier_col] = ((train_df[var] < lower_bound) | \n",
    "                                   (train_df[var] > upper_bound))\n",
    "            new_target_cols.append(outlier_col)\n",
    "            \n",
    "            # Créer version winsorisée\n",
    "            wins_col = f'{var_type}_winsorized'\n",
    "            train_df[wins_col] = train_df[var].clip(lower_bound, upper_bound)\n",
    "            new_target_cols.append(wins_col)\n",
    "            \n",
    "            # Statistiques\n",
    "            target_stats[f'{var_type}_outliers'] = {\n",
    "                'n_outliers': train_df[outlier_col].sum(),\n",
    "                'pct_outliers': train_df[outlier_col].mean() * 100,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            }\n",
    "    \n",
    "    # 3. Création d'indicateurs de risque\n",
    "    print(\"\\nCréation d'indicateurs de risque:\")\n",
    "    \n",
    "    # Ratio coût/fréquence\n",
    "    if freq_var in train_df.columns and cost_var in train_df.columns:\n",
    "        train_df['cost_freq_ratio'] = train_df[cost_var] / train_df[freq_var].replace(0, np.nan)\n",
    "        new_target_cols.append('cost_freq_ratio')\n",
    "    \n",
    "    # Charge par année\n",
    "    if charge_var in train_df.columns and 'ANNEE_ASSURANCE' in train_df.columns:\n",
    "        train_df['charge_per_year'] = train_df[charge_var] / train_df['ANNEE_ASSURANCE']\n",
    "        new_target_cols.append('charge_per_year')\n",
    "    \n",
    "    # 4. Gestion des valeurs manquantes\n",
    "    print(\"\\nAnalyse des valeurs manquantes:\")\n",
    "    \n",
    "    for var, var_type in zip([freq_var, cost_var, charge_var], ['freq', 'cost', 'charge']):\n",
    "        if var in train_df.columns:\n",
    "            missing_pct = train_df[var].isnull().mean() * 100\n",
    "            target_stats[f'{var_type}_missing'] = {\n",
    "                'n_missing': train_df[var].isnull().sum(),\n",
    "                'pct_missing': missing_pct\n",
    "            }\n",
    "            print(f\"{var}: {missing_pct:.2f}% de valeurs manquantes\")\n",
    "    \n",
    "    # 5. Création de bins pour les variables continues\n",
    "    print(\"\\nCréation de bins:\")\n",
    "    \n",
    "    for var, var_type in zip([freq_var, cost_var, charge_var], ['freq', 'cost', 'charge']):\n",
    "        if var in train_df.columns:\n",
    "            # Créer des bins (10 catégories)\n",
    "            bins_col = f'{var_type}_bins'\n",
    "            train_df[bins_col] = pd.qcut(train_df[var], q=10, labels=False, duplicates='drop')\n",
    "            new_target_cols.append(bins_col)\n",
    "            \n",
    "            # Encoder en one-hot si nécessaire\n",
    "            dummies = pd.get_dummies(train_df[bins_col], prefix=f'{var_type}_bin')\n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            new_target_cols.extend(dummies.columns)\n",
    "    \n",
    "    # Statistiques finales\n",
    "    print(\"\\n=== Statistiques des nouvelles variables cibles ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_target_cols)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_target_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'new_cols': new_target_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df,\n",
    "        'stats': target_stats\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "target_results = handle_target_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = target_results['train_df']\n",
    "test_df = target_results['test_df']\n",
    "\n",
    "# Afficher les statistiques détaillées\n",
    "print(\"\\n=== Résumé des statistiques ===\")\n",
    "for category, stats in target_results['stats'].items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_id_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables d'identification ===\")\n",
    "    \n",
    "    id_stats = {}\n",
    "    id_var = ID_VARS['identifiers'][0]  # 'ID'\n",
    "    properties = ID_VARS['properties']\n",
    "    \n",
    "    # 1. Vérification de base des IDs\n",
    "    print(\"\\nVérification de base des IDs:\")\n",
    "    \n",
    "    # Vérifier la présence de l'ID\n",
    "    if id_var not in train_df.columns or id_var not in test_df.columns:\n",
    "        raise ValueError(f\"Variable {id_var} non trouvée dans les données\")\n",
    "    \n",
    "    # Statistiques initiales\n",
    "    id_stats['initial'] = {\n",
    "        'train_rows': len(train_df),\n",
    "        'test_rows': len(test_df),\n",
    "        'train_unique_ids': train_df[id_var].nunique(),\n",
    "        'test_unique_ids': test_df[id_var].nunique()\n",
    "    }\n",
    "    \n",
    "    # 2. Vérification de l'unicité\n",
    "    print(\"\\nVérification de l'unicité:\")\n",
    "    \n",
    "    if properties['unique']:\n",
    "        # Train\n",
    "        train_duplicates = train_df[train_df[id_var].duplicated(keep=False)]\n",
    "        n_train_dupes = len(train_duplicates)\n",
    "        \n",
    "        # Test\n",
    "        test_duplicates = test_df[test_df[id_var].duplicated(keep=False)]\n",
    "        n_test_dupes = len(test_duplicates)\n",
    "        \n",
    "        id_stats['duplicates'] = {\n",
    "            'train_duplicates': n_train_dupes,\n",
    "            'test_duplicates': n_test_dupes\n",
    "        }\n",
    "        \n",
    "        if n_train_dupes > 0 or n_test_dupes > 0:\n",
    "            print(f\"ATTENTION! Doublons trouvés:\")\n",
    "            print(f\"  Train: {n_train_dupes} lignes\")\n",
    "            print(f\"  Test: {n_test_dupes} lignes\")\n",
    "            \n",
    "            # Créer un indicateur de doublon\n",
    "            train_df['is_duplicate'] = train_df[id_var].duplicated(keep=False)\n",
    "            test_df['is_duplicate'] = test_df[id_var].duplicated(keep=False)\n",
    "    \n",
    "    # 3. Vérification des valeurs nulles\n",
    "    print(\"\\nVérification des valeurs nulles:\")\n",
    "    \n",
    "    if properties['not_null']:\n",
    "        # Train\n",
    "        train_nulls = train_df[id_var].isnull().sum()\n",
    "        test_nulls = test_df[id_var].isnull().sum()\n",
    "        \n",
    "        id_stats['null_values'] = {\n",
    "            'train_nulls': train_nulls,\n",
    "            'test_nulls': test_nulls\n",
    "        }\n",
    "        \n",
    "        if train_nulls > 0 or test_nulls > 0:\n",
    "            print(f\"ATTENTION! Valeurs nulles trouvées:\")\n",
    "            print(f\"  Train: {train_nulls} lignes\")\n",
    "            print(f\"  Test: {test_nulls} lignes\")\n",
    "    \n",
    "    # 4. Vérification du type de données\n",
    "    print(\"\\nVérification du type de données:\")\n",
    "    \n",
    "    expected_type = properties['type']\n",
    "    train_type = train_df[id_var].dtype\n",
    "    test_type = test_df[id_var].dtype\n",
    "    \n",
    "    id_stats['data_types'] = {\n",
    "        'expected': expected_type,\n",
    "        'train_actual': str(train_type),\n",
    "        'test_actual': str(test_type)\n",
    "    }\n",
    "    \n",
    "    if expected_type == 'str':\n",
    "        if not pd.api.types.is_string_dtype(train_type):\n",
    "            print(f\"ATTENTION! Type incorrect dans train: {train_type}\")\n",
    "            train_df[id_var] = train_df[id_var].astype(str)\n",
    "        \n",
    "        if not pd.api.types.is_string_dtype(test_type):\n",
    "            print(f\"ATTENTION! Type incorrect dans test: {test_type}\")\n",
    "            test_df[id_var] = test_df[id_var].astype(str)\n",
    "    \n",
    "    # 5. Analyse du format des IDs\n",
    "    print(\"\\nAnalyse du format des IDs:\")\n",
    "    \n",
    "    # Longueur des IDs\n",
    "    train_lengths = train_df[id_var].astype(str).str.len()\n",
    "    test_lengths = test_df[id_var].astype(str).str.len()\n",
    "    \n",
    "    id_stats['length_stats'] = {\n",
    "        'train_min_length': train_lengths.min(),\n",
    "        'train_max_length': train_lengths.max(),\n",
    "        'test_min_length': test_lengths.min(),\n",
    "        'test_max_length': test_lengths.max()\n",
    "    }\n",
    "    \n",
    "    if train_lengths.min() != train_lengths.max():\n",
    "        print(f\"ATTENTION! Longueurs variables dans train: {train_lengths.min()}-{train_lengths.max()}\")\n",
    "    \n",
    "    if test_lengths.min() != test_lengths.max():\n",
    "        print(f\"ATTENTION! Longueurs variables dans test: {test_lengths.min()}-{test_lengths.max()}\")\n",
    "    \n",
    "    # 6. Vérification de la cohérence train/test\n",
    "    print(\"\\nVérification de la cohérence train/test:\")\n",
    "    \n",
    "    common_ids = set(train_df[id_var]).intersection(set(test_df[id_var]))\n",
    "    id_stats['train_test_overlap'] = {\n",
    "        'n_common_ids': len(common_ids),\n",
    "        'pct_train': len(common_ids) / len(train_df) * 100,\n",
    "        'pct_test': len(common_ids) / len(test_df) * 100\n",
    "    }\n",
    "    \n",
    "    if common_ids:\n",
    "        print(f\"ATTENTION! {len(common_ids)} IDs communs entre train et test\")\n",
    "        \n",
    "        # Marquer les IDs communs\n",
    "        train_df['in_test'] = train_df[id_var].isin(test_df[id_var])\n",
    "        test_df['in_train'] = test_df[id_var].isin(train_df[id_var])\n",
    "    \n",
    "    # 7. Création de hash (si nécessaire)\n",
    "    if properties.get('create_hash', False):\n",
    "        print(\"\\nCréation de hash des IDs:\")\n",
    "        \n",
    "        train_df['id_hash'] = train_df[id_var].apply(lambda x: hash(str(x)))\n",
    "        test_df['id_hash'] = test_df[id_var].apply(lambda x: hash(str(x)))\n",
    "    \n",
    "    # Résumé final\n",
    "    print(\"\\n=== Résumé des vérifications ===\")\n",
    "    for category, stats in id_stats.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return {\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df,\n",
    "        'stats': id_stats,\n",
    "        'has_issues': (n_train_dupes > 0 or n_test_dupes > 0 or \n",
    "                      train_nulls > 0 or test_nulls > 0 or \n",
    "                      len(common_ids) > 0)\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "id_results = handle_id_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = id_results['train_df']\n",
    "test_df = id_results['test_df']\n",
    "\n",
    "# Si des problèmes ont été détectés\n",
    "if id_results['has_issues']:\n",
    "    print(\"\\nATTENTION! Des problèmes ont été détectés avec les IDs.\")\n",
    "    print(\"Veuillez vérifier les détails ci-dessus et prendre les mesures appropriées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_consolidate_data(train_df, test_df):\n",
    "    print(\"=== Nettoyage et consolidation des données ===\")\n",
    "    \n",
    "    # 1. Traiter chaque groupe de variables\n",
    "    print(\"\\n1. Traitement des variables par groupe:\")\n",
    "    \n",
    "    # Weather\n",
    "    print(\"\\nTraitement des variables météorologiques...\")\n",
    "    weather_results = handle_weather_vars(train_df, test_df)\n",
    "    train_df = weather_results['train_df']\n",
    "    test_df = weather_results['test_df']\n",
    "    \n",
    "    # Building\n",
    "    print(\"\\nTraitement des variables bâtiment...\")\n",
    "    building_results = handle_building_vars(train_df, test_df)\n",
    "    train_df = building_results['train_df']\n",
    "    test_df = building_results['test_df']\n",
    "    \n",
    "    # Geographic\n",
    "    print(\"\\nTraitement des variables géographiques...\")\n",
    "    geographic_results = handle_geographic_vars(train_df, test_df)\n",
    "    train_df = geographic_results['train_df']\n",
    "    test_df = geographic_results['test_df']\n",
    "    \n",
    "    # Insurance\n",
    "    print(\"\\nTraitement des variables d'assurance...\")\n",
    "    insurance_results = handle_insurance_vars(train_df, test_df)\n",
    "    train_df = insurance_results['train_df']\n",
    "    test_df = insurance_results['test_df']\n",
    "    \n",
    "    # Demographic\n",
    "    print(\"\\nTraitement des variables démographiques...\")\n",
    "    demographic_results = handle_demographic_vars(train_df, test_df)\n",
    "    train_df = demographic_results['train_df']\n",
    "    test_df = demographic_results['test_df']\n",
    "    \n",
    "    # Activity\n",
    "    print(\"\\nTraitement des variables d'activité...\")\n",
    "    activity_results = handle_activity_vars(train_df, test_df)\n",
    "    train_df = activity_results['train_df']\n",
    "    test_df = activity_results['test_df']\n",
    "    \n",
    "    # Target (seulement pour train_df)\n",
    "    print(\"\\nTraitement des variables cibles...\")\n",
    "    target_results = handle_target_vars(train_df, test_df)\n",
    "    train_df = target_results['train_df']\n",
    "    test_df = target_results['test_df']\n",
    "    \n",
    "    # ID\n",
    "    print(\"\\nTraitement des variables d'identification...\")\n",
    "    id_results = handle_id_vars(train_df, test_df)\n",
    "    train_df = id_results['train_df']\n",
    "    test_df = id_results['test_df']\n",
    "    \n",
    "    # 2. Consolider les colonnes à supprimer\n",
    "    cols_to_drop = []\n",
    "    cols_to_drop.extend(weather_results.get('to_drop', []))\n",
    "    cols_to_drop.extend(building_results.get('to_drop', []))\n",
    "    cols_to_drop.extend(geographic_results.get('to_drop', []))\n",
    "    cols_to_drop.extend(insurance_results.get('to_drop', []))\n",
    "    cols_to_drop.extend(demographic_results.get('to_drop', []))\n",
    "    cols_to_drop.extend(activity_results.get('to_drop', []))\n",
    "    \n",
    "    # 3. Consolider les nouvelles colonnes\n",
    "    new_cols = []\n",
    "    new_cols.extend(weather_results.get('new_cols', []))\n",
    "    new_cols.extend(building_results.get('new_cols', []))\n",
    "    new_cols.extend(geographic_results.get('new_cols', []))\n",
    "    new_cols.extend(insurance_results.get('new_cols', []))\n",
    "    new_cols.extend(demographic_results.get('new_cols', []))\n",
    "    new_cols.extend(activity_results.get('new_cols', []))\n",
    "    new_cols.extend(target_results.get('new_cols', []))\n",
    "    \n",
    "    # 4. Vérifier la qualité des données consolidées\n",
    "    print(\"\\n=== Vérification finale des données ===\")\n",
    "    \n",
    "    # Statistiques sur les nouvelles variables\n",
    "    print(\"\\nStatistiques des nouvelles variables numériques:\")\n",
    "    numeric_cols = [col for col in new_cols if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    # Valeurs manquantes\n",
    "    missing_pct = (train_df[new_cols].isnull().mean() * 100).round(2)\n",
    "    print(\"\\nPourcentage de valeurs manquantes dans les nouvelles variables:\")\n",
    "    print(missing_pct[missing_pct > 0])\n",
    "    \n",
    "    # Dimensions finales\n",
    "    print(\"\\nDimensions finales:\")\n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "    \n",
    "    # 5. Supprimer les anciennes colonnes\n",
    "    train_df_clean = train_df.drop(columns=cols_to_drop)\n",
    "    test_df_clean = test_df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return {\n",
    "        'train_df': train_df_clean,\n",
    "        'test_df': test_df_clean,\n",
    "        'new_cols': new_cols,\n",
    "        'dropped_cols': cols_to_drop,\n",
    "        'stats': {\n",
    "            'n_new_cols': len(new_cols),\n",
    "            'n_dropped_cols': len(cols_to_drop),\n",
    "            'train_shape': train_df_clean.shape,\n",
    "            'test_shape': test_df_clean.shape,\n",
    "            'missing_stats': missing_pct[missing_pct > 0].to_dict()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter le nettoyage\n",
    "results = clean_and_consolidate_data(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = results['train_df']\n",
    "test_df = results['test_df']\n",
    "\n",
    "# Afficher le résumé\n",
    "print(\"\\n=== Résumé des modifications ===\")\n",
    "print(f\"Nombre de nouvelles colonnes créées: {results['stats']['n_new_cols']}\")\n",
    "print(f\"Nombre de colonnes supprimées: {results['stats']['n_dropped_cols']}\")\n",
    "print(\"\\nDimensions finales:\")\n",
    "print(f\"Train: {results['stats']['train_shape']}\")\n",
    "print(f\"Test: {results['stats']['test_shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process_weather_data(X_train, X_test, missing_summary, weather_cols):\n",
    "    \"\"\"\n",
    "    Pipeline complet pour le traitement des variables météo:\n",
    "    1. Nettoyage et imputation des variables individuelles\n",
    "    2. Création de variables agrégées \n",
    "    \"\"\"\n",
    "    # Étape 1: Nettoyage initial et imputation\n",
    "    X_train_clean, X_test_clean, missing_indicators = clean_weather_vars(\n",
    "        X_train, X_test, missing_summary, weather_cols\n",
    "    )\n",
    "    \n",
    "    # Étape 2: Création de variables agrégées\n",
    "    weather_results = aggregate_weather_vars(X_train_clean, X_test_clean)\n",
    "    \n",
    "    # Collecter toutes les nouvelles variables créées\n",
    "    new_cols = weather_results['new_cols']\n",
    "    cols_to_drop = weather_results['to_drop']\n",
    "    \n",
    "    # Récupérer les DataFrames finaux\n",
    "    X_train_final = weather_results['train_df'] if 'train_df' in weather_results else X_train_clean\n",
    "    X_test_final = weather_results['test_df'] if 'test_df' in weather_results else X_test_clean\n",
    "    \n",
    "    # Ajouter les indicateurs de valeurs manquantes\n",
    "    for col, indicator in missing_indicators.items():\n",
    "        X_train_final[col] = indicator\n",
    "        X_test_final[col] = indicator  # Appliquer aussi au jeu de test\n",
    "    \n",
    "    return {\n",
    "        'train_df': X_train_final,\n",
    "        'test_df': X_test_final,\n",
    "        'new_cols': new_cols,\n",
    "        'to_drop': cols_to_drop,\n",
    "        'missing_indicators': missing_indicators\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def process_building_data(X_train, X_test, missing_summary, building_cols):\n",
    "    \"\"\"\n",
    "    Pipeline complet pour le traitement des variables bâtiment:\n",
    "    1. Nettoyage et imputation des variables individuelles\n",
    "    2. Création de variables agrégées et dérivées\n",
    "    \"\"\"\n",
    "    # Étape 1: Nettoyage initial et imputation\n",
    "    X_train_clean, X_test_clean, missing_indicators = clean_building_vars(\n",
    "        X_train, X_test, missing_summary, building_cols\n",
    "    )\n",
    "    \n",
    "    # Étape 2: Création de variables agrégées\n",
    "    building_results = aggregate_building_vars(X_train_clean, X_test_clean)\n",
    "    \n",
    "    # Collecter toutes les nouvelles variables créées\n",
    "    new_cols = building_results['new_cols']\n",
    "    cols_to_drop = building_results['to_drop']\n",
    "    \n",
    "    # Récupérer les DataFrames finaux\n",
    "    X_train_final = building_results['train_df']\n",
    "    X_test_final = building_results['test_df']\n",
    "    \n",
    "    # Ajouter les indicateurs de valeurs manquantes\n",
    "    for col, indicator in missing_indicators.items():\n",
    "        X_train_final[col] = indicator\n",
    "        X_test_final[col] = indicator  # Appliquer aussi au jeu de test\n",
    "    \n",
    "    return {\n",
    "        'train_df': X_train_final,\n",
    "        'test_df': X_test_final,\n",
    "        'new_cols': new_cols,\n",
    "        'to_drop': cols_to_drop,\n",
    "        'missing_indicators': missing_indicators\n",
    "    }\n",
    "\n",
    "def clean_building_vars(X_train, X_test, missing_summary, building_cols):\n",
    "    \"\"\"\n",
    "    Traite les variables bâtiment avec vérification détaillée des types\n",
    "    et imputation des valeurs manquantes\n",
    "    \"\"\"\n",
    "    missing_indicators = {}\n",
    "    print(\"\\nNettoyage des variables bâtiment:\")\n",
    "    \n",
    "    for col in building_cols:\n",
    "        if col in X_train.columns:\n",
    "            missing_pct = missing_summary.loc[col, 'Missing Percentage']\n",
    "            print(f\"\\nColonne {col}:\")\n",
    "            print(f\"- Pourcentage de valeurs manquantes: {missing_pct:.2f}%\")\n",
    "            print(f\"- Type initial: {X_train[col].dtype}\")\n",
    "            \n",
    "            # Créer indicateur de valeurs manquantes\n",
    "            if missing_pct > 0:\n",
    "                missing_indicators[f'{col}_is_missing'] = X_train[col].isnull().astype(int)\n",
    "            \n",
    "            if missing_pct <= 70:  # On traite uniquement si moins de 70% manquant\n",
    "                if X_train[col].dtype in ['int64', 'float64']:\n",
    "                    # Pour les variables numériques\n",
    "                    median_val = X_train[col].median()\n",
    "                    X_train[col].fillna(median_val, inplace=True)\n",
    "                    X_test[col].fillna(median_val, inplace=True)\n",
    "                    print(f\"- Variable numérique, imputation avec la médiane: {median_val}\")\n",
    "                    print(f\"- Plage de valeurs: [{X_train[col].min()}, {X_train[col].max()}]\")\n",
    "                else:\n",
    "                    # Pour les variables catégorielles\n",
    "                    # Filtrer les valeurs non-nulles avant de les afficher\n",
    "                    unique_vals = X_train[col].dropna().unique()\n",
    "                    print(\"- Variable catégorielle, valeurs uniques:\")\n",
    "                    print(unique_vals[:5])\n",
    "                    \n",
    "                    # Si peu de valeurs uniques, on pourrait considérer une conversion en numérique\n",
    "                    if len(unique_vals) < 10:\n",
    "                        print(f\"- Nombre de catégories: {len(unique_vals)}\")\n",
    "                        print(\"- Distribution des valeurs:\")\n",
    "                        print(X_train[col].value_counts().head())\n",
    "                    \n",
    "                    X_train[col].fillna('UNKNOWN', inplace=True)\n",
    "                    X_test[col].fillna('UNKNOWN', inplace=True)\n",
    "                    print(f\"- Imputation avec 'UNKNOWN'\")\n",
    "            else:\n",
    "                print(\"- Colonne ignorée (>70% manquant)\")\n",
    "            \n",
    "            # Vérification après traitement\n",
    "            missing_after = X_train[col].isnull().sum()\n",
    "            if missing_after > 0:\n",
    "                print(f\"⚠️ ATTENTION: {missing_after} valeurs manquantes restantes!\")\n",
    "    \n",
    "    return X_train, X_test, missing_indicators\n",
    "\n",
    "def aggregate_building_vars(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Crée des variables agrégées et des indicateurs composites à partir\n",
    "    des variables bâtiment nettoyées\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Création de variables bâtiment agrégées ===\")\n",
    "    \n",
    "    new_building_cols = []\n",
    "    \n",
    "    # 1. Consolider les hauteurs\n",
    "    height_cols = BUILDING_VARS['height']\n",
    "    if height_cols:\n",
    "        print(\"\\nConsolidation des hauteurs:\")\n",
    "        height_aggs = {\n",
    "            'height_mean': 'mean',\n",
    "            'height_max': 'max',\n",
    "            'height_min': 'min'\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in height_aggs.items():\n",
    "            train_df[col_name] = train_df[height_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[height_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(height_aggs)} variables de hauteur agrégées\")\n",
    "    \n",
    "    # 2. Consolider les surfaces\n",
    "    surface_cols = BUILDING_VARS['surface']\n",
    "    if surface_cols:\n",
    "        print(\"\\nConsolidation des surfaces:\")\n",
    "        surface_aggs = {\n",
    "            'surface_total': 'sum',\n",
    "            'surface_mean': 'mean',\n",
    "            'surface_max': 'max',\n",
    "            'n_surfaces': lambda x: x.notna().sum()  # Nombre de surfaces renseignées\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in surface_aggs.items():\n",
    "            train_df[col_name] = train_df[surface_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[surface_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(surface_aggs)} variables de surface agrégées\")\n",
    "    \n",
    "    # 3. Consolider le nombre de bâtiments\n",
    "    building_cols = BUILDING_VARS['buildings']\n",
    "    if building_cols:\n",
    "        print(\"\\nConsolidation du nombre de bâtiments:\")\n",
    "        building_aggs = {\n",
    "            'buildings_total': 'sum',\n",
    "            'buildings_count': lambda x: x.notna().sum(),  # Nombre de types de bâtiments\n",
    "            'buildings_types': lambda x: (x > 0).sum()     # Nombre de types avec bâtiments\n",
    "        }\n",
    "        \n",
    "        for col_name, agg_func in building_aggs.items():\n",
    "            train_df[col_name] = train_df[building_cols].agg(agg_func, axis=1)\n",
    "            test_df[col_name] = test_df[building_cols].agg(agg_func, axis=1)\n",
    "            new_building_cols.append(col_name)\n",
    "        \n",
    "        print(f\"Créé {len(building_aggs)} variables de comptage de bâtiments\")\n",
    "    \n",
    "    # 4. Créer des ratios et indicateurs composites\n",
    "    print(\"\\nCréation d'indicateurs composites:\")\n",
    "    \n",
    "    # Ratio surface/bâtiment\n",
    "    if 'surface_total' in new_building_cols and 'buildings_total' in new_building_cols:\n",
    "        train_df['surface_per_building'] = (train_df['surface_total'] / \n",
    "                                          train_df['buildings_total'].replace(0, np.nan))\n",
    "        test_df['surface_per_building'] = (test_df['surface_total'] / \n",
    "                                         test_df['buildings_total'].replace(0, np.nan))\n",
    "        new_building_cols.append('surface_per_building')\n",
    "    \n",
    "    # Ratio hauteur/surface\n",
    "    if 'height_mean' in new_building_cols and 'surface_total' in new_building_cols:\n",
    "        train_df['height_surface_ratio'] = (train_df['height_mean'] / \n",
    "                                          train_df['surface_total'].replace(0, np.nan))\n",
    "        test_df['height_surface_ratio'] = (test_df['height_mean'] / \n",
    "                                         test_df['surface_total'].replace(0, np.nan))\n",
    "        new_building_cols.append('height_surface_ratio')\n",
    "    \n",
    "    # 5. Encoder les caractéristiques catégorielles\n",
    "    char_cols = BUILDING_VARS['characteristics']\n",
    "    if char_cols:\n",
    "        print(\"\\nEncodage des caractéristiques:\")\n",
    "        for col in char_cols:\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=f'char_{col}')\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=f'char_{col}')\n",
    "            \n",
    "            # Assurer les mêmes colonnes dans train et test\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_building_cols.extend(dummies.columns)\n",
    "        \n",
    "        print(f\"Créé {len(dummies.columns)} variables dummy pour les caractéristiques\")\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    building_cols_to_drop = sum([\n",
    "        BUILDING_VARS['height'],\n",
    "        BUILDING_VARS['surface'],\n",
    "        BUILDING_VARS['buildings'],\n",
    "        BUILDING_VARS['characteristics']\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables bâtiment ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_building_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(building_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_building_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': building_cols_to_drop,\n",
    "        'new_cols': new_building_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Utilisation de la fonction fusionnée\n",
    "building_data = process_building_data(\n",
    "    X_train, X_test, missing_summary, groups['Building']\n",
    ")\n",
    "\n",
    "# Mise à jour des DataFrames\n",
    "X_train = building_data['train_df']\n",
    "X_test = building_data['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_activity_vars(train_df, test_df):\n",
    "    print(\"=== Traitement des variables d'activité ===\")\n",
    "    \n",
    "    new_activity_cols = []\n",
    "    \n",
    "    # 1. Traitement des équipements\n",
    "    if 'equipment' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement des équipements:\")\n",
    "        equip_cols = ACTIVITY_VARS['equipment']\n",
    "        \n",
    "        # Nombre total d'équipements\n",
    "        train_df['n_equipements_total'] = (train_df[equip_cols] > 0).sum(axis=1)\n",
    "        test_df['n_equipements_total'] = (test_df[equip_cols] > 0).sum(axis=1)\n",
    "        new_activity_cols.append('n_equipements_total')\n",
    "        \n",
    "        # Score de complexité des équipements (pondéré par le numéro d'équipement)\n",
    "        equip_weights = {col: int(col.replace('EQUIPEMENT', '')) for col in equip_cols}\n",
    "        train_df['score_complexite_equip'] = sum(train_df[col] * weight \n",
    "                                               for col, weight in equip_weights.items())\n",
    "        test_df['score_complexite_equip'] = sum(test_df[col] * weight \n",
    "                                              for col, weight in equip_weights.items())\n",
    "        new_activity_cols.append('score_complexite_equip')\n",
    "        \n",
    "        # Encodage one-hot des équipements\n",
    "        for col in equip_cols:\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=f'equip_{col}')\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=f'equip_{col}')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 2. Traitement de l'activité et vocation\n",
    "    if 'activity' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement de l'activité et vocation:\")\n",
    "        activity_cols = ACTIVITY_VARS['activity']\n",
    "        \n",
    "        # Encodage des variables catégorielles\n",
    "        for col in activity_cols:\n",
    "            # Encodage avec gestion de la fréquence\n",
    "            encoding_map = (train_df[col].value_counts() / len(train_df)).to_dict()\n",
    "            \n",
    "            # Appliquer l'encodage\n",
    "            train_df[f'{col}_freq'] = train_df[col].map(encoding_map)\n",
    "            test_df[f'{col}_freq'] = test_df[col].map(encoding_map)\n",
    "            new_activity_cols.append(f'{col}_freq')\n",
    "            \n",
    "            # Encodage one-hot classique\n",
    "            dummies = pd.get_dummies(train_df[col], prefix=col)\n",
    "            test_dummies = pd.get_dummies(test_df[col], prefix=col)\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "        \n",
    "        # Créer des combinaisons d'activité-vocation si disponibles\n",
    "        if 'ACTIVIT2' in activity_cols and 'VOCATION' in activity_cols:\n",
    "            train_df['activ_voc'] = train_df['ACTIVIT2'] + '_' + train_df['VOCATION']\n",
    "            test_df['activ_voc'] = test_df['ACTIVIT2'] + '_' + test_df['VOCATION']\n",
    "            \n",
    "            # Encoder cette nouvelle combinaison\n",
    "            dummies = pd.get_dummies(train_df['activ_voc'], prefix='activ_voc')\n",
    "            test_dummies = pd.get_dummies(test_df['activ_voc'], prefix='activ_voc')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 3. Traitement de la taille et catégorie\n",
    "    if 'size_category' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement de la taille et catégorie:\")\n",
    "        size_cols = [col for col in ACTIVITY_VARS['size_category'] if col.startswith('TAILLE')]\n",
    "        \n",
    "        # Score de taille global\n",
    "        if size_cols:\n",
    "            train_df['score_taille'] = train_df[size_cols].mean(axis=1)\n",
    "            test_df['score_taille'] = test_df[size_cols].mean(axis=1)\n",
    "            new_activity_cols.append('score_taille')\n",
    "            \n",
    "            # Variabilité de la taille\n",
    "            train_df['var_taille'] = train_df[size_cols].std(axis=1)\n",
    "            test_df['var_taille'] = test_df[size_cols].std(axis=1)\n",
    "            new_activity_cols.append('var_taille')\n",
    "        \n",
    "        # Traitement de COEFASS\n",
    "        if 'COEFASS' in ACTIVITY_VARS['size_category']:\n",
    "            # Encodage fréquentiel\n",
    "            coef_map = (train_df['COEFASS'].value_counts() / len(train_df)).to_dict()\n",
    "            train_df['coefass_freq'] = train_df['COEFASS'].map(coef_map)\n",
    "            test_df['coefass_freq'] = test_df['COEFASS'].map(coef_map)\n",
    "            new_activity_cols.append('coefass_freq')\n",
    "            \n",
    "            # Encodage one-hot\n",
    "            dummies = pd.get_dummies(train_df['COEFASS'], prefix='coefass')\n",
    "            test_dummies = pd.get_dummies(test_df['COEFASS'], prefix='coefass')\n",
    "            \n",
    "            # Assurer les mêmes colonnes\n",
    "            for dummy_col in dummies.columns:\n",
    "                if dummy_col not in test_dummies.columns:\n",
    "                    test_dummies[dummy_col] = 0\n",
    "            \n",
    "            train_df = pd.concat([train_df, dummies], axis=1)\n",
    "            test_df = pd.concat([test_df, test_dummies], axis=1)\n",
    "            new_activity_cols.extend(dummies.columns)\n",
    "    \n",
    "    # 4. Traitement des services d'urgence\n",
    "    if 'emergency' in ACTIVITY_VARS:\n",
    "        print(\"\\nTraitement des services d'urgence:\")\n",
    "        emergency_cols = ACTIVITY_VARS['emergency']\n",
    "        # Garder ces variables telles quelles\n",
    "        new_activity_cols.extend(emergency_cols)\n",
    "    \n",
    "    # Identifier les colonnes à supprimer\n",
    "    activity_cols_to_drop = sum([\n",
    "        ACTIVITY_VARS['equipment'],\n",
    "        ACTIVITY_VARS['activity'],\n",
    "        ACTIVITY_VARS['size_category']\n",
    "    ], [])\n",
    "    \n",
    "    # Statistiques des nouvelles variables\n",
    "    print(\"\\n=== Statistiques des nouvelles variables d'activité ===\")\n",
    "    print(f\"\\nNombre de variables créées: {len(new_activity_cols)}\")\n",
    "    print(f\"Nombre de variables à supprimer: {len(activity_cols_to_drop)}\")\n",
    "    \n",
    "    print(\"\\nStatistiques descriptives des variables numériques:\")\n",
    "    numeric_cols = [col for col in new_activity_cols \n",
    "                   if train_df[col].dtype in ['int64', 'float64']]\n",
    "    print(train_df[numeric_cols].describe())\n",
    "    \n",
    "    return {\n",
    "        'to_drop': activity_cols_to_drop,\n",
    "        'new_cols': new_activity_cols,\n",
    "        'train_df': train_df,\n",
    "        'test_df': test_df\n",
    "    }\n",
    "\n",
    "# Exécuter la fonction\n",
    "activity_results = handle_activity_vars(train_df, test_df)\n",
    "\n",
    "# Mettre à jour les DataFrames\n",
    "train_df = activity_results['train_df']\n",
    "test_df = activity_results['test_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_variable_groups():\n",
    "    print(\"=== Vérification des groupes de variables ===\")\n",
    "    \n",
    "    # Fonction récursive pour extraire toutes les variables d'une structure imbriquée\n",
    "    def extract_variables(structure):\n",
    "        variables = set()\n",
    "        if isinstance(structure, dict):\n",
    "            for value in structure.values():\n",
    "                if isinstance(value, (list, set)):\n",
    "                    variables.update(value)\n",
    "                elif isinstance(value, dict):\n",
    "                    variables.update(extract_variables(value))\n",
    "        elif isinstance(structure, (list, set)):\n",
    "            variables.update(structure)\n",
    "        return variables\n",
    "    \n",
    "    # Rassembler toutes les variables des groupes\n",
    "    group_stats = {}\n",
    "    all_grouped_vars = set()\n",
    "    \n",
    "    # Traiter chaque groupe principal\n",
    "    groups_dict = {\n",
    "        'WEATHER': WEATHER_VARS,\n",
    "        'BUILDING': BUILDING_VARS,\n",
    "        'INSURANCE': INSURANCE_VARS,\n",
    "        'GEOGRAPHIC': GEOGRAPHIC_VARS,\n",
    "        'DEMOGRAPHIC': DEMOGRAPHIC_VARS,\n",
    "        'ACTIVITY': ACTIVITY_VARS,\n",
    "        'TARGET': TARGET_VARS['primary'],  # Cas spécial pour TARGET_VARS\n",
    "        'ID': ID_VARS['identifiers']       # Cas spécial pour ID_VARS\n",
    "    }\n",
    "    \n",
    "    # Analyser chaque groupe\n",
    "    for group_name, group_structure in groups_dict.items():\n",
    "        group_vars = extract_variables(group_structure)\n",
    "        group_stats[group_name] = {\n",
    "            'count': len(group_vars),\n",
    "            'variables': sorted(group_vars)\n",
    "        }\n",
    "        all_grouped_vars.update(group_vars)\n",
    "    \n",
    "    # Comparer avec les variables du dataset\n",
    "    all_df_vars = set(train_df.columns)\n",
    "    ungrouped_vars = all_df_vars - all_grouped_vars\n",
    "    overlapping_vars = set()\n",
    "    \n",
    "    # Vérifier les chevauchements entre groupes\n",
    "    for group1 in groups_dict.keys():\n",
    "        for group2 in groups_dict.keys():\n",
    "            if group1 < group2:  # Éviter les comparaisons doubles\n",
    "                vars1 = set(group_stats[group1]['variables'])\n",
    "                vars2 = set(group_stats[group2]['variables'])\n",
    "                overlap = vars1.intersection(vars2)\n",
    "                if overlap:\n",
    "                    overlapping_vars.update(overlap)\n",
    "                    print(f\"\\nChevauchement entre {group1} et {group2}:\")\n",
    "                    print(sorted(overlap))\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(\"\\n=== Statistiques par groupe ===\")\n",
    "    for group_name, stats in group_stats.items():\n",
    "        print(f\"\\n{group_name}:\")\n",
    "        print(f\"  Nombre de variables: {stats['count']}\")\n",
    "        print(f\"  Exemples: {stats['variables'][:3]}...\")\n",
    "    \n",
    "    print(\"\\n=== Résumé global ===\")\n",
    "    print(f\"Variables dans le dataset: {len(all_df_vars)}\")\n",
    "    print(f\"Variables groupées: {len(all_grouped_vars)}\")\n",
    "    print(f\"Variables non groupées: {len(ungrouped_vars)}\")\n",
    "    print(f\"Variables en chevauchement: {len(overlapping_vars)}\")\n",
    "    \n",
    "    if ungrouped_vars:\n",
    "        print(\"\\n=== Variables non groupées ===\")\n",
    "        print(sorted(ungrouped_vars))\n",
    "    \n",
    "    if overlapping_vars:\n",
    "        print(\"\\n=== Variables en chevauchement ===\")\n",
    "        print(sorted(overlapping_vars))\n",
    "    \n",
    "    return {\n",
    "        'group_stats': group_stats,\n",
    "        'summary': {\n",
    "            'total_vars': len(all_df_vars),\n",
    "            'grouped_vars': len(all_grouped_vars),\n",
    "            'ungrouped_vars': sorted(ungrouped_vars),\n",
    "            'overlapping_vars': sorted(overlapping_vars)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Exécuter la vérification\n",
    "verification_results = verify_variable_groups()\n",
    "\n",
    "# Afficher des recommandations si nécessaire\n",
    "if verification_results['summary']['ungrouped_vars']:\n",
    "    print(\"\\n=== Recommandations ===\")\n",
    "    print(\"Variables à grouper:\")\n",
    "    for var in verification_results['summary']['ungrouped_vars']:\n",
    "        # Suggérer un groupe basé sur le nom de la variable\n",
    "        if any(x in var.upper() for x in ['TEMP', 'WIND', 'RAIN']):\n",
    "            print(f\"  {var} -> WEATHER_VARS\")\n",
    "        elif any(x in var.upper() for x in ['BUILD', 'SURFACE', 'HEIGHT']):\n",
    "            print(f\"  {var} -> BUILDING_VARS\")\n",
    "        elif any(x in var.upper() for x in ['RISK', 'INSURANCE', 'CAPITAL']):\n",
    "            print(f\"  {var} -> INSURANCE_VARS\")\n",
    "        else:\n",
    "            print(f\"  {var} -> Groupe à déterminer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
